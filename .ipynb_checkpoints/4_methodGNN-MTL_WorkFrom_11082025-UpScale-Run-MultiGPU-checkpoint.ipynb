{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb42dc8-77b1-4ea2-8a93-849a53a2d5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# constant\n",
    "import multiprocessing as mp \n",
    "mp.set_start_method('spawn', force=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d53c109-35fd-4bed-b396-44ba6eab6354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LETTER_TO_NUM = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                       'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                       'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19,\n",
    "                       'N': 2, 'Y': 18, 'M': 12, 'X':20}\n",
    "\n",
    "NUM_TO_LETTER = {v:k for k, v in LETTER_TO_NUM.items()}\n",
    "\n",
    "ATOM_VOCAB = [\n",
    "    'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca',\n",
    "    'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag',\n",
    "    'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni',\n",
    "    'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'unk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22941f1-a773-461a-8368-38041f0d91c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e609fcb2-a1c0-4610-bf45-b94a185000b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drug-target binding affinity datasets\n",
    "\"\"\"\n",
    "import math\n",
    "import yaml\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class DTA(data.Dataset):\n",
    "    \"\"\"\n",
    "    Base class for loading drug-target binding affinity datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, df=None, data_list=None, onthefly=False,\n",
    "                prot_featurize_fn=None, drug_featurize_fn=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "            df : pd.DataFrame with columns [`drug`, `protein`, `y`],\n",
    "                where `drug`: drug key, `protein`: protein key, `y`: binding affinity.\n",
    "            data_list : list of dict (same order as df)\n",
    "                if `onthefly` is True, data_list has the PDB coordinates and SMILES strings\n",
    "                    {`drug`: SDF file path, `protein`: coordinates dict (`pdb_data` in `DTATask`), `y`: float}\n",
    "                if `onthefly` is False, data_list has the cached torch_geometric graphs\n",
    "                    {`drug`: `torch_geometric.data.Data`, `protein`: `torch_geometric.data.Data`, `y`: float}\n",
    "                `protein` has attributes:\n",
    "                    -x          alpha carbon coordinates, shape [n_nodes, 3]\n",
    "                    -edge_index edge indices, shape [2, n_edges]\n",
    "                    -seq        sequence converted to int tensor according to `self.letter_to_num`, shape [n_nodes]\n",
    "                    -name       name of the protein structure, string\n",
    "                    -node_s     node scalar features, shape [n_nodes, 6]\n",
    "                    -node_v     node vector features, shape [n_nodes, 3, 3]\n",
    "                    -edge_s     edge scalar features, shape [n_edges, 39]\n",
    "                    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "                    -mask       node mask, `False` for nodes with missing data that are excluded from message passing\n",
    "                    -seq_emb    sequence embedding (ESM1b), shape [n_nodes, 1280]\n",
    "                `drug` has attributes:\n",
    "                    -x          atom coordinates, shape [n_nodes, 3]\n",
    "                    -edge_index edge indices, shape [2, n_edges]\n",
    "                    -node_s     node scalar features, shape [n_nodes, 66]\n",
    "                    -node_v     node vector features, shape [n_nodes, 1, 3]\n",
    "                    -edge_s     edge scalar features, shape [n_edges, 16]\n",
    "                    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "                    -name       name of the drug, string\n",
    "            onthefly : bool\n",
    "                whether to featurize data on the fly or pre-compute\n",
    "            prot_featurize_fn : function\n",
    "                function to featurize a protein.\n",
    "            drug_featurize_fn : function\n",
    "                function to featurize a drug.\n",
    "        \"\"\"\n",
    "        super(DTA, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.data_list = data_list\n",
    "        self.onthefly = onthefly\n",
    "        if onthefly:\n",
    "            assert prot_featurize_fn is not None, 'prot_featurize_fn must be provided'\n",
    "            assert drug_featurize_fn is not None, 'drug_featurize_fn must be provided'\n",
    "        self.prot_featurize_fn = prot_featurize_fn\n",
    "        self.drug_featurize_fn = drug_featurize_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.onthefly:\n",
    "            drug = self.drug_featurize_fn(\n",
    "                self.data_list[idx]['drug'],\n",
    "                name=self.data_list[idx]['drug_name']\n",
    "            )\n",
    "            prot = self.prot_featurize_fn(\n",
    "                self.data_list[idx]['protein'],\n",
    "                name=self.data_list[idx]['protein_name']\n",
    "            )\n",
    "        else:\n",
    "            drug = self.data_list[idx]['drug']\n",
    "            prot = self.data_list[idx]['protein']\n",
    "        y = self.data_list[idx]['y']\n",
    "        item = {'drug': drug, 'protein': prot, 'y': y}\n",
    "        return item\n",
    "\n",
    "\n",
    "def create_fold(df, fold_seed, frac):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds by random splitting.\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test = df.sample(frac = test_frac, replace = False, random_state = fold_seed)\n",
    "    train_val = df[~df.index.isin(test.index)]\n",
    "    val = train_val.sample(frac = val_frac/(1-test_frac), replace = False, random_state = 1)\n",
    "    train = train_val[~train_val.index.isin(val.index)]\n",
    "\n",
    "    return {'train': train.reset_index(drop = True),\n",
    "            'valid': val.reset_index(drop = True),\n",
    "            'test': test.reset_index(drop = True)}\n",
    "\n",
    "\n",
    "def create_fold_setting_cold(df, fold_seed, frac, entity):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds by drug/protein-wise splitting.\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    gene_drop = df[entity].drop_duplicates().sample(frac = test_frac, replace = False, random_state = fold_seed).values\n",
    "\n",
    "    test = df[df[entity].isin(gene_drop)]\n",
    "\n",
    "    train_val = df[~df[entity].isin(gene_drop)]\n",
    "\n",
    "    gene_drop_val = train_val[entity].drop_duplicates().sample(frac = val_frac/(1-test_frac), replace = False, random_state = fold_seed).values\n",
    "    val = train_val[train_val[entity].isin(gene_drop_val)]\n",
    "    train = train_val[~train_val[entity].isin(gene_drop_val)]\n",
    "\n",
    "    return {'train': train.reset_index(drop = True),\n",
    "            'valid': val.reset_index(drop = True),\n",
    "            'test': test.reset_index(drop = True)}\n",
    "\n",
    "\n",
    "def create_full_ood_set(df, fold_seed, frac):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds such that drugs and proteins are\n",
    "    not overlapped in train and test sets. Train and valid may share\n",
    "    drugs and proteins (random split).\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test_drugs = df['drug'].drop_duplicates().sample(frac=test_frac, replace=False, random_state=fold_seed).values\n",
    "    test_prots = df['protein'].drop_duplicates().sample(frac=test_frac, replace=False, random_state=fold_seed).values\n",
    "\n",
    "    test = df[(df['drug'].isin(test_drugs)) & (df['protein'].isin(test_prots))]\n",
    "    train_val = df[(~df['drug'].isin(test_drugs)) & (~df['protein'].isin(test_prots))]\n",
    "\n",
    "    val = train_val.sample(frac=val_frac/(1-test_frac), replace=False, random_state=fold_seed)\n",
    "    train = train_val[~train_val.index.isin(val.index)]\n",
    "\n",
    "    return {'train': train.reset_index(drop=True),\n",
    "            'valid': val.reset_index(drop=True),\n",
    "            'test': test.reset_index(drop=True)}\n",
    "\n",
    "\n",
    "def create_seq_identity_fold(df, mmseqs_seq_clus_df, fold_seed, frac, min_clus_in_split=5):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/drorlab/atom3d/blob/master/atom3d/splits/sequence.py\n",
    "    Clusters are selected randomly into validation and test sets,\n",
    "    but to ensure that there is some diversity in each set\n",
    "    (i.e. a split does not consist of a single sequence cluster), a minimum number of clusters in each split is enforced.\n",
    "    Some data examples may be removed in order to satisfy this constraint.\n",
    "    \"\"\"\n",
    "    _rng = np.random.RandomState(fold_seed)\n",
    "\n",
    "    def _parse_mmseqs_cluster_res(mmseqs_seq_clus_df):\n",
    "        clus2seq, seq2clus = {}, {}\n",
    "        for rep, sdf in mmseqs_seq_clus_df.groupby('rep'):\n",
    "            for seq in sdf['seq']:\n",
    "                if rep not in clus2seq:\n",
    "                    clus2seq[rep] = []\n",
    "                clus2seq[rep].append(seq)\n",
    "                seq2clus[seq] = rep\n",
    "        return seq2clus, clus2seq\n",
    "\n",
    "    def _create_cluster_split(df, seq2clus, clus2seq, to_use, split_size, min_clus_in_split):\n",
    "        data = df.copy()\n",
    "        all_prot = set(seq2clus.keys())\n",
    "        used = all_prot.difference(to_use)\n",
    "        split = None\n",
    "        while True:\n",
    "            p = _rng.choice(sorted(to_use))\n",
    "            c = seq2clus[p]\n",
    "            members = set(clus2seq[c])\n",
    "            members = members.difference(used)\n",
    "            if len(members) == 0:\n",
    "                continue\n",
    "            # ensure that at least min_fam_in_split families in each split\n",
    "            max_clust_size = int(np.ceil(split_size / min_clus_in_split))\n",
    "            sel_prot = list(members)[:max_clust_size]\n",
    "            sel_df = data[data['protein'].isin(sel_prot)]\n",
    "            split = sel_df if split is None else pd.concat([split, sel_df])\n",
    "            to_use = to_use.difference(members)\n",
    "            used = used.union(members)\n",
    "            if len(split) >= split_size:\n",
    "                break\n",
    "        split = split.reset_index(drop=True)\n",
    "        return split, to_use\n",
    "\n",
    "    seq2clus, clus2seq = _parse_mmseqs_cluster_res(mmseqs_seq_clus_df)\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test_size, val_size = len(df) * test_frac, len(df) * val_frac\n",
    "    to_use = set(seq2clus.keys())\n",
    "\n",
    "    val_df, to_use = _create_cluster_split(df, seq2clus, clus2seq, to_use, val_size, min_clus_in_split)\n",
    "    test_df, to_use = _create_cluster_split(df, seq2clus, clus2seq, to_use, test_size, min_clus_in_split)\n",
    "    train_df = df[df['protein'].isin(to_use)].reset_index(drop=True)\n",
    "    train_df['split'] = 'train'\n",
    "    val_df['split'] = 'valid'\n",
    "    test_df['split'] = 'test'\n",
    "\n",
    "    assert len(set(train_df['protein']) & set(val_df['protein'])) == 0\n",
    "    assert len(set(test_df['protein']) & set(val_df['protein'])) == 0\n",
    "    assert len(set(train_df['protein']) & set(test_df['protein'])) == 0\n",
    "\n",
    "    return {'train': train_df.reset_index(drop=True),\n",
    "            'valid': val_df.reset_index(drop=True),\n",
    "            'test': test_df.reset_index(drop=True)}\n",
    "\n",
    "\n",
    "class DTATask(object):\n",
    "    \"\"\"\n",
    "    Drug-target binding task (e.g., KIBA or Davis).\n",
    "    Three splits: train/valid/test, each split is a DTA() class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            task_name=None,\n",
    "            df=None,\n",
    "            prot_pdb_id=None, pdb_data=None,\n",
    "            emb_dir=None,\n",
    "            drug_sdf_dir=None,\n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_clus_df=None,\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_name: str\n",
    "            Name of the task (e.g., KIBA, Davis, etc.)\n",
    "        df: pd.DataFrame\n",
    "            Dataframe containing the data\n",
    "        prot_pdb_id: dict\n",
    "            Dictionary mapping protein name to PDB ID\n",
    "        pdb_data: dict\n",
    "            A json format of pocket structure data, where key is the PDB ID\n",
    "            and value is the corresponding PDB structure data in a dictionary:\n",
    "                -'name': kinase name\n",
    "                -'UniProt_id': UniProt ID\n",
    "                -'PDB_id': PDB ID,\n",
    "                -'chain': chain ID,\n",
    "                -'seq': pocket sequence,                \n",
    "                -'coords': coordinates of the 'N', 'CA', 'C', 'O' atoms of the pocket residues,\n",
    "                    - \"N\": [[x, y, z], ...]\n",
    "                    - \"CA\": [[], ...],\n",
    "                    - \"C\": [[], ...],\n",
    "                    - \"O\": [[], ...]               \n",
    "            (there are some other keys but only for internal use)\n",
    "        emb_dir: str\n",
    "            Directory containing the protein embeddings\n",
    "        drug_sdf_dir: str\n",
    "            Directory containing the drug SDF files\n",
    "        num_pos_emb: int\n",
    "            Dimension of positional embeddings\n",
    "        num_rbf: int\n",
    "            Number of radial basis functions\n",
    "        contact_cutoff: float\n",
    "            Cutoff distance for defining residue-residue contacts\n",
    "        split_method: str\n",
    "            how to split train/test sets, \n",
    "            -`random`: random split\n",
    "            -`protein`: split by protein\n",
    "            -`drug`: split by drug\n",
    "            -`both`: unseen drugs and proteins in test set\n",
    "            -`seqid`: split by protein sequence identity \n",
    "                (need to priovide the MMseqs2 sequence cluster result,\n",
    "                see `mmseqs_seq_clus_df`)\n",
    "        split_frac: list\n",
    "            Fraction of data in train/valid/test sets\n",
    "        mmseqs_seq_clus_df: pd.DataFrame\n",
    "            Dataframe containing the MMseqs2 sequence cluster result\n",
    "            using a desired sequence identity cutoff\n",
    "        seed: int\n",
    "            Random seed\n",
    "        onthefly: bool\n",
    "            whether to featurize data on the fly or pre-compute\n",
    "        \"\"\"\n",
    "        self.task_name = task_name        \n",
    "        self.prot_pdb_id = prot_pdb_id\n",
    "        self.pdb_data = pdb_data        \n",
    "        self.emb_dir = emb_dir\n",
    "        self.df = df\n",
    "        self.prot_featurize_params = dict(\n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff)        \n",
    "        self.drug_sdf_dir = drug_sdf_dir        \n",
    "        self._prot2pdb = None\n",
    "        self._pdb_graph_db = None        \n",
    "        self._drug2sdf_file = None\n",
    "        self._drug_sdf_db = None\n",
    "        self.split_method = split_method\n",
    "        self.split_frac = split_frac\n",
    "        self.mmseqs_seq_clus_df = mmseqs_seq_clus_df\n",
    "        self.seed = seed\n",
    "        self.onthefly = onthefly\n",
    "\n",
    "    def _format_pdb_entry(self, _data):\n",
    "        _coords = _data[\"coords\"]\n",
    "        entry = {\n",
    "            \"name\": _data[\"name\"],\n",
    "            \"seq\": _data[\"seq\"],\n",
    "            \"coords\": list(zip(_coords[\"N\"], _coords[\"CA\"], _coords[\"C\"], _coords[\"O\"])),\n",
    "        }        \n",
    "        if self.emb_dir is not None:\n",
    "            embed_file = f\"{_data['PDB_id']}.{_data['chain']}.pt\"\n",
    "            entry[\"embed\"] = f\"{self.emb_dir}/{embed_file}\"\n",
    "        return entry\n",
    "\n",
    "    @property\n",
    "    def prot2pdb(self):\n",
    "        if self._prot2pdb is None:\n",
    "            self._prot2pdb = {}\n",
    "            for prot, pdb in self.prot_pdb_id.items():\n",
    "                _pdb_entry = self.pdb_data[pdb]\n",
    "                self._prot2pdb[prot] = self._format_pdb_entry(_pdb_entry)\n",
    "        return self._prot2pdb\n",
    "\n",
    "    @property\n",
    "    def pdb_graph_db(self):\n",
    "        if self._pdb_graph_db is None:\n",
    "            self._pdb_graph_db = pdb_graph.pdb_to_graphs(self.prot2pdb,\n",
    "                self.prot_featurize_params)\n",
    "        return self._pdb_graph_db\n",
    "\n",
    "    @property\n",
    "    def drug2sdf_file(self):\n",
    "        if self._drug2sdf_file is None:            \n",
    "            drug2sdf_file = {f.stem : str(f) for f in Path(self.drug_sdf_dir).glob('*.sdf')}\n",
    "            # Convert str keys to int for Davis\n",
    "            if self.task_name == 'DAVIS' and all([k.isdigit() for k in drug2sdf_file.keys()]):\n",
    "                drug2sdf_file = {int(k) : v for k, v in drug2sdf_file.items()}\n",
    "            self._drug2sdf_file = drug2sdf_file\n",
    "        return self._drug2sdf_file\n",
    "\n",
    "    @property\n",
    "    def drug_sdf_db(self):\n",
    "        if self._drug_sdf_db is None:\n",
    "            self._drug_sdf_db = mol_graph.sdf_to_graphs(self.drug2sdf_file)\n",
    "        return self._drug_sdf_db\n",
    "\n",
    "\n",
    "    def build_data(self, df, onthefly=False):\n",
    "        records = df.to_dict('records')\n",
    "        data_list = []\n",
    "        for entry in records:\n",
    "            drug = entry['drug']\n",
    "            prot = entry['protein']\n",
    "            if onthefly:\n",
    "                pf = self.prot2pdb[prot]\n",
    "                df = self.drug2sdf_file[drug]\n",
    "            else:                \n",
    "                pf = self.pdb_graph_db[prot]                \n",
    "                df = self.drug_sdf_db[drug]\n",
    "            data_list.append({'drug': df, 'protein': pf, 'y': entry['y'],\n",
    "                'drug_name': drug, 'protein_name': prot})\n",
    "        if onthefly:\n",
    "            prot_featurize_fn = partial(\n",
    "                pdb_graph.featurize_protein_graph,\n",
    "                **self.prot_featurize_params)            \n",
    "            drug_featurize_fn = mol_graph.featurize_drug\n",
    "        else:\n",
    "            prot_featurize_fn, drug_featurize_fn = None, None\n",
    "        data = DTA(df=df, data_list=data_list, onthefly=onthefly,\n",
    "            prot_featurize_fn=prot_featurize_fn, drug_featurize_fn=drug_featurize_fn)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_split(self, df=None, split_method=None,\n",
    "            split_frac=None, seed=None, onthefly=None,\n",
    "            return_df=False):\n",
    "        df = df or self.df\n",
    "        split_method = split_method or self.split_method\n",
    "        split_frac = split_frac or self.split_frac\n",
    "        seed = seed or self.seed\n",
    "        onthefly = onthefly or self.onthefly\n",
    "        if split_method == 'random':\n",
    "            split_df = create_fold(self.df, seed, split_frac)\n",
    "        elif split_method == 'drug':\n",
    "            split_df = create_fold_setting_cold(self.df, seed, split_frac, 'drug')\n",
    "        elif split_method == 'protein':\n",
    "            split_df = create_fold_setting_cold(self.df, seed, split_frac, 'protein')\n",
    "        elif split_method == 'both':\n",
    "            split_df = create_full_ood_set(self.df, seed, split_frac)\n",
    "        elif split_method == 'seqid':\n",
    "            split_df = create_seq_identity_fold(\n",
    "                self.df, self.mmseqs_seq_clus_df, seed, split_frac)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown split method: {}\".format(split_method))\n",
    "        split_data = {}\n",
    "        for split, df in split_df.items():\n",
    "            split_data[split] = self.build_data(df, onthefly=onthefly)\n",
    "        if return_df:\n",
    "            return split_data, split_df\n",
    "        else:\n",
    "            return split_data\n",
    "\n",
    "\n",
    "class KIBA(DTATask):\n",
    "    \"\"\"\n",
    "    KIBA drug-target interaction dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            data_path='../data/KIBA/kiba_data.tsv',            \n",
    "            pdb_map='../data/KIBA/kiba_uniprot2pdb.yaml',\n",
    "            pdb_json='../data/structure/pockets_structure.json',                        \n",
    "            emb_dir='../data/esm1b',           \n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,            \n",
    "            drug_sdf_dir='../data/structure/kiba_mol3d_sdf',\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_cluster_file='../data/KIBA/kiba_cluster_id50_cluster.tsv',\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        df = pd.read_table(data_path)        \n",
    "        prot_pdb_id = yaml.safe_load(open(pdb_map, 'r'))\n",
    "        pdb_data = json.load(open(pdb_json, 'r'))                \n",
    "        mmseqs_seq_clus_df = pd.read_table(mmseqs_seq_cluster_file, names=['rep', 'seq'])\n",
    "        super(KIBA, self).__init__(\n",
    "            task_name='KIBA',\n",
    "            df=df, \n",
    "            prot_pdb_id=prot_pdb_id, pdb_data=pdb_data,\n",
    "            emb_dir=emb_dir,            \n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "            drug_sdf_dir=drug_sdf_dir,\n",
    "            split_method=split_method, split_frac=split_frac,\n",
    "            mmseqs_seq_clus_df=mmseqs_seq_clus_df,\n",
    "            seed=seed, onthefly=onthefly\n",
    "            )\n",
    "\n",
    "\n",
    "class DAVIS(DTATask):\n",
    "    \"\"\"\n",
    "    DAVIS drug-target interaction dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            data_path='../data/DAVIS/davis_data.tsv',            \n",
    "            pdb_map='../data/DAVIS/davis_protein2pdb.yaml',\n",
    "            pdb_json='../data/structure/pockets_structure.json',                        \n",
    "            emb_dir='../data/esm1b',           \n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,            \n",
    "            drug_sdf_dir='../data/structure/davis_mol3d_sdf',\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_cluster_file='../data/DAVIS/davis_cluster_id50_cluster.tsv',\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        df = pd.read_table(data_path)        \n",
    "        prot_pdb_id = yaml.safe_load(open(pdb_map, 'r'))\n",
    "        pdb_data = json.load(open(pdb_json, 'r'))        \n",
    "        mmseqs_seq_clus_df = pd.read_table(mmseqs_seq_cluster_file, names=['rep', 'seq'])\n",
    "        super(DAVIS, self).__init__(\n",
    "            task_name='DAVIS',\n",
    "            df=df, \n",
    "            prot_pdb_id=prot_pdb_id, pdb_data=pdb_data,\n",
    "            emb_dir=emb_dir,            \n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "            drug_sdf_dir=drug_sdf_dir,\n",
    "            split_method=split_method, split_frac=split_frac,\n",
    "            mmseqs_seq_clus_df=mmseqs_seq_clus_df,\n",
    "            seed=seed, onthefly=onthefly\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c80511-2ef1-47a7-a6e2-21236a2b6262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c3931f-b6b7-4e9f-8ba0-f74b5c65bb8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rldif118/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parallel_train_per_epoch(\n",
    "    kwargs=None, test_loader=None,\n",
    "    n_epochs=None, eval_freq=None, test_freq=None,\n",
    "    monitoring_score='pearson',\n",
    "    loss_fn=None, logger=None,\n",
    "    test_after_train=True,\n",
    "):\n",
    "    midx = kwargs['midx']\n",
    "    model = kwargs['model']\n",
    "    optimizer = kwargs['optimizer']\n",
    "    train_loader = kwargs['train_loader']\n",
    "    valid_loader = kwargs['valid_loader']\n",
    "    device = kwargs['device']\n",
    "    stopper = kwargs['stopper']\n",
    "    best_model_state_dict = kwargs['best_model_state_dict']\n",
    "    if stopper.early_stop:\n",
    "        return kwargs\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yh = model(xd, xp)\n",
    "            loss = loss_fn(yh, y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / step\n",
    "        if epoch % eval_freq == 0:\n",
    "            val_results = _parallel_test(\n",
    "                {'model': model, 'midx': midx, 'test_loader': valid_loader, 'device': device},\n",
    "                loss_fn=loss_fn, logger=logger\n",
    "            )\n",
    "            is_best = stopper.update(val_results['metrics'][monitoring_score])\n",
    "            if is_best:\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            logger.info(f\"M-{midx} E-{epoch} | Train Loss: {train_loss:.4f} | Valid Loss: {val_results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in val_results['metrics'].items()])\n",
    "                + f\" | best {monitoring_score}: {stopper.best_score:.4f}\"\n",
    "                )\n",
    "        if test_freq is not None and epoch % test_freq == 0:\n",
    "            test_results = _parallel_test(\n",
    "                {'midx': midx, 'model': model, 'test_loader': test_loader, 'device': device},\n",
    "                loss_fn=loss_fn, logger=logger\n",
    "            )\n",
    "            logger.info(f\"M-{midx} E-{epoch} | Test Loss: {test_results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in test_results['metrics'].items()])\n",
    "                )\n",
    "\n",
    "        if stopper.early_stop:\n",
    "            logger.info('Eearly stop at epoch {}'.format(epoch))\n",
    "\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "    if test_after_train:\n",
    "        test_results = _parallel_test(\n",
    "            {'midx': midx, 'model': model, 'test_loader': test_loader, 'device': device},\n",
    "            loss_fn=loss_fn,\n",
    "            test_tag=f\"Model {midx}\", print_log=True, logger=logger\n",
    "        )\n",
    "    rets = dict(midx = midx, model = model)\n",
    "    return rets\n",
    "\n",
    "\n",
    "def _parallel_test(\n",
    "    kwargs=None, loss_fn=None, \n",
    "    test_tag=None, print_log=False, logger=None,\n",
    "):\n",
    "    midx = kwargs['midx']\n",
    "    model = kwargs['model']\n",
    "    test_loader = kwargs['test_loader']\n",
    "    device = kwargs['device']\n",
    "    model.eval()\n",
    "    yt, yp, total_loss = torch.Tensor(), torch.Tensor(), 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_loader, start=1):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            yh = model(xd, xp)\n",
    "            loss = loss_fn(yh, y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "            yp = torch.cat([yp, yh.detach().cpu()], dim=0)\n",
    "            yt = torch.cat([yt, y.detach().cpu()], dim=0)\n",
    "    yt = yt.numpy()\n",
    "    yp = yp.view(-1).numpy()\n",
    "    results = {\n",
    "        'midx': midx,\n",
    "        'y_true': yt,\n",
    "        'y_pred': yp,\n",
    "        'loss': total_loss / step,\n",
    "    }\n",
    "    eval_metrics = evaluation_metrics(\n",
    "        yt, yp,\n",
    "        eval_metrics=['mse', 'spearman', 'pearson']\n",
    "    )\n",
    "    results['metrics'] = eval_metrics\n",
    "    if print_log:\n",
    "        logger.info(f\"{test_tag} | Test Loss: {results['loss']:.4f} | \"\\\n",
    "            + ' | '.join([f'{k}: {v:.4f}' for k, v in results['metrics'].items()]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def _unpack_evidential_output(output):\n",
    "    mu, v, alpha, beta = torch.split(output, output.shape[1]//4, dim=1)\n",
    "    inverse_evidence = 1. / ((alpha - 1) * v)\n",
    "    var = beta * inverse_evidence\n",
    "    return mu, var, inverse_evidence\n",
    "\n",
    "\n",
    "class DTAExperiment(object):\n",
    "    def __init__(self,\n",
    "        task=None,\n",
    "        split_method='protein',\n",
    "        split_frac=[0.7, 0.1, 0.2],\n",
    "        prot_gcn_dims=[128, 128, 128], prot_gcn_bn=False,\n",
    "        prot_fc_dims=[1024, 128],\n",
    "        drug_in_dim=66, drug_fc_dims=[1024, 128], drug_gcn_dims=[128, 64],\n",
    "        mlp_dims=[1024, 512], mlp_dropout=0.25,\n",
    "        num_pos_emb=16, num_rbf=16,\n",
    "        contact_cutoff=8.,\n",
    "        n_ensembles=1, n_epochs=500, batch_size=256,\n",
    "        lr=0.001,        \n",
    "        seed=42, onthefly=False,\n",
    "        uncertainty=False, parallel=False,\n",
    "        output_dir='../output', save_log=False\n",
    "    ):\n",
    "        self.saver = Saver(output_dir)\n",
    "        self.logger = Logger(logfile=self.saver.save_dir/'exp.log' if save_log else None)\n",
    "\n",
    "        self.uncertainty = uncertainty\n",
    "        self.parallel = parallel\n",
    "        self.n_ensembles = n_ensembles\n",
    "        if self.uncertainty and self.n_ensembles < 2:\n",
    "            raise ValueError('n_ensembles must be greater than 1 when uncertainty is True')            \n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        dataset_klass = {\n",
    "            'kiba': KIBA,\n",
    "            'davis': DAVIS,\n",
    "        }[task]\n",
    "\n",
    "        self.dataset = dataset_klass(\n",
    "            split_method=split_method,\n",
    "            split_frac=split_frac,\n",
    "            seed=seed,\n",
    "            onthefly=onthefly,\n",
    "            num_pos_emb=num_pos_emb,\n",
    "            num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "        )\n",
    "        self._task_data_df_split = None\n",
    "        self._task_loader = None\n",
    "\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if self.parallel and n_gpus < self.n_ensembles:\n",
    "            self.logger.warning(f\"Visible GPUs ({n_gpus}) is fewer than \"\n",
    "            f\"number of models ({self.n_ensembles}). Some models will be run on the same GPU\"\n",
    "            )\n",
    "        self.devices = [torch.device(f'cuda:{i % n_gpus}')\n",
    "            for i in range(self.n_ensembles)]\n",
    "        self.model_config = dict(\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=prot_gcn_dims,            \n",
    "            prot_fc_dims=prot_fc_dims,\n",
    "            drug_node_in_dim=[66, 1], \n",
    "            drug_node_h_dims=drug_gcn_dims,\n",
    "            drug_fc_dims=drug_fc_dims,            \n",
    "            mlp_dims=mlp_dims, mlp_dropout=mlp_dropout)\n",
    "        self.build_model()\n",
    "        self.criterion = F.mse_loss\n",
    "\n",
    "        self.split_method = split_method\n",
    "        self.split_frac = split_frac\n",
    "\n",
    "        self.logger.info(self.models[0])\n",
    "        self.logger.info(self.optimizers[0])\n",
    "\n",
    "    def build_model(self):\n",
    "        self.models = [DTAModel(**self.model_config).to(self.devices[i])\n",
    "                        for i in range(self.n_ensembles)]\n",
    "        self.optimizers = [optim.Adam(model.parameters(), lr=self.lr) for model in self.models]\n",
    "\n",
    "    def _get_data_loader(self, dataset, shuffle=False):\n",
    "        return torch_geometric.loader.DataLoader(\n",
    "                    dataset=dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    pin_memory=False,\n",
    "                    num_workers=0,\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def task_data_df_split(self):\n",
    "        if self._task_data_df_split is None:\n",
    "            (data, df) = self.dataset.get_split(return_df=True)\n",
    "            self._task_data_df_split = (data, df)\n",
    "        return self._task_data_df_split\n",
    "\n",
    "    @property\n",
    "    def task_data(self):\n",
    "        return self.task_data_df_split[0]\n",
    "\n",
    "    @property\n",
    "    def task_df(self):\n",
    "        return self.task_data_df_split[1]\n",
    "\n",
    "    @property\n",
    "    def task_loader(self):\n",
    "        if self._task_loader is None:\n",
    "            _loader = {\n",
    "                s: self._get_data_loader(\n",
    "                    self.task_data[s], shuffle=(s == 'train'))\n",
    "                for s in self.task_data\n",
    "            }\n",
    "            self._task_loader = _loader\n",
    "        return self._task_loader\n",
    "\n",
    "    def recalibrate_std(self, df, recalib_df):\n",
    "        y_mean = recalib_df['y_pred'].values\n",
    "        y_std = recalib_df['y_std'].values\n",
    "        y_true = recalib_df['y_true'].values\n",
    "        std_ratio = uct.recalibration.optimize_recalibration_ratio(\n",
    "            y_mean, y_std, y_true, criterion=\"miscal\")\n",
    "        df['y_std_recalib'] = df['y_std'] * std_ratio\n",
    "        return df\n",
    "\n",
    "    def _format_predict_df(self, results,\n",
    "            test_df=None, esb_yp=None, recalib_df=None):\n",
    "        \"\"\"\n",
    "        results: dict with keys y_pred, y_true, y_var\n",
    "        \"\"\"\n",
    "        df = self.task_df['test'].copy() if test_df is None else test_df.copy()\n",
    "        assert np.allclose(results['y_true'], df['y'].values)\n",
    "        df = df.rename(columns={'y': 'y_true'})\n",
    "        df['y_pred'] = results['y_pred']\n",
    "        if esb_yp is not None:\n",
    "            if self.uncertainty:\n",
    "                df['y_std'] = np.std(esb_yp, axis=0)\n",
    "                if recalib_df is not None:\n",
    "                    df = self.recalibrate_std(df, recalib_df)\n",
    "            for i in range(self.n_ensembles):\n",
    "                df[f'y_pred_{i + 1}'] = esb_yp[i]\n",
    "        return df\n",
    "\n",
    "    def train(self, n_epochs=None, patience=None,\n",
    "                eval_freq=1, test_freq=None,\n",
    "                monitoring_score='pearson',\n",
    "                train_data=None, valid_data=None,                \n",
    "                rebuild_model=False,\n",
    "                test_after_train=False):\n",
    "        n_epochs = n_epochs or self.n_epochs\n",
    "        if rebuild_model:\n",
    "            self.build_model()\n",
    "        tl, vl = self.task_loader['train'], self.task_loader['valid']\n",
    "        rets_list = []\n",
    "        for i in range(self.n_ensembles):\n",
    "            stp = EarlyStopping(eval_freq=eval_freq, patience=patience,\n",
    "                                    higher_better=(monitoring_score != 'mse'))\n",
    "            rets = dict(\n",
    "                midx = i + 1,\n",
    "                model = self.models[i],\n",
    "                optimizer = self.optimizers[i],\n",
    "                device = self.devices[i],\n",
    "                train_loader = tl,\n",
    "                valid_loader = vl,\n",
    "                stopper = stp,\n",
    "                best_model_state_dict = None,\n",
    "            )\n",
    "            rets_list.append(rets)\n",
    "\n",
    "        rets_list = Parallel(n_jobs=(self.n_ensembles if self.parallel else 1), prefer=\"threads\")(\n",
    "            delayed(_parallel_train_per_epoch)(\n",
    "                kwargs=rets_list[i],\n",
    "                test_loader=self.task_loader['test'],\n",
    "                n_epochs=n_epochs, eval_freq=eval_freq, test_freq=test_freq,\n",
    "                monitoring_score=monitoring_score,\n",
    "                loss_fn=self.criterion, logger=self.logger,\n",
    "                test_after_train=test_after_train,\n",
    "            ) for i in range(self.n_ensembles))\n",
    "\n",
    "        for i, rets in enumerate(rets_list):\n",
    "            self.models[rets['midx'] - 1] = rets['model']\n",
    "\n",
    "\n",
    "    def test(self, test_model=None, test_loader=None,\n",
    "                test_data=None, test_df=None,\n",
    "                recalib_df=None,\n",
    "                save_prediction=False, save_df_name='prediction.tsv',\n",
    "                test_tag=None, print_log=False):\n",
    "        test_models = self.models if test_model is None else [test_model]\n",
    "        if test_data is not None:\n",
    "            assert test_df is not None, 'test_df must be provided if test_data used'\n",
    "            test_loader = self._get_data_loader(test_data)\n",
    "        elif test_loader is not None:\n",
    "            assert test_df is not None, 'test_df must be provided if test_loader used'\n",
    "        else:\n",
    "            test_loader = self.task_loader['test']\n",
    "        rets_list = []\n",
    "        for i, model in enumerate(test_models):\n",
    "            rets = _parallel_test(\n",
    "                kwargs={\n",
    "                    'midx': i + 1,\n",
    "                    'model': model,\n",
    "                    'test_loader': test_loader,\n",
    "                    'device': self.devices[i],\n",
    "                },\n",
    "                loss_fn=self.criterion,\n",
    "                test_tag=f\"Model {i+1}\", print_log=True, logger=self.logger\n",
    "            )\n",
    "            rets_list.append(rets)\n",
    "\n",
    "\n",
    "        esb_yp, esb_loss = None, 0\n",
    "        for rets in rets_list:\n",
    "            esb_yp = rets['y_pred'].reshape(1, -1) if esb_yp is None else\\\n",
    "                np.vstack((esb_yp, rets['y_pred'].reshape(1, -1)))\n",
    "            esb_loss += rets['loss']\n",
    "\n",
    "        y_true = rets['y_true']\n",
    "        y_pred = np.mean(esb_yp, axis=0)\n",
    "        esb_loss /= len(test_models)\n",
    "        results = {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "            'loss': esb_loss,\n",
    "        }\n",
    "\n",
    "        eval_metrics = evaluation_metrics(\n",
    "            y_true, y_pred,\n",
    "            eval_metrics=['mse', 'spearman', 'pearson']\n",
    "        )\n",
    "        results['metrics'] = eval_metrics\n",
    "        results['df'] = self._format_predict_df(results,\n",
    "            test_df=test_df, esb_yp=esb_yp, recalib_df=recalib_df)\n",
    "        if save_prediction:\n",
    "            self.saver.save_df(results['df'], save_df_name, float_format='%g')\n",
    "        if print_log:\n",
    "            self.logger.info(f\"{test_tag} | Test Loss: {results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in results['metrics'].items()]))\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fec765d-8b35-48a5-9f33-860a70b44aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d227b1-aeb1-4b03-8792-e3f8fb043952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Geometric Vector Perceptrons\n",
    "From: https://github.com/drorlab/gvp-pytorch/blob/82af6b22eaf8311c15733117b0071408d24ed877/gvp/__init__.py\n",
    "\"\"\"\n",
    "import torch, functools\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "def tuple_sum(*args):\n",
    "    '''\n",
    "    Sums any number of tuples (s, V) elementwise.\n",
    "    '''\n",
    "    return tuple(map(sum, zip(*args)))\n",
    "\n",
    "def tuple_cat(*args, dim=-1):\n",
    "    '''\n",
    "    Concatenates any number of tuples (s, V) elementwise.\n",
    "    \n",
    "    :param dim: dimension along which to concatenate when viewed\n",
    "                as the `dim` index for the scalar-channel tensors.\n",
    "                This means that `dim=-1` will be applied as\n",
    "                `dim=-2` for the vector-channel tensors.\n",
    "    '''\n",
    "    dim %= len(args[0][0].shape)\n",
    "    s_args, v_args = list(zip(*args))\n",
    "    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n",
    "\n",
    "def tuple_index(x, idx):\n",
    "    '''\n",
    "    Indexes into a tuple (s, V) along the first dimension.\n",
    "    \n",
    "    :param idx: any object which can be used to index into a `torch.Tensor`\n",
    "    '''\n",
    "    return x[0][idx], x[1][idx]\n",
    "\n",
    "def randn(n, dims, device=\"cpu\"):\n",
    "    '''\n",
    "    Returns random tuples (s, V) drawn elementwise from a normal distribution.\n",
    "    \n",
    "    :param n: number of data points\n",
    "    :param dims: tuple of dimensions (n_scalar, n_vector)\n",
    "    \n",
    "    :return: (s, V) with s.shape = (n, n_scalar) and\n",
    "             V.shape = (n, n_vector, 3)\n",
    "    '''\n",
    "    return torch.randn(n, dims[0], device=device), \\\n",
    "            torch.randn(n, dims[1], 3, device=device)\n",
    "\n",
    "def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n",
    "    '''\n",
    "    L2 norm of tensor clamped above a minimum value `eps`.\n",
    "    \n",
    "    :param sqrt: if `False`, returns the square of the L2 norm\n",
    "    '''\n",
    "    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n",
    "    return torch.sqrt(out) if sqrt else out\n",
    "\n",
    "def _split(x, nv):\n",
    "    '''\n",
    "    Splits a merged representation of (s, V) back into a tuple. \n",
    "    Should be used only with `_merge(s, V)` and only if the tuple \n",
    "    representation cannot be used.\n",
    "    \n",
    "    :param x: the `torch.Tensor` returned from `_merge`\n",
    "    :param nv: the number of vector channels in the input to `_merge`\n",
    "    '''\n",
    "    v = torch.reshape(x[..., -3*nv:], x.shape[:-1] + (nv, 3))\n",
    "    s = x[..., :-3*nv]\n",
    "    return s, v\n",
    "\n",
    "def _merge(s, v):\n",
    "    '''\n",
    "    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n",
    "    vector channels are flattened and appended to the scalar channels.\n",
    "    Should be used only if the tuple representation cannot be used.\n",
    "    Use `_split(x, nv)` to reverse.\n",
    "    '''\n",
    "    v = torch.reshape(v, v.shape[:-2] + (3*v.shape[-2],))\n",
    "    return torch.cat([s, v], -1)\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    '''\n",
    "    Geometric Vector Perceptron. See manuscript and README.md\n",
    "    for more details.\n",
    "    \n",
    "    :param in_dims: tuple (n_scalar, n_vector)\n",
    "    :param out_dims: tuple (n_scalar, n_vector)\n",
    "    :param h_dim: intermediate number of vector channels, optional\n",
    "    :param activations: tuple of functions (scalar_act, vector_act)\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, h_dim=None,\n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVP, self).__init__()\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.vector_gate = vector_gate\n",
    "        if self.vi: \n",
    "            self.h_dim = h_dim or max(self.vi, self.vo) \n",
    "            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n",
    "            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n",
    "            if self.vo:\n",
    "                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n",
    "                if self.vector_gate: self.wsv = nn.Linear(self.so, self.vo)\n",
    "        else:\n",
    "            self.ws = nn.Linear(self.si, self.so)\n",
    "        \n",
    "        self.scalar_act, self.vector_act = activations\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`, \n",
    "                  or (if vectors_in is 0), a single `torch.Tensor`\n",
    "        :return: tuple (s, V) of `torch.Tensor`,\n",
    "                 or (if vectors_out is 0), a single `torch.Tensor`\n",
    "        '''\n",
    "        if self.vi:\n",
    "            s, v = x\n",
    "            v = torch.transpose(v, -1, -2)\n",
    "            vh = self.wh(v)    \n",
    "            vn = _norm_no_nan(vh, axis=-2)\n",
    "            s = self.ws(torch.cat([s, vn], -1))\n",
    "            if self.vo: \n",
    "                v = self.wv(vh) \n",
    "                v = torch.transpose(v, -1, -2)\n",
    "                if self.vector_gate: \n",
    "                    if self.vector_act:\n",
    "                        gate = self.wsv(self.vector_act(s))\n",
    "                    else:\n",
    "                        gate = self.wsv(s)\n",
    "                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n",
    "                elif self.vector_act:\n",
    "                    v = v * self.vector_act(\n",
    "                        _norm_no_nan(v, axis=-1, keepdims=True))\n",
    "        else:\n",
    "            s = self.ws(x)\n",
    "            if self.vo:\n",
    "                v = torch.zeros(s.shape[0], self.vo, 3,\n",
    "                                device=self.dummy_param.device)\n",
    "        if self.scalar_act:\n",
    "            s = self.scalar_act(s)\n",
    "        \n",
    "        return (s, v) if self.vo else s\n",
    "\n",
    "class _VDropout(nn.Module):\n",
    "    '''\n",
    "    Vector channel dropout where the elements of each\n",
    "    vector channel are dropped together.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(_VDropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: `torch.Tensor` corresponding to vector channels\n",
    "        '''\n",
    "        device = self.dummy_param.device\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = torch.bernoulli(\n",
    "            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n",
    "        ).unsqueeze(-1)\n",
    "        x = mask * x / (1 - self.drop_rate)\n",
    "        return x\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    '''\n",
    "    Combined dropout for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.sdropout = nn.Dropout(drop_rate)\n",
    "        self.vdropout = _VDropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if type(x) is torch.Tensor:\n",
    "            return self.sdropout(x)\n",
    "        s, v = x\n",
    "        return self.sdropout(s), self.vdropout(v)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Combined LayerNorm for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, dims):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.s, self.v = dims\n",
    "        self.scalar_norm = nn.LayerNorm(self.s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if not self.v:\n",
    "            return self.scalar_norm(x)\n",
    "        s, v = x\n",
    "        vn = _norm_no_nan(v, axis=-1, keepdims=True, sqrt=False)\n",
    "        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True))\n",
    "        return self.scalar_norm(s), v / vn\n",
    "\n",
    "class GVPConv(MessagePassing):\n",
    "    '''\n",
    "    Graph convolution / message passing with Geometric Vector Perceptrons.\n",
    "    Takes in a graph with node and edge embeddings,\n",
    "    and returns new node embeddings.\n",
    "    \n",
    "    This does NOT do residual updates and pointwise feedforward layers\n",
    "    ---see `GVPConvLayer`.\n",
    "    \n",
    "    :param in_dims: input node embedding dimensions (n_scalar, n_vector)\n",
    "    :param out_dims: output node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_layers: number of GVPs in the message function\n",
    "    :param module_list: preconstructed message function, overrides n_layers\n",
    "    :param aggr: should be \"add\" if some incoming edges are masked, as in\n",
    "                 a masked autoregressive decoder architecture, otherwise \"mean\"\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, edge_dims,\n",
    "                 n_layers=3, module_list=None, aggr=\"mean\", \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVPConv, self).__init__(aggr=aggr)\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.se, self.ve = edge_dims\n",
    "        \n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        \n",
    "        module_list = module_list or []\n",
    "        if not module_list:\n",
    "            if n_layers == 1:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), \n",
    "                        (self.so, self.vo), activations=(None, None)))\n",
    "            else:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), out_dims)\n",
    "                )\n",
    "                for i in range(n_layers - 2):\n",
    "                    module_list.append(GVP_(out_dims, out_dims))\n",
    "                module_list.append(GVP_(out_dims, out_dims,\n",
    "                                       activations=(None, None)))\n",
    "        self.message_func = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        '''\n",
    "        x_s, x_v = x\n",
    "        message = self.propagate(edge_index, \n",
    "                    s=x_s, v=x_v.reshape(x_v.shape[0], 3*x_v.shape[1]),\n",
    "                    edge_attr=edge_attr)\n",
    "        return _split(message, self.vo) \n",
    "\n",
    "    def message(self, s_i, v_i, s_j, v_j, edge_attr):\n",
    "        v_j = v_j.view(v_j.shape[0], v_j.shape[1]//3, 3)\n",
    "        v_i = v_i.view(v_i.shape[0], v_i.shape[1]//3, 3)\n",
    "        message = tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n",
    "        message = self.message_func(message)\n",
    "        return _merge(*message)\n",
    "\n",
    "\n",
    "class GVPConvLayer(nn.Module):\n",
    "    '''\n",
    "    Full graph convolution / message passing layer with \n",
    "    Geometric Vector Perceptrons. Residually updates node embeddings with\n",
    "    aggregated incoming messages, applies a pointwise feedforward \n",
    "    network to node embeddings, and returns updated node embeddings.\n",
    "    \n",
    "    To only compute the aggregated messages, see `GVPConv`.\n",
    "    \n",
    "    :param node_dims: node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_message: number of GVPs to use in message function\n",
    "    :param n_feedforward: number of GVPs to use in feedforward function\n",
    "    :param drop_rate: drop probability in all dropout layers\n",
    "    :param autoregressive: if `True`, this `GVPConvLayer` will be used\n",
    "           with a different set of input node embeddings for messages\n",
    "           where src >= dst\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, node_dims, edge_dims,\n",
    "                 n_message=3, n_feedforward=2, drop_rate=.1,\n",
    "                 autoregressive=False, \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        \n",
    "        super(GVPConvLayer, self).__init__()\n",
    "        self.conv = GVPConv(node_dims, node_dims, edge_dims, n_message,\n",
    "                           aggr=\"add\" if autoregressive else \"mean\",\n",
    "                           activations=activations, vector_gate=vector_gate)\n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        self.norm = nn.ModuleList([LayerNorm(node_dims) for _ in range(2)])\n",
    "        self.dropout = nn.ModuleList([Dropout(drop_rate) for _ in range(2)])\n",
    "\n",
    "        ff_func = []\n",
    "        if n_feedforward == 1:\n",
    "            ff_func.append(GVP_(node_dims, node_dims, activations=(None, None)))\n",
    "        else:\n",
    "            hid_dims = 4*node_dims[0], 2*node_dims[1]\n",
    "            ff_func.append(GVP_(node_dims, hid_dims))\n",
    "            for i in range(n_feedforward-2):\n",
    "                ff_func.append(GVP_(hid_dims, hid_dims))\n",
    "            ff_func.append(GVP_(hid_dims, node_dims, activations=(None, None)))\n",
    "        self.ff_func = nn.Sequential(*ff_func)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr,\n",
    "                autoregressive_x=None, node_mask=None):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. \n",
    "                If not `None`, will be used as src node embeddings\n",
    "                for forming messages where src >= dst. The corrent node \n",
    "                embeddings `x` will still be the base of the update and the \n",
    "                pointwise feedforward.\n",
    "        :param node_mask: array of type `bool` to index into the first\n",
    "                dim of node embeddings (s, V). If not `None`, only\n",
    "                these nodes will be updated.\n",
    "        '''\n",
    "        \n",
    "        if autoregressive_x is not None:\n",
    "            src, dst = edge_index\n",
    "            mask = src < dst\n",
    "            edge_index_forward = edge_index[:, mask]\n",
    "            edge_index_backward = edge_index[:, ~mask]\n",
    "            edge_attr_forward = tuple_index(edge_attr, mask)\n",
    "            edge_attr_backward = tuple_index(edge_attr, ~mask)\n",
    "            \n",
    "            dh = tuple_sum(\n",
    "                self.conv(x, edge_index_forward, edge_attr_forward),\n",
    "                self.conv(autoregressive_x, edge_index_backward, edge_attr_backward)\n",
    "            )\n",
    "            \n",
    "            count = scatter_add(torch.ones_like(dst), dst,\n",
    "                        dim_size=dh[0].size(0)).clamp(min=1).unsqueeze(-1)\n",
    "            \n",
    "            dh = dh[0] / count, dh[1] / count.unsqueeze(-1)\n",
    "\n",
    "        else:\n",
    "            dh = self.conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_ = x\n",
    "            x, dh = tuple_index(x, node_mask), tuple_index(dh, node_mask)\n",
    "            \n",
    "        x = self.norm[0](tuple_sum(x, self.dropout[0](dh)))\n",
    "        \n",
    "        dh = self.ff_func(x)\n",
    "        x = self.norm[1](tuple_sum(x, self.dropout[1](dh)))\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_[0][node_mask], x_[1][node_mask] = x[0], x[1]\n",
    "            x = x_\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1d98cc-d70d-403d-a4c7-d42853242cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d7ae00-8629-4b44-96cb-86a98b4a89a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def eval_mse(y_true, y_pred, squared=True):\n",
    "    \"\"\"Evaluate mse/rmse and return the results.\n",
    "    squared: bool, default=True\n",
    "        If True returns MSE value, if False returns RMSE value.\n",
    "    \"\"\"\n",
    "    return metrics.mean_squared_error(y_true, y_pred, squared=squared)\n",
    "\n",
    "def eval_pearson(y_true, y_pred):\n",
    "    \"\"\"Evaluate Pearson correlation and return the results.\"\"\"\n",
    "    return stats.pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def eval_spearman(y_true, y_pred):\n",
    "    \"\"\"Evaluate Spearman correlation and return the results.\"\"\"\n",
    "    return stats.spearmanr(y_true, y_pred)[0]\n",
    "\n",
    "def eval_r2(y_true, y_pred):\n",
    "    \"\"\"Evaluate R2 and return the results.\"\"\"\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "def eval_auroc(y_true, y_pred):\n",
    "    \"\"\"Evaluate AUROC and return the results.\"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_pred)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def eval_auprc(y_true, y_pred):\n",
    "    \"\"\"Evaluate AUPRC and return the results.\"\"\"\n",
    "    pre, rec, _ = metrics.precision_recall_curve(y_true, y_pred)\n",
    "    return metrics.auc(rec, pre)\n",
    "\n",
    "\n",
    "def evaluation_metrics(y_true=None, y_pred=None,\n",
    "\t\teval_metrics=[]):\n",
    "    \"\"\"Evaluate eval_metrics and return the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: true labels\n",
    "    y_pred: predicted labels\n",
    "    eval_metrics: a list of evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for m in eval_metrics:\n",
    "        if m == 'mse':\n",
    "            s = eval_mse(y_true, y_pred, squared=True)\n",
    "        elif m == 'rmse':\n",
    "            s = eval_mse(y_true, y_pred, squared=False)\n",
    "        elif m == 'pearson':\n",
    "            s = eval_pearson(y_true, y_pred)\n",
    "        elif m == 'spearman':\n",
    "            s = eval_spearman(y_true, y_pred)\n",
    "        elif m == 'r2':\n",
    "            s = eval_r2(y_true, y_pred)\n",
    "        elif m == 'auroc':\n",
    "            s = eval_auroc(y_true, y_pred)\n",
    "        elif m == 'auprc':\n",
    "            s = eval_auprc(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError('Unknown evaluation metric: {}'.format(m))\n",
    "        results[m] = s        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ca458d-a138-46f4-9c00-1036e5c83d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc134f2c-52cd-4079-89de-d4113b745fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "\n",
    "class Prot3DGraphModel(nn.Module):\n",
    "    def __init__(self,\n",
    "        d_vocab=21, d_embed=20,\n",
    "        d_dihedrals=6, d_pretrained_emb=1280, d_edge=39,\n",
    "        d_gcn=[128, 256, 256],\n",
    "    ):\n",
    "        super(Prot3DGraphModel, self).__init__()\n",
    "        d_gcn_in = d_gcn[0]\n",
    "        self.embed = nn.Embedding(d_vocab, d_embed)\n",
    "        self.proj_node = nn.Linear(d_embed + d_dihedrals + d_pretrained_emb, d_gcn_in)\n",
    "        self.proj_edge = nn.Linear(d_edge, d_gcn_in)\n",
    "        gcn_layer_sizes = [d_gcn_in] + d_gcn\n",
    "        layers = []\n",
    "        for i in range(len(gcn_layer_sizes) - 1):            \n",
    "            layers.append((\n",
    "                torch_geometric.nn.TransformerConv(\n",
    "                    gcn_layer_sizes[i], gcn_layer_sizes[i + 1], edge_dim=d_gcn_in),\n",
    "                'x, edge_index, edge_attr -> x'\n",
    "            ))            \n",
    "            layers.append(nn.LeakyReLU())            \n",
    "        \n",
    "        self.gcn = torch_geometric.nn.Sequential(\n",
    "            'x, edge_index, edge_attr', layers)        \n",
    "        self.pool = torch_geometric.nn.global_mean_pool\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.seq, data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = self.embed(x)\n",
    "        s = data.node_s\n",
    "        emb = data.seq_emb\n",
    "        x = torch.cat([x, s, emb], dim=-1)\n",
    "\n",
    "        edge_attr = data.edge_s\n",
    "\n",
    "        x = self.proj_node(x)\n",
    "        edge_attr = self.proj_edge(edge_attr)\n",
    "\n",
    "        x = self.gcn(x, edge_index, edge_attr)\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DrugGVPModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        node_in_dim=[66, 1], node_h_dim=[128, 64],\n",
    "        edge_in_dim=[16, 1], edge_h_dim=[32, 1],\n",
    "        num_layers=3, drop_rate=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_in_dim : list of int\n",
    "            Input dimension of drug node features (si, vi).\n",
    "            Scalar node feartures have shape (N, si).\n",
    "            Vector node features have shape (N, vi, 3).\n",
    "        node_h_dims : list of int\n",
    "            Hidden dimension of drug node features (so, vo).\n",
    "            Scalar node feartures have shape (N, so).\n",
    "            Vector node features have shape (N, vo, 3).\n",
    "        \"\"\"\n",
    "        super(DrugGVPModel, self).__init__()\n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm(node_in_dim),\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None))\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm(edge_in_dim),\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate)\n",
    "            for _ in range(num_layers))\n",
    "\n",
    "        ns, _ = node_h_dim\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(node_h_dim),\n",
    "            GVP(node_h_dim, (ns, 0)))\n",
    "\n",
    "    def forward(self, xd):\n",
    "        # Unpack input data\n",
    "        h_V = (xd.node_s, xd.node_v)\n",
    "        h_E = (xd.edge_s, xd.edge_v)\n",
    "        edge_index = xd.edge_index\n",
    "        batch = xd.batch\n",
    "\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        out = self.W_out(h_V)\n",
    "\n",
    "        # per-graph mean\n",
    "        out = torch_geometric.nn.global_add_pool(out, batch)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DTAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=[128, 256, 256],\n",
    "            prot_fc_dims=[1024, 128],\n",
    "            drug_node_in_dim=[66, 1], drug_node_h_dims=[128, 64],\n",
    "            drug_edge_in_dim=[16, 1], drug_edge_h_dims=[32, 1],            \n",
    "            drug_fc_dims=[1024, 128],\n",
    "            mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super(DTAModel, self).__init__()\n",
    "\n",
    "        self.drug_model = DrugGVPModel(\n",
    "            node_in_dim=drug_node_in_dim, node_h_dim=drug_node_h_dims,\n",
    "            edge_in_dim=drug_edge_in_dim, edge_h_dim=drug_edge_h_dims,\n",
    "        )\n",
    "        drug_emb_dim = drug_node_h_dims[0]\n",
    "\n",
    "        self.prot_model = Prot3DGraphModel(\n",
    "            d_pretrained_emb=prot_emb_dim, d_gcn=prot_gcn_dims\n",
    "        )\n",
    "        prot_emb_dim = prot_gcn_dims[-1]\n",
    "\n",
    "        self.drug_fc = self.get_fc_layers(\n",
    "            [drug_emb_dim] + drug_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "       \n",
    "        self.prot_fc = self.get_fc_layers(\n",
    "            [prot_emb_dim] + prot_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "\n",
    "        self.top_fc = self.get_fc_layers(\n",
    "            [drug_fc_dims[-1] + prot_fc_dims[-1]] + mlp_dims + [1],\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "\n",
    "    def get_fc_layers(self, hidden_sizes,\n",
    "            dropout=0, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True):\n",
    "        act_fn = torch.nn.LeakyReLU()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:])):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if not no_last_activation or i != len(hidden_sizes) - 2:\n",
    "                layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                if not no_last_dropout or i != len(hidden_sizes) - 2:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            if batchnorm and i != len(hidden_sizes) - 2:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xd, xp):\n",
    "        xd = self.drug_model(xd)\n",
    "        xp = self.prot_model(xp)\n",
    "\n",
    "        xd = self.drug_fc(xd)\n",
    "        xp = self.prot_fc(xp)\n",
    "\n",
    "        x = torch.cat([xd, xp], dim=1)\n",
    "        x = self.top_fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ea2858-a77c-4602-9326-5a73895f5c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mol_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fad8eed-07c4-4fc8-912e-718abd457cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rdkit \n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_geometric\n",
    "import torch_cluster\n",
    "\n",
    "\n",
    "\n",
    "def onehot_encoder(a=None, alphabet=None, default=None, drop_first=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: array of numerical value of categorical feature classes.\n",
    "    alphabet: valid values of feature classes.\n",
    "    default: default class if out of alphabet.\n",
    "    Returns\n",
    "    -------\n",
    "    A 2-D one-hot array with size |x| * |alphabet|\n",
    "    '''\n",
    "    # replace out-of-vocabulary classes\n",
    "    alphabet_set = set(alphabet)\n",
    "    a = [x if x in alphabet_set else default for x in a]\n",
    "\n",
    "    # cast to category to force class not present\n",
    "    a = pd.Categorical(a, categories=alphabet)\n",
    "\n",
    "    onehot = pd.get_dummies(pd.Series(a), columns=alphabet, drop_first=drop_first)\n",
    "    return onehot.values\n",
    "\n",
    "\n",
    "def _build_atom_feature(mol):\n",
    "    # dim: 44 + 7 + 7 + 7 + 1\n",
    "    feature_alphabet = {\n",
    "        # (alphabet, default value)\n",
    "        'GetSymbol': (ATOM_VOCAB, 'unk'),\n",
    "        'GetDegree': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetTotalNumHs': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetImplicitValence': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetIsAromatic': ([0, 1], 1)\n",
    "    }\n",
    "\n",
    "    atom_feature = None\n",
    "    for attr in ['GetSymbol', 'GetDegree', 'GetTotalNumHs',\n",
    "                'GetImplicitValence', 'GetIsAromatic']:\n",
    "        feature = [getattr(atom, attr)() for atom in mol.GetAtoms()]\n",
    "        feature = onehot_encoder(feature,\n",
    "                    alphabet=feature_alphabet[attr][0],\n",
    "                    default=feature_alphabet[attr][1],\n",
    "                    drop_first=(attr in ['GetIsAromatic']) # binary-class feature\n",
    "                )\n",
    "        atom_feature = feature if atom_feature is None else np.concatenate((atom_feature, feature), axis=1)\n",
    "    atom_feature = atom_feature.astype(np.float32)\n",
    "    return atom_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _build_edge_feature(coords, edge_index, D_max=4.5, num_rbf=16):\n",
    "    E_vectors = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), D_max=D_max, D_count=num_rbf)\n",
    "\n",
    "    edge_s = rbf\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "    edge_s, edge_v = map(torch.nan_to_num, (edge_s, edge_v))\n",
    "\n",
    "    return edge_s, edge_v\n",
    "\n",
    "\n",
    "def sdf_to_graphs(data_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list: dict, drug key -> sdf file path\n",
    "    Returns\n",
    "    -------\n",
    "    graphs : dict\n",
    "        A list of torch_geometric graphs. drug key -> graph\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for key, sdf_path in tqdm(data_list.items(), desc='sdf'):\n",
    "        graphs[key] = featurize_drug(sdf_path, name=key)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def featurize_drug(sdf_path, name=None, edge_cutoff=4.5, num_rbf=16):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_path: str\n",
    "        Path to sdf file\n",
    "    name: str\n",
    "        Name of drug\n",
    "    Returns\n",
    "    -------\n",
    "    graph: torch_geometric.data.Data\n",
    "        A torch_geometric graph\n",
    "    \"\"\"\n",
    "    mol = rdkit.Chem.MolFromMolFile(sdf_path)\n",
    "    conf = mol.GetConformer()\n",
    "    with torch.no_grad():\n",
    "        coords = conf.GetPositions()\n",
    "        coords = torch.as_tensor(coords, dtype=torch.float32)\n",
    "        atom_feature = _build_atom_feature(mol)\n",
    "        atom_feature = torch.as_tensor(atom_feature, dtype=torch.float32)\n",
    "        edge_index = torch_cluster.radius_graph(coords, r=edge_cutoff)\n",
    "\n",
    "    node_s = atom_feature\n",
    "    node_v = coords.unsqueeze(1)\n",
    "    # edge_v, edge_index = _build_edge_feature(mol)\n",
    "    edge_s, edge_v = _build_edge_feature(\n",
    "        coords, edge_index, D_max=edge_cutoff, num_rbf=num_rbf)\n",
    "\n",
    "    data = torch_geometric.data.Data(\n",
    "        x=coords, edge_index=edge_index, name=name,\n",
    "        node_v=node_v, node_s=node_s, edge_v=edge_v, edge_s=edge_s)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58102e29-ffe6-4f61-8133-5e060bdad409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46c657dd-d7c1-41c5-81a6-7ea0361ee12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_train_args(parser):\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--task', help='Task name')\n",
    "    parser.add_argument('--split_method', default='random',\n",
    "        choices=['random', 'protein', 'drug', 'both', 'seqid'],\n",
    "        help='Split method: random, protein, drug, or both')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "        help='Random Seed')\n",
    "\n",
    "    # Data representation parameters\n",
    "    parser.add_argument('--contact_cutoff', type=float, default=8.,\n",
    "        help='cutoff of C-alpha distance to define protein contact graph')\n",
    "    parser.add_argument('--num_pos_emb', type=int, default=16,\n",
    "        help='number of positional embeddings')\n",
    "    parser.add_argument('--num_rbf', type=int, default=16,\n",
    "        help='number of RBF kernels')\n",
    "\n",
    "    # Protein model parameters\n",
    "    parser.add_argument('--prot_gcn_dims', type=int, nargs='+', default=[128, 256, 256],\n",
    "        help='protein GCN layers dimensions')\n",
    "    parser.add_argument('--prot_fc_dims', type=int, nargs='+', default=[1024, 128],\n",
    "        help='protein FC layers dimensions')\n",
    "\n",
    "    # Drug model parameters\n",
    "    parser.add_argument('--drug_gcn_dims', type=int, nargs='+', default=[128, 64],\n",
    "        help='drug GVP hidden layers dimensions')\n",
    "    parser.add_argument('--drug_fc_dims', type=int, nargs='+', default=[1024, 128],\n",
    "        help='drug FC layers dimensions')\n",
    "\n",
    "    # Top model parameters\n",
    "    parser.add_argument('--mlp_dims', type=int, nargs='+', default=[1024, 512],\n",
    "        help='top MLP layers dimensions')\n",
    "    parser.add_argument('--mlp_dropout', type=float, default=0.25,\n",
    "        help='dropout rate in top MLP')\n",
    "\n",
    "    # uncertainty parameters\n",
    "    parser.add_argument('--uncertainty', action='store_true',\n",
    "        help='estimate uncertainty')\n",
    "    parser.add_argument('--recalibrate', action='store_true',\n",
    "        help='recalibrate uncertainty')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--n_ensembles', type=int, default=1,\n",
    "        help='number of ensembles')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "        help='batch size')\n",
    "    parser.add_argument('--n_epochs', type=int, default=500,\n",
    "        help='number of epochs')\n",
    "    parser.add_argument('--patience', action='store', type=int,\n",
    "        help='patience for early stopping')\n",
    "    parser.add_argument('--eval_freq', type=int, default=1,\n",
    "        help='evaluation frequency')\n",
    "    parser.add_argument('--test_freq', type=int,\n",
    "        help='test frequency')\n",
    "    parser.add_argument('--lr', type=float, default=0.0005,\n",
    "        help='learning rate')\n",
    "    parser.add_argument('--monitor_metric', default='pearson',\n",
    "        help='validation metric to monitor for deciding best checkpoint')\n",
    "    parser.add_argument('--parallel', action='store_true',\n",
    "        help='run ensembles in parallel on multiple GPUs')\n",
    "\n",
    "    # Save parameters\n",
    "    parser.add_argument('--output_dir', action='store', default='../output', help='output folder')\n",
    "    parser.add_argument('--save_log', action='store_true', default=False, help='save log file')\n",
    "    parser.add_argument('--save_checkpoint', action='store_true', default=False, help='save checkpoint')\n",
    "    parser.add_argument('--save_prediction', action='store_true', default=False, help='save prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aafb1bb0-95bc-4f7a-9009-1980ebc167ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pdb_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2dc096f-9f21-4ae4-91f8-f6760fbeff24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from\n",
    "https://github.com/jingraham/neurips19-graph-protein-design\n",
    "https://github.com/drorlab/gvp-pytorch\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_cluster\n",
    "\n",
    "\n",
    "def pdb_to_graphs(prot_data, params):\n",
    "    \"\"\"\n",
    "    Converts a list of protein dict to a list of torch_geometric graphs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prot_data : dict\n",
    "        A list of protein data dict. see format in `featurize_protein_graph()`.\n",
    "    params : dict\n",
    "        A dictionary of parameters defined in `featurize_protein_graph()`.\n",
    "    Returns\n",
    "    -------\n",
    "    graphs : dict\n",
    "        A list of torch_geometric graphs. protein key -> graph\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for key, struct in tqdm(prot_data.items(), desc='pdb'):\n",
    "        graphs[key] = featurize_protein_graph(\n",
    "            struct, name=key, **params)\n",
    "    return graphs\n",
    "\n",
    "def featurize_protein_graph(\n",
    "        protein, name=None,\n",
    "        num_pos_emb=16, num_rbf=16,        \n",
    "        contact_cutoff=8.,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters: see comments of DTATask() in dta.py\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        coords = torch.as_tensor(protein['coords'], dtype=torch.float32)\n",
    "        seq = torch.as_tensor([LETTER_TO_NUM[a] for a in protein['seq']], dtype=torch.long)        \n",
    "        seq_emb = torch.load(protein['embed'])\n",
    "\n",
    "        mask = torch.isfinite(coords.sum(dim=(1,2)))\n",
    "        coords[~mask] = np.inf\n",
    "\n",
    "        X_ca = coords[:, 1]        \n",
    "        ca_mask = torch.isfinite(X_ca.sum(dim=(1)))\n",
    "        ca_mask = ca_mask.float()\n",
    "        ca_mask_2D = torch.unsqueeze(ca_mask, 0) * torch.unsqueeze(ca_mask, 1)\n",
    "        dX_ca = torch.unsqueeze(X_ca, 0) - torch.unsqueeze(X_ca, 1)\n",
    "        D_ca = ca_mask_2D * torch.sqrt(torch.sum(dX_ca**2, 2) + 1e-6)\n",
    "        edge_index = torch.nonzero((D_ca < contact_cutoff) & (ca_mask_2D == 1))\n",
    "        edge_index = edge_index.t().contiguous()\n",
    "        \n",
    "\n",
    "        O_feature = _local_frame(X_ca, edge_index)\n",
    "        pos_embeddings = _positional_embeddings(edge_index, num_embeddings=num_pos_emb)\n",
    "        E_vectors = X_ca[edge_index[0]] - X_ca[edge_index[1]]\n",
    "        rbf = _rbf(E_vectors.norm(dim=-1), D_count=num_rbf)\n",
    "\n",
    "        dihedrals = _dihedrals(coords)\n",
    "        orientations = _orientations(X_ca)\n",
    "        sidechains = _sidechains(coords)\n",
    "\n",
    "        node_s = dihedrals\n",
    "        node_v = torch.cat([orientations, sidechains.unsqueeze(-2)], dim=-2)\n",
    "        edge_s = torch.cat([rbf, O_feature, pos_embeddings], dim=-1)\n",
    "        edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "        node_s, node_v, edge_s, edge_v = map(torch.nan_to_num,\n",
    "                (node_s, node_v, edge_s, edge_v))\n",
    "\n",
    "    data = torch_geometric.data.Data(x=X_ca, seq=seq, name=name,\n",
    "                                        node_s=node_s, node_v=node_v,\n",
    "                                        edge_s=edge_s, edge_v=edge_v,\n",
    "                                        edge_index=edge_index, mask=mask,                                        \n",
    "                                        seq_emb=seq_emb)\n",
    "    return data\n",
    "\n",
    "\n",
    "def _dihedrals(X, eps=1e-7):\n",
    "    X = torch.reshape(X[:, :3], [3 * X.shape[0], 3])\n",
    "    dX = X[1:] - X[:-1]\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_2 = U[:-2]\n",
    "    u_1 = U[1:-1]\n",
    "    u_0 = U[2:]\n",
    "\n",
    "    # Backbone normals\n",
    "    n_2 = _normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "    n_1 = _normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "    # Angle between normals\n",
    "    cosD = torch.sum(n_2 * n_1, -1)\n",
    "    cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n",
    "    D = torch.sign(torch.sum(u_2 * n_1, -1)) * torch.acos(cosD)\n",
    "\n",
    "    # This scheme will remove phi[0], psi[-1], omega[-1]\n",
    "    D = F.pad(D, [1, 2])\n",
    "    D = torch.reshape(D, [-1, 3])\n",
    "    # Lift angle representations to the circle\n",
    "    D_features = torch.cat([torch.cos(D), torch.sin(D)], 1)\n",
    "    return D_features\n",
    "\n",
    "\n",
    "def _positional_embeddings(edge_index,\n",
    "                            num_embeddings=None,\n",
    "                            period_range=[2, 1000]):\n",
    "    d = edge_index[0] - edge_index[1]\n",
    "\n",
    "    frequency = torch.exp(\n",
    "        torch.arange(0, num_embeddings, 2, dtype=torch.float32)\n",
    "        * -(np.log(10000.0) / num_embeddings)\n",
    "    )\n",
    "    angles = d.unsqueeze(-1) * frequency\n",
    "    E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n",
    "    return E\n",
    "\n",
    "\n",
    "def _orientations(X):\n",
    "    forward = _normalize(X[1:] - X[:-1])\n",
    "    backward = _normalize(X[:-1] - X[1:])\n",
    "    forward = F.pad(forward, [0, 0, 0, 1])\n",
    "    backward = F.pad(backward, [0, 0, 1, 0])\n",
    "    return torch.cat([forward.unsqueeze(-2), backward.unsqueeze(-2)], -2)\n",
    "\n",
    "\n",
    "def _sidechains(X):\n",
    "    n, origin, c = X[:, 0], X[:, 1], X[:, 2]\n",
    "    c, n = _normalize(c - origin), _normalize(n - origin)\n",
    "    bisector = _normalize(c + n)\n",
    "    perp = _normalize(torch.cross(c, n))\n",
    "    vec = -bisector * math.sqrt(1 / 3) - perp * math.sqrt(2 / 3)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    '''\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    '''\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n",
    "    '''\n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "\n",
    "def _local_frame(X, edge_index, eps=1e-6):\n",
    "    dX = X[1:] - X[:-1]\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_2 = U[:-2]\n",
    "    u_1 = U[1:-1]\n",
    "    u_0 = U[2:]\n",
    "\n",
    "    # Backbone normals\n",
    "    n_2 = _normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "    n_1 = _normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "    o_1 = _normalize(u_2 - u_1, dim=-1)\n",
    "    O = torch.stack((o_1, n_2, torch.cross(o_1, n_2)), 1)\n",
    "    O = F.pad(O, (0, 0, 0, 0, 1, 2), 'constant', 0)\n",
    "\n",
    "    # dX = X[edge_index[0]] - X[edge_index[1]]\n",
    "    dX = X[edge_index[1]] - X[edge_index[0]]\n",
    "    dX = _normalize(dX, dim=-1)\n",
    "    # dU = torch.bmm(O[edge_index[1]], dX.unsqueeze(2)).squeeze(2)\n",
    "    dU = torch.bmm(O[edge_index[0]], dX.unsqueeze(2)).squeeze(2)\n",
    "    R = torch.bmm(O[edge_index[0]].transpose(-1,-2), O[edge_index[1]])\n",
    "    Q = _quaternions(R)\n",
    "    O_features = torch.cat((dU,Q), dim=-1)\n",
    "\n",
    "    return O_features\n",
    "\n",
    "\n",
    "def _quaternions(R):\n",
    "    # Simple Wikipedia version\n",
    "    # en.wikipedia.org/wiki/Rotation_matrix#Quaternion\n",
    "    # For other options see math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n",
    "    diag = torch.diagonal(R, dim1=-2, dim2=-1)\n",
    "    Rxx, Ryy, Rzz = diag.unbind(-1)\n",
    "    magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n",
    "            Rxx - Ryy - Rzz,\n",
    "        - Rxx + Ryy - Rzz,\n",
    "        - Rxx - Ryy + Rzz\n",
    "    ], -1)))\n",
    "    _R = lambda i,j: R[:, i, j]\n",
    "    signs = torch.sign(torch.stack([\n",
    "        _R(2,1) - _R(1,2),\n",
    "        _R(0,2) - _R(2,0),\n",
    "        _R(1,0) - _R(0,1)\n",
    "    ], -1))\n",
    "    xyz = signs * magnitudes\n",
    "    # The relu enforces a non-negative trace\n",
    "    w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n",
    "    Q = torch.cat((xyz, w), -1)\n",
    "    Q = F.normalize(Q, dim=-1)\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e637af3-15d5-4356-982b-7422397d7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17642161-f1cd-44c2-b2b9-6ee3500778b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "import torch\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, logfile=None, level=logging.INFO):\n",
    "        '''\n",
    "        logfile: pathlib object\n",
    "        '''\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(level)\n",
    "        formatter = logging.Formatter(\"%(asctime)s\\t%(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        for hd in self.logger.handlers[:]:\n",
    "            self.logger.removeHandler(hd)\n",
    "\n",
    "        sh = logging.StreamHandler(sys.stdout)\n",
    "        sh.setFormatter(formatter)\n",
    "        self.logger.addHandler(sh)\n",
    "\n",
    "        if logfile is not None:\n",
    "            logfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "            fh = logging.FileHandler(logfile, 'w')\n",
    "            fh.setFormatter(formatter)\n",
    "            self.logger.addHandler(fh)\n",
    "\n",
    "    def debug(self, msg):\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "    def info(self, msg):\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def warning(self, msg):\n",
    "        self.logger.warning(msg)\n",
    "\n",
    "    def error(self, msg):\n",
    "        self.logger.error(msg)\n",
    "\n",
    "\n",
    "class Saver(object):\n",
    "    def __init__(self, output_dir):        \n",
    "        self.save_dir = pathlib.Path(output_dir)\n",
    "    \n",
    "    def mkdir(self):\n",
    "        self.save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def save_ckp(self, pt, filename='checkpoint.pt'):\n",
    "        self.mkdir()\n",
    "        torch.save(pt, str(self.save_dir/filename))\n",
    "\n",
    "    def save_df(self, df, filename, float_format='%.6f'):\n",
    "        self.mkdir()\n",
    "        df.to_csv(self.save_dir/filename, float_format=float_format, index=False, sep='\\t')\n",
    "    \n",
    "    def save_config(self, config, filename, overwrite=True):\n",
    "        self.mkdir()\n",
    "        with open(self.save_dir/filename, 'w') as f:\n",
    "            yaml.dump(config, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, \n",
    "            patience=100, eval_freq=1, best_score=None, \n",
    "            delta=1e-9, higher_better=True):\n",
    "        self.patience = patience\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_score = best_score\n",
    "        self.delta = delta\n",
    "        self.higher_better = higher_better\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def not_improved(self, val_score):\n",
    "        if np.isnan(val_score):\n",
    "            return True\n",
    "        if self.higher_better:\n",
    "            return val_score < self.best_score + self.delta\n",
    "        else:\n",
    "            return val_score > self.best_score - self.delta\n",
    "    \n",
    "    def update(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            is_best = True\n",
    "        elif self.not_improved(val_score):\n",
    "            self.counter += self.eval_freq\n",
    "            if (self.patience is not None) and (self.counter > self.patience):\n",
    "                self.early_stop = True\n",
    "            is_best = False\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            is_best = True\n",
    "        return is_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbe98214-96b9-4569-a931-200a9526b44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b60bf18-13dc-4ccd-836e-9d488c5645f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_esm_embedding(seq, esm_model):\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = esm_model(**inputs)\n",
    "    token_representations = outputs.last_hidden_state\n",
    "    # remove [CLS] and [EOS] if needed\n",
    "    emb = token_representations[0, 1:-1]\n",
    "    return emb.cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Dataset builder for DTA class\n",
    "def build_dataset(df_fold, pdb_structures, exp_cols = \"pKi\", is_pred = False):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        if is_pred == True:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": 0\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": float(row[exp_cols]),\n",
    "            })\n",
    "    return DTA(df=df_fold, data_list=data_list)\n",
    "\n",
    "\n",
    "def extract_backbone_coords(structure, pdb_id, pdb_path):\n",
    "    coords = {\"N\": [], \"CA\": [], \"C\": [], \"O\": []}\n",
    "    seq = \"\"\n",
    "\n",
    "    model = structure[0]\n",
    "\n",
    "    valid_chain = None\n",
    "    for chain in model:\n",
    "        if any(is_aa(res, standard=True) for res in chain):\n",
    "            valid_chain = chain\n",
    "            break\n",
    "\n",
    "    if valid_chain is None:\n",
    "        print(\"No valid chains: \", pdb_id, pdb_path)\n",
    "        return None, None, None\n",
    "\n",
    "    chain_id = valid_chain.id\n",
    "\n",
    "    for res in valid_chain:\n",
    "        if not is_aa(res, standard=True):\n",
    "            continue\n",
    "        seq += res.resname[0]  # fallback, not exact 1-letter code\n",
    "\n",
    "        for atom_name in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "            if atom_name in res:\n",
    "                coords[atom_name].append(res[atom_name].coord.tolist())\n",
    "            else:\n",
    "                coords[atom_name].append([float(\"nan\")] * 3)\n",
    "\n",
    "    return seq, coords, chain_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc203eb-e8d2-47e0-9034-1edf69bc2983",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d71a36e-c25c-463d-bd25-d4e12a1cc4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= MODIFICATIONS FOR MULTI-TASK LEARNING =============\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. MASKED MSE LOSS WITH TASK WEIGHTING\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked MSE loss that handles NaN values and applies task-specific weighting\n",
    "    based on the inverse of the range of each task.\n",
    "    \"\"\"\n",
    "    def __init__(self, task_ranges=None):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.task_ranges = task_ranges\n",
    "        if task_ranges is not None:\n",
    "            # Calculate task weights based on inverse range\n",
    "            weights = []\n",
    "            for range_val in task_ranges.values():\n",
    "                weights.append(1.0 / range_val if range_val > 0 else 1.0)\n",
    "            total_weight = sum(weights)\n",
    "            self.task_weights = torch.tensor([w / total_weight for w in weights])\n",
    "        else:\n",
    "            self.task_weights = None\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: [batch_size, n_tasks]\n",
    "        target: [batch_size, n_tasks]\n",
    "        \"\"\"\n",
    "        # Create mask for non-NaN values\n",
    "        mask = ~torch.isnan(target)\n",
    "        \n",
    "        # Calculate MSE only for non-NaN values\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # Apply mask\n",
    "        pred_masked = pred[mask]\n",
    "        target_masked = target[mask]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        se = (pred_masked - target_masked) ** 2\n",
    "        \n",
    "        # If we have task weights, apply them\n",
    "        if self.task_weights is not None:\n",
    "            # Expand mask to get task indices\n",
    "            task_indices = torch.where(mask)[1]\n",
    "            weights = self.task_weights.to(pred.device)[task_indices]\n",
    "            weighted_se = se * weights\n",
    "            loss = weighted_se.mean()\n",
    "        else:\n",
    "            loss = se.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "# 2. MODIFIED DTA MODEL FOR MULTI-TASK LEARNING\n",
    "class MTL_DTAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            task_names=['pKi', 'pEC50', 'pKd', 'pIC50'],  # List of tasks\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=[128, 256, 256],\n",
    "            prot_fc_dims=[1024, 128],\n",
    "            drug_node_in_dim=[66, 1], drug_node_h_dims=[128, 64],\n",
    "            drug_edge_in_dim=[16, 1], drug_edge_h_dims=[32, 1],            \n",
    "            drug_fc_dims=[1024, 128],\n",
    "            mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super(MTL_DTAModel, self).__init__()\n",
    "        \n",
    "        self.task_names = task_names\n",
    "        self.n_tasks = len(task_names)\n",
    "        \n",
    "        # Same encoders as before\n",
    "        self.drug_model = DrugGVPModel(\n",
    "            node_in_dim=drug_node_in_dim, node_h_dim=drug_node_h_dims,\n",
    "            edge_in_dim=drug_edge_in_dim, edge_h_dim=drug_edge_h_dims,\n",
    "        )\n",
    "        drug_emb_dim = drug_node_h_dims[0]\n",
    "        \n",
    "        self.prot_model = Prot3DGraphModel(\n",
    "            d_pretrained_emb=prot_emb_dim, d_gcn=prot_gcn_dims\n",
    "        )\n",
    "        prot_emb_dim = prot_gcn_dims[-1]\n",
    "        \n",
    "        self.drug_fc = self.get_fc_layers(\n",
    "            [drug_emb_dim] + drug_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "       \n",
    "        self.prot_fc = self.get_fc_layers(\n",
    "            [prot_emb_dim] + prot_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Shared representation layers\n",
    "        self.shared_fc = self.get_fc_layers(\n",
    "            [drug_fc_dims[-1] + prot_fc_dims[-1]] + mlp_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Task-specific heads (one for each task)\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            task: nn.Linear(mlp_dims[-1], 1) for task in task_names\n",
    "        })\n",
    "    \n",
    "    def get_fc_layers(self, hidden_sizes,\n",
    "            dropout=0, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True):\n",
    "        act_fn = torch.nn.LeakyReLU()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:])):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if not no_last_activation or i != len(hidden_sizes) - 2:\n",
    "                layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                if not no_last_dropout or i != len(hidden_sizes) - 2:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            if batchnorm and i != len(hidden_sizes) - 2:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xd, xp):\n",
    "        # Encode drug and protein\n",
    "        xd = self.drug_model(xd)\n",
    "        xp = self.prot_model(xp)\n",
    "        \n",
    "        # Process through FC layers\n",
    "        xd = self.drug_fc(xd)\n",
    "        xp = self.prot_fc(xp)\n",
    "        \n",
    "        # Concatenate and process through shared layers\n",
    "        x = torch.cat([xd, xp], dim=1)\n",
    "        shared_repr = self.shared_fc(x)\n",
    "        \n",
    "        # Generate predictions for each task\n",
    "        outputs = []\n",
    "        for task in self.task_names:\n",
    "            task_pred = self.task_heads[task](shared_repr)\n",
    "            outputs.append(task_pred)\n",
    "        \n",
    "        # Stack outputs: [batch_size, n_tasks]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# 3. MODIFIED DTA DATASET CLASS\n",
    "class MTL_DTA(data.Dataset):\n",
    "    def __init__(self, df=None, data_list=None, task_cols=None, onthefly=False,\n",
    "                prot_featurize_fn=None, drug_featurize_fn=None):\n",
    "        super(MTL_DTA, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.data_list = data_list\n",
    "        self.task_cols = task_cols or ['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "        self.onthefly = onthefly\n",
    "        self.prot_featurize_fn = prot_featurize_fn\n",
    "        self.drug_featurize_fn = drug_featurize_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.onthefly:\n",
    "            drug = self.drug_featurize_fn(\n",
    "                self.data_list[idx]['drug'],\n",
    "                name=self.data_list[idx]['drug_name']\n",
    "            )\n",
    "            prot = self.prot_featurize_fn(\n",
    "                self.data_list[idx]['protein'],\n",
    "                name=self.data_list[idx]['protein_name']\n",
    "            )\n",
    "        else:\n",
    "            drug = self.data_list[idx]['drug']\n",
    "            prot = self.data_list[idx]['protein']\n",
    "        \n",
    "        # Get multi-task targets\n",
    "        y_multi = []\n",
    "        for task in self.task_cols:\n",
    "            val = self.data_list[idx].get(task, np.nan)\n",
    "            y_multi.append(val if not pd.isna(val) else np.nan)\n",
    "        \n",
    "        y = torch.tensor(y_multi, dtype=torch.float32)\n",
    "        \n",
    "        item = {'drug': drug, 'protein': prot, 'y': y}\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Dataset builder for DTA class\n",
    "def build_dataset(df_fold, pdb_structures, exp_cols = \"pKi\", is_pred = False):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        if is_pred == True:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": 0\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": float(row[exp_cols]),\n",
    "            })\n",
    "    return DTA(df=df_fold, data_list=data_list)\n",
    "\n",
    "\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 5. MODIFIED TRAINING LOOP\n",
    "def train_mtl_model(model, train_loader, valid_loader, task_cols, task_ranges, \n",
    "                    n_epochs=100, lr=0.0005, device='cuda', patience=20):\n",
    "    \"\"\"\n",
    "    Training loop for multi-task learning model\n",
    "    \n",
    "    Args:\n",
    "        task_cols: List of task column names\n",
    "        task_ranges: Dict mapping task names to their value ranges\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges)\n",
    "    stopper = EarlyStopping(patience=patience, higher_better=False)\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)  # [batch_size, n_tasks]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)  # [batch_size, n_tasks]\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_n_batches = 0\n",
    "        task_metrics = {task: {'mse': 0, 'n': 0} for task in task_cols}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                val_n_batches += 1\n",
    "                \n",
    "                # Calculate per-task metrics\n",
    "                for i, task in enumerate(task_cols):\n",
    "                    mask = ~torch.isnan(y[:, i])\n",
    "                    if mask.sum() > 0:\n",
    "                        task_mse = F.mse_loss(pred[mask, i], y[mask, i])\n",
    "                        task_metrics[task]['mse'] += task_mse.item()\n",
    "                        task_metrics[task]['n'] += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / n_batches\n",
    "        avg_val_loss = val_loss / val_n_batches if val_n_batches > 0 else float('inf')\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Valid Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        for task in task_cols:\n",
    "            if task_metrics[task]['n'] > 0:\n",
    "                avg_task_mse = task_metrics[task]['mse'] / task_metrics[task]['n']\n",
    "                print(f\"  {task} MSE: {avg_task_mse:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if stopper.update(avg_val_loss):\n",
    "            best_model = model.state_dict()\n",
    "        if stopper.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. EXAMPLE USAGE\n",
    "def prepare_mtl_experiment(df, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Prepare data for multi-task learning\n",
    "    \"\"\"\n",
    "    # Calculate task ranges for weighting\n",
    "    task_ranges = {}\n",
    "    for task in task_cols:\n",
    "        if task in df.columns:\n",
    "            valid_values = df[task].dropna()\n",
    "            if len(valid_values) > 0:\n",
    "                task_ranges[task] = valid_values.max() - valid_values.min()\n",
    "            else:\n",
    "                task_ranges[task] = 1.0\n",
    "        else:\n",
    "            task_ranges[task] = 1.0\n",
    "    \n",
    "    print(\"Task ranges for weighting:\")\n",
    "    for task, range_val in task_ranges.items():\n",
    "        weight = 1.0 / range_val if range_val > 0 else 1.0\n",
    "        normalized_weight = weight / sum(1.0/r if r > 0 else 1.0 for r in task_ranges.values())\n",
    "        print(f\"  {task}: range={range_val:.2f}, weight={normalized_weight:.4f}\")\n",
    "    \n",
    "    return task_ranges\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def structureJSON(df, esm_model):\n",
    "    structure_dict = {}\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        pdb_path = row[\"standardized_protein_pdb\"]\n",
    "        try:\n",
    "\n",
    "            pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "\n",
    "            structure = parser.get_structure(pdb_id, pdb_path)\n",
    "            seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "            if seq is None:\n",
    "                available = [c.id for c in structure[0]]\n",
    "                print(f\"[SKIP] {pdb_id}: no usable chain found (available: {available})\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Stack in order: N, CA, C, O --> [L, 4, 3]\n",
    "            coords_stacked = []\n",
    "            for i in range(len(coords[\"N\"])):\n",
    "                coord_group = []\n",
    "                for atom in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "                    coord_group.append(coords[atom][i])\n",
    "                coords_stacked.append(coord_group)\n",
    "\n",
    "            if coords_stacked is None:\n",
    "                print(f\"[SKIP] {pdb_id}: no usable coords found (available: {pdb_path})\")\n",
    "                continue\n",
    "\n",
    "                \n",
    "            embedding = get_esm_embedding(seq, esm_model)\n",
    "            torch.save(embedding, f\"esm_embeddings/{pdb_id}.pt\")\n",
    "\n",
    "            if coords_stacked != None and embedding != None:\n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": f\"esm_embeddings/{pdb_id}.pt\"\n",
    "\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"../data/pockets_structure.json\", \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to pockets_structure.json\")\n",
    "\n",
    "    return(structure_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb9d52c7-479c-45ce-b88a-5bb69ed2a292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from Bio.PDB import PDBParser  # make sure Biopython is installed\n",
    "\n",
    "# Assumes you already have:\n",
    "# - extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "# - get_esm_embedding(seq, esm_model)\n",
    "\n",
    "ATOMS = (\"N\", \"CA\", \"C\", \"O\")\n",
    "EMBED_DIR = Path(\"esm_embeddings\")\n",
    "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _stack_backbone(coords):\n",
    "    # coords: dict with keys \"N\",\"CA\",\"C\",\"O\", each a list of [x,y,z]\n",
    "    L = len(coords[\"N\"])\n",
    "    return [[coords[a][i] for a in ATOMS] for i in range(L)]\n",
    "\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Worker: parse PDB, extract seq/coords/chain, return tuple or a skip marker.\n",
    "    Runs in a separate process; initializes its own parser.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "\n",
    "        seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "        if seq is None:\n",
    "            available = [c.id for c in structure[0]]\n",
    "            return (\"skip\", pdb_id, f\"no usable chain (available: {available})\")\n",
    "\n",
    "        if not coords or any(k not in coords for k in ATOMS) or len(coords[\"N\"]) == 0:\n",
    "            return (\"skip\", pdb_id, \"no usable coords\")\n",
    "\n",
    "        coords_stacked = _stack_backbone(coords)\n",
    "        if not coords_stacked:\n",
    "            return (\"skip\", pdb_id, \"empty coords after stacking\")\n",
    "\n",
    "        return (\"ok\", pdb_id, seq, coords_stacked, chain_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"error\", os.path.basename(pdb_path).split('.')[0], str(e))\n",
    "\n",
    "def structureJSON(df, esm_model, max_workers=None, embed_batch_size=8, out_json=\"../data/pockets_structure.json\"):\n",
    "    structure_dict = {}\n",
    "\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    results = []\n",
    "\n",
    "    # Phase 1: parallel PDB parsing + coordinate extraction\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"PDB -> seq/coords\"):\n",
    "            status_tuple = fut.result()\n",
    "            results.append(status_tuple)\n",
    "\n",
    "    # Log skips/errors (fast)\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            _, pdb_id, msg = r\n",
    "            print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "        elif tag == \"error\":\n",
    "            _, pdb_id, err = r\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "\n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "\n",
    "    # Phase 2: embeddings on a single device (GPU/CPU) to avoid per-process model copies\n",
    "    # Optionally batch if your get_esm_embedding supports lists; otherwise do per-sequence.\n",
    "    # Here we do per-sequence by default; simple and safe.\n",
    "    for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, desc=\"ESM embeddings\"):\n",
    "        try:\n",
    "            embedding = get_esm_embedding(seq, esm_model)  # ensure this returns a tensor\n",
    "            torch.save(embedding, EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "\n",
    "            structure_dict[pdb_id] = {\n",
    "                \"name\": pdb_id,\n",
    "                \"UniProt_id\": \"UNKNOWN\",\n",
    "                \"PDB_id\": pdb_id,\n",
    "                \"chain\": chain_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords\": coords_stacked,         # [[N,CA,C,O], ...], each as [x,y,z]\n",
    "                \"embed\": str(EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to {os.path.basename(out_json)}\")\n",
    "    return structure_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "def structureJSON_chunked(df, esm_model, max_workers=None, embed_batch_size=8, \n",
    "                          chunk_size=100000, out_dir=\"../data/structure_chunks/\"):\n",
    "    \"\"\"\n",
    "    Process structures in chunks to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model: ESM model for embeddings\n",
    "        max_workers: Number of parallel workers\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(EMBED_DIR, exist_ok=True)\n",
    "    \n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"Processing {total_pdbs} PDBs in {num_chunks} chunks of max {chunk_size} each\")\n",
    "    \n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"\\n=== Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_paths)} PDBs) ===\")\n",
    "        \n",
    "        structure_dict = {}\n",
    "        results = []\n",
    "        \n",
    "        # Phase 1: Parallel PDB parsing for this chunk\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_process_pdb_path, p): p for p in chunk_paths}\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                          desc=f\"Chunk {chunk_idx + 1} - PDB parsing\"):\n",
    "                status_tuple = fut.result()\n",
    "                results.append(status_tuple)\n",
    "        \n",
    "        # Log errors for this chunk\n",
    "        for r in results:\n",
    "            tag = r[0]\n",
    "            if tag == \"skip\":\n",
    "                _, pdb_id, msg = r\n",
    "                print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "            elif tag == \"error\":\n",
    "                _, pdb_id, err = r\n",
    "                print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "        \n",
    "        # Keep only successful items\n",
    "        ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                    for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                    for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "        \n",
    "        # Phase 2: ESM embeddings for this chunk\n",
    "        for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, \n",
    "                                                          desc=f\"Chunk {chunk_idx + 1} - ESM embeddings\"):\n",
    "            try:\n",
    "                embedding = get_esm_embedding(seq, esm_model)\n",
    "                embed_path = EMBED_DIR / f\"{pdb_id}.pt\"\n",
    "                torch.save(embedding, embed_path)\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": str(embed_path)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Save this chunk\n",
    "        chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "        chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "        with open(chunk_path, \"w\") as f:\n",
    "            json.dump(structure_dict, f, indent=2)\n",
    "        \n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": chunk_idx,\n",
    "            \"filename\": chunk_filename,\n",
    "            \"path\": chunk_path,\n",
    "            \"num_structures\": len(structure_dict),\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        \n",
    "        print(f\"✅ Chunk {chunk_idx + 1} saved: {len(structure_dict)} structures to {chunk_filename}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ All chunks processed. Metadata saved to {metadata_path}\")\n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            # Quick scan to build index (could be saved in metadata for efficiency)\n",
    "            with open(chunk_path, \"r\") as f:\n",
    "                chunk_data = json.load(f)\n",
    "                for pdb_id in chunk_data.keys():\n",
    "                    self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d49b9fa7-c897-4d2f-aac7-7b6d9a91019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "\n",
    "def process_single_chunk(args):\n",
    "    \"\"\"\n",
    "    Process a single chunk of PDB files independently.\n",
    "    This function is designed to be run in parallel.\n",
    "    \n",
    "    Args:\n",
    "        args: tuple of (chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name)\n",
    "    \n",
    "    Returns:\n",
    "        dict with chunk processing results\n",
    "    \"\"\"\n",
    "    chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name = args\n",
    "    \n",
    "    # Import inside function for multiprocessing\n",
    "    from transformers import EsmModel, EsmTokenizer\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    print(f\"\\n[Chunk {chunk_idx}] Starting processing of {len(pdb_paths)} PDBs\")\n",
    "    \n",
    "    # Load ESM model for this process\n",
    "    print(f\"[Chunk {chunk_idx}] Loading ESM model...\")\n",
    "    tokenizer = EsmTokenizer.from_pretrained(esm_model_name)\n",
    "    esm_model = EsmModel.from_pretrained(esm_model_name)\n",
    "    esm_model.eval()\n",
    "    \n",
    "    # Move to GPU if available (each process gets its own GPU memory)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        # For multi-GPU, assign different chunks to different GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        gpu_id = chunk_idx % num_gpus\n",
    "        device = torch.device(f'cuda:{gpu_id}')\n",
    "        esm_model = esm_model.to(device)\n",
    "        print(f\"[Chunk {chunk_idx}] Using GPU {gpu_id}\")\n",
    "    else:\n",
    "        print(f\"[Chunk {chunk_idx}] Using CPU\")\n",
    "    \n",
    "    structure_dict = {}\n",
    "    results = []\n",
    "    \n",
    "    # Phase 1: Parallel PDB parsing within this chunk\n",
    "    max_workers = min(8, cpu_count() // 4)  # Limit workers per chunk\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                      desc=f\"Chunk {chunk_idx} - PDB parsing\", position=chunk_idx):\n",
    "            try:\n",
    "                status_tuple = fut.result(timeout=30)  # Add timeout\n",
    "                results.append(status_tuple)\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] Error processing PDB: {e}\")\n",
    "    \n",
    "    # Log errors\n",
    "    error_count = 0\n",
    "    skip_count = 0\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            skip_count += 1\n",
    "        elif tag == \"error\":\n",
    "            error_count += 1\n",
    "    \n",
    "    if error_count > 0 or skip_count > 0:\n",
    "        print(f\"[Chunk {chunk_idx}] Skipped: {skip_count}, Errors: {error_count}\")\n",
    "    \n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "    \n",
    "    # Phase 2: ESM embeddings (batch processing for efficiency)\n",
    "    print(f\"[Chunk {chunk_idx}] Computing ESM embeddings for {len(ok_items)} proteins...\")\n",
    "    \n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in batches to optimize GPU usage\n",
    "    batch_size = 8\n",
    "    for i in tqdm(range(0, len(ok_items), batch_size), \n",
    "                  desc=f\"Chunk {chunk_idx} - ESM embeddings\", position=chunk_idx):\n",
    "        batch = ok_items[i:i+batch_size]\n",
    "        \n",
    "        for pdb_id, seq, coords_stacked, chain_id in batch:\n",
    "            try:\n",
    "                # Compute embedding\n",
    "                with torch.no_grad():\n",
    "                    embedding = get_esm_embedding(seq, esm_model, tokenizer, device)\n",
    "                \n",
    "                # Save embedding\n",
    "                embed_path = os.path.join(embed_dir, f\"{pdb_id}.pt\")\n",
    "                torch.save(embedding.cpu(), embed_path)  # Save on CPU to free GPU memory\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": embed_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Periodic garbage collection\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save chunk\n",
    "    chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "    chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "    with open(chunk_path, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"[Chunk {chunk_idx}] ✅ Completed: {len(structure_dict)} structures saved\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del esm_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"chunk_idx\": chunk_idx,\n",
    "        \"filename\": chunk_filename,\n",
    "        \"path\": chunk_path,\n",
    "        \"num_structures\": len(structure_dict),\n",
    "        \"num_errors\": error_count,\n",
    "        \"num_skipped\": skip_count\n",
    "    }\n",
    "\n",
    "\n",
    "def get_esm_embedding(seq, esm_model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Get ESM embedding for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        seq: Protein sequence\n",
    "        esm_model: ESM model\n",
    "        tokenizer: ESM tokenizer\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = esm_model(**inputs)\n",
    "        # Use mean pooling over sequence length\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def structureJSON_chunked(df, esm_model_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "                         chunk_size=100000, max_chunks_parallel=4,\n",
    "                         out_dir=\"../data/structure_chunks/\",\n",
    "                         embed_dir=\"../data/embeddings/\"):\n",
    "    \"\"\"\n",
    "    Process structures in parallel chunks to avoid memory issues and maximize speed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model_name: Name of ESM model to use\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        max_chunks_parallel: Maximum number of chunks to process in parallel\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "        embed_dir: Directory to save embeddings\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique PDB paths (avoid duplicates)\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].unique().tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Processing {total_pdbs} unique PDBs in {num_chunks} chunks\")\n",
    "    print(f\"Chunk size: {chunk_size}, Parallel chunks: {max_chunks_parallel}\")\n",
    "    print(f\"=\" * 80)\n",
    "    \n",
    "    # Prepare chunk arguments\n",
    "    chunk_args = []\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        chunk_args.append((\n",
    "            chunk_idx,\n",
    "            chunk_paths,\n",
    "            out_dir,\n",
    "            embed_dir,\n",
    "            esm_model_name\n",
    "        ))\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    chunk_results = []\n",
    "    \n",
    "    # Determine optimal number of parallel processes\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    if num_gpus > 0:\n",
    "        # If we have GPUs, process one chunk per GPU\n",
    "        parallel_chunks = min(max_chunks_parallel, num_gpus, num_chunks)\n",
    "        print(f\"Using {num_gpus} GPUs, processing {parallel_chunks} chunks in parallel\")\n",
    "    else:\n",
    "        # CPU only - limit parallelism to avoid memory issues\n",
    "        parallel_chunks = min(max_chunks_parallel, cpu_count() // 4, num_chunks)\n",
    "        print(f\"Using CPU only, processing {parallel_chunks} chunks in parallel\")\n",
    "    \n",
    "    # Process in batches of parallel chunks\n",
    "    for batch_start in range(0, num_chunks, parallel_chunks):\n",
    "        batch_end = min(batch_start + parallel_chunks, num_chunks)\n",
    "        batch_args = chunk_args[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\nProcessing chunk batch {batch_start+1}-{batch_end} of {num_chunks}\")\n",
    "        \n",
    "        if len(batch_args) == 1:\n",
    "            # Single chunk - process directly\n",
    "            result = process_single_chunk(batch_args[0])\n",
    "            chunk_results.append(result)\n",
    "        else:\n",
    "            # Multiple chunks - use multiprocessing\n",
    "            with Pool(processes=len(batch_args)) as pool:\n",
    "                results = pool.map(process_single_chunk, batch_args)\n",
    "                chunk_results.extend(results)\n",
    "        \n",
    "        # Garbage collection between batches\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create metadata\n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"total_structures\": sum(r[\"num_structures\"] for r in chunk_results),\n",
    "        \"total_errors\": sum(r[\"num_errors\"] for r in chunk_results),\n",
    "        \"total_skipped\": sum(r[\"num_skipped\"] for r in chunk_results),\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Add chunk info with proper indices\n",
    "    start_idx = 0\n",
    "    for result in sorted(chunk_results, key=lambda x: x[\"chunk_idx\"]):\n",
    "        end_idx = start_idx + result[\"num_structures\"]\n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": result[\"chunk_idx\"],\n",
    "            \"filename\": result[\"filename\"],\n",
    "            \"path\": result[\"path\"],\n",
    "            \"num_structures\": result[\"num_structures\"],\n",
    "            \"num_errors\": result[\"num_errors\"],\n",
    "            \"num_skipped\": result[\"num_skipped\"],\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✅ Processing complete!\")\n",
    "    print(f\"  - Total structures: {chunk_metadata['total_structures']}\")\n",
    "    print(f\"  - Total errors: {chunk_metadata['total_errors']}\")\n",
    "    print(f\"  - Total skipped: {chunk_metadata['total_skipped']}\")\n",
    "    print(f\"  - Metadata saved: {metadata_path}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "# ============= Helper function for PDB processing =============\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Process a single PDB file to extract sequence and coordinates.\n",
    "    This function runs in a separate process.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (status, pdb_id, data...) where status is \"ok\", \"skip\", or \"error\"\n",
    "    \"\"\"\n",
    "    from Bio.PDB import PDBParser, is_aa\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    pdb_id = os.path.splitext(os.path.basename(pdb_path))[0]\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "        \n",
    "        # Get first model\n",
    "        model = structure[0]\n",
    "        \n",
    "        # Process each chain\n",
    "        for chain in model:\n",
    "            residues = [r for r in chain if is_aa(r)]\n",
    "            if len(residues) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract sequence\n",
    "            seq = ''.join([seq1(r.resname) for r in residues])\n",
    "            \n",
    "            # Extract coordinates [N, CA, C, O] for each residue\n",
    "            coords = []\n",
    "            for residue in residues:\n",
    "                try:\n",
    "                    n_coord = residue['N'].coord.tolist()\n",
    "                    ca_coord = residue['CA'].coord.tolist()\n",
    "                    c_coord = residue['C'].coord.tolist()\n",
    "                    o_coord = residue['O'].coord.tolist()\n",
    "                    coords.append([n_coord, ca_coord, c_coord, o_coord])\n",
    "                except:\n",
    "                    # Missing atoms - use zeros\n",
    "                    coords.append([[0,0,0], [0,0,0], [0,0,0], [0,0,0]])\n",
    "            \n",
    "            return (\"ok\", pdb_id, seq, coords, chain.id)\n",
    "        \n",
    "        return (\"skip\", pdb_id, \"No valid chains found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (\"error\", pdb_id, str(e))\n",
    "\n",
    "\n",
    "# ============= Optimized Chunk Loader (same as before) =============\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f: # speed up JSON load from StructureChunkLoader\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded metadata: {self.metadata['total_structures']} structures in {self.metadata['num_chunks']} chunks\")\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, \"r\") as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                    for pdb_id in chunk_data.keys():\n",
    "                        self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "            else:\n",
    "                print(f\"Warning: Chunk file not found: {chunk_path}\")\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "            gc.collect()  # Force garbage collection\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e67e5b-0dfe-4164-bf68-d8111f45f829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ce606-ee95-4850-bb99-c3f6095e9858",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19d6b3cd-b54d-42d2-85f0-035da7ebd23d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ranges for weighting:\n",
      "  pKi: range=9.99, weight=0.1280\n",
      "  pEC50: range=7.82, weight=0.1636\n",
      "  pKd: range=10.97, weight=0.1166\n",
      "  pKd (Wang, FEP): range=4.75, weight=0.2693\n",
      "  pIC50: range=8.92, weight=0.1434\n",
      "  potency: range=7.14, weight=0.1791\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parser = PDBParser(QUIET=True)\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Defne tasks to train on\n",
    "task_cols = ['pKi', 'pEC50', 'pKd', 'pKd (Wang, FEP)', 'pIC50', 'potency']\n",
    "\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\", engine=\"fastparquet\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "col_nan = [\"standardized_protein_pdb\", \"standardized_ligand_sdf\"] + task_cols\n",
    "# df = df[df['is_experimental'] == True]\n",
    "df = df.dropna(how = \"all\", subset=col_nan)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[df[\"standardized_protein_pdb\"].isna()==False]\n",
    "df = df[df[\"standardized_ligand_sdf\"].isna()==False]\n",
    "df = df[:50000]\n",
    "\n",
    "# Calculate task ranges from your dataframe\n",
    "task_ranges = prepare_mtl_experiment(df, task_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bff598-24c9-4d26-924c-5dc700614db9",
   "metadata": {},
   "source": [
    "# Load ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "309da2a6-a78f-4aa2-8413-63bee8a064e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-32): 33 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ESM-2 model\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "esm_model = EsmModel.from_pretrained(model_name)\n",
    "esm_model.eval().cuda() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e8321-de80-48c4-9172-9d5463eb7e4b",
   "metadata": {},
   "source": [
    "# Generate structure dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "463288cb-569f-45a9-9b57-9b2e238afa16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized chunk loader with multiple performance improvements:\n",
    "1. Use pickle or msgpack instead of JSON (10-50x faster)\n",
    "2. Memory-mapped files for zero-copy access\n",
    "3. Parallel chunk loading\n",
    "4. Pre-computed index for O(1) lookups\n",
    "5. Optional HDF5 storage for even faster access\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import mmap\n",
    "import struct\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from functools import lru_cache\n",
    "import time\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "import h5py\n",
    "import msgpack\n",
    "import lmdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============= SOLUTION 1: Pickle-based Fast Loader =============\n",
    "class FastPickleChunkLoader:\n",
    "    \"\"\"\n",
    "    5-10x faster than JSON using pickle format\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=5):\n",
    "        self.chunk_dir = Path(chunk_dir)\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        self.cache_order = []\n",
    "        \n",
    "        print(\"Initializing FastPickleChunkLoader...\")\n",
    "        \n",
    "        # Check if pickle versions exist, if not convert from JSON\n",
    "        self._ensure_pickle_chunks()\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_pkl = self.chunk_dir / \"chunk_metadata.pkl\"\n",
    "        if metadata_pkl.exists():\n",
    "            with open(metadata_pkl, \"rb\") as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "        else:\n",
    "            # Fallback to JSON and convert\n",
    "            with open(self.chunk_dir / \"chunk_metadata.json\", \"r\") as f:\n",
    "                self.metadata = json.load(f)\n",
    "            with open(metadata_pkl, \"wb\") as f:\n",
    "                pickle.dump(self.metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f\"Loaded metadata: {self.metadata['total_structures']} structures in {self.metadata['num_chunks']} chunks\")\n",
    "        \n",
    "        # Build optimized lookup using numpy arrays for faster access\n",
    "        self._build_optimized_index()\n",
    "        \n",
    "        # Pre-load frequently used chunks\n",
    "        self._preload_chunks()\n",
    "    \n",
    "    def _ensure_pickle_chunks(self):\n",
    "        \"\"\"Convert JSON chunks to pickle format if they don't exist\"\"\"\n",
    "        json_files = list(self.chunk_dir.glob(\"chunk_*.json\"))\n",
    "        \n",
    "        for json_file in tqdm(json_files, desc=\"Converting to pickle\"):\n",
    "            pkl_file = json_file.with_suffix('.pkl')\n",
    "            \n",
    "            if not pkl_file.exists():\n",
    "                with open(json_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                with open(pkl_file, 'wb') as f:\n",
    "                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "                print(f\"Converted {json_file.name} to pickle format\")\n",
    "    \n",
    "    def _build_optimized_index(self):\n",
    "        \"\"\"Build optimized index for O(1) lookups\"\"\"\n",
    "        self.pdb_to_chunk = {}\n",
    "        \n",
    "        # Use parallel loading for index building\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = []\n",
    "            \n",
    "            for chunk_info in self.metadata[\"chunks\"]:\n",
    "                pkl_path = self.chunk_dir / chunk_info[\"filename\"].replace('.json', '.pkl')\n",
    "                if pkl_path.exists():\n",
    "                    future = executor.submit(self._load_chunk_index, pkl_path, chunk_info[\"chunk_idx\"])\n",
    "                    futures.append(future)\n",
    "            \n",
    "            for future in futures:\n",
    "                pdb_ids, chunk_idx = future.result()\n",
    "                for pdb_id in pdb_ids:\n",
    "                    self.pdb_to_chunk[pdb_id] = chunk_idx\n",
    "    \n",
    "    def _load_chunk_index(self, pkl_path, chunk_idx):\n",
    "        \"\"\"Load just the keys from a chunk file\"\"\"\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            chunk_data = pickle.load(f)\n",
    "        return list(chunk_data.keys()), chunk_idx\n",
    "    \n",
    "    def _preload_chunks(self, n_chunks=2):\n",
    "        \"\"\"Pre-load the first few chunks into cache\"\"\"\n",
    "        for i in range(min(n_chunks, len(self.metadata[\"chunks\"]))):\n",
    "            self._load_chunk(i)\n",
    "    \n",
    "    @lru_cache(maxsize=10)\n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk with caching\"\"\"\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        pkl_path = self.chunk_dir / chunk_info[\"filename\"].replace('.json', '.pkl')\n",
    "        \n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Optimized batch retrieval with parallel loading\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load chunks in parallel\n",
    "        results = {}\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {}\n",
    "            \n",
    "            for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "                future = executor.submit(self._load_chunk, chunk_idx)\n",
    "                futures[future] = chunk_pdb_ids\n",
    "            \n",
    "            for future, chunk_pdb_ids in futures.items():\n",
    "                chunk_data = future.result()\n",
    "                for pdb_id in chunk_pdb_ids:\n",
    "                    if pdb_id in chunk_data:\n",
    "                        results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get single structure\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "\n",
    "# ============= SOLUTION 2: HDF5-based Ultra-Fast Loader =============\n",
    "class HDF5ChunkLoader:\n",
    "    \"\"\"\n",
    "    Ultra-fast loading using HDF5 format with compression\n",
    "    Best for numerical data, 20-100x faster than JSON\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", convert=True):\n",
    "        self.chunk_dir = Path(chunk_dir)\n",
    "        self.h5_file = self.chunk_dir / \"structures.h5\"\n",
    "        \n",
    "        if convert and not self.h5_file.exists():\n",
    "            self._convert_to_hdf5()\n",
    "        \n",
    "        # Open HDF5 file with swmr mode for concurrent reads\n",
    "        self.h5f = h5py.File(self.h5_file, 'r', swmr=True)\n",
    "        \n",
    "        # Build index\n",
    "        self.pdb_ids = set(self.h5f.keys())\n",
    "        print(f\"Loaded {len(self.pdb_ids)} structures from HDF5\")\n",
    "    \n",
    "    def _convert_to_hdf5(self):\n",
    "        \"\"\"Convert JSON chunks to HDF5 format\"\"\"\n",
    "        print(\"Converting to HDF5 format...\")\n",
    "        \n",
    "        # Load all chunks\n",
    "        all_structures = {}\n",
    "        json_files = list(self.chunk_dir.glob(\"chunk_*.json\"))\n",
    "        \n",
    "        for json_file in tqdm(json_files, desc=\"Loading chunks\"):\n",
    "            with open(json_file, 'r') as f:\n",
    "                chunk_data = json.load(f)\n",
    "                all_structures.update(chunk_data)\n",
    "        \n",
    "        # Create HDF5 file with compression\n",
    "        with h5py.File(self.h5_file, 'w') as h5f:\n",
    "            for pdb_id, structure in tqdm(all_structures.items(), desc=\"Writing HDF5\"):\n",
    "                grp = h5f.create_group(pdb_id)\n",
    "                \n",
    "                # Store each field efficiently\n",
    "                for key, value in structure.items():\n",
    "                    if isinstance(value, (list, tuple)):\n",
    "                        value = np.array(value)\n",
    "                    \n",
    "                    if isinstance(value, np.ndarray):\n",
    "                        grp.create_dataset(key, data=value, compression='gzip', compression_opts=1)\n",
    "                    else:\n",
    "                        grp.attrs[key] = value\n",
    "        \n",
    "        print(f\"Created HDF5 file: {self.h5_file}\")\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get single structure from HDF5\"\"\"\n",
    "        if pdb_id not in self.h5f:\n",
    "            return None\n",
    "        \n",
    "        grp = self.h5f[pdb_id]\n",
    "        structure = dict(grp.attrs)\n",
    "        \n",
    "        for key in grp.keys():\n",
    "            structure[key] = grp[key][:]\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get batch of structures from HDF5\"\"\"\n",
    "        results = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.h5f:\n",
    "                results[pdb_id] = self.get(pdb_id)\n",
    "        return results\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'h5f'):\n",
    "            self.h5f.close()\n",
    "\n",
    "# ============= SOLUTION 3: LMDB-based Memory-Mapped Loader =============\n",
    "class LMDBChunkLoader:\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", convert=True):\n",
    "        self.chunk_dir = Path(chunk_dir)\n",
    "        self.lmdb_dir = self.chunk_dir / \"structures_lmdb\"\n",
    "        \n",
    "        if convert and not self.lmdb_dir.exists():\n",
    "            self._convert_to_lmdb()\n",
    "        \n",
    "        # Open LMDB with large map size\n",
    "        self.env = lmdb.open(\n",
    "            str(self.lmdb_dir),\n",
    "            map_size=10 * 1024**3,  # 10GB\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=True,\n",
    "            meminit=False\n",
    "        )\n",
    "        \n",
    "        # Build index - EXCLUDE METADATA KEYS\n",
    "        with self.env.begin() as txn:\n",
    "            self.pdb_ids = set()\n",
    "            for key, _ in txn.cursor():\n",
    "                key_str = key.decode()\n",
    "                # Skip metadata entries\n",
    "                if key_str not in {'total_errors', 'num_chunks', 'chunk_size', 'total_structures', 'chunks'}:\n",
    "                    self.pdb_ids.add(key_str)\n",
    "        \n",
    "        print(f\"Loaded {len(self.pdb_ids)} structures from LMDB\")\n",
    "    \n",
    "    def _convert_to_lmdb(self):\n",
    "        \"\"\"Convert JSON chunks to LMDB format\"\"\"\n",
    "        print(\"Converting to LMDB format...\")\n",
    "        \n",
    "        self.lmdb_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create LMDB\n",
    "        env = lmdb.open(\n",
    "            str(self.lmdb_dir),\n",
    "            map_size=10 * 1024**3  # 10GB\n",
    "        )\n",
    "        \n",
    "        # FIX: Use correct file pattern\n",
    "        json_files = list(self.chunk_dir.glob(\"structures_chunk_*.json\"))  # Changed from \"chunk_*.json\"\n",
    "        \n",
    "        if not json_files:\n",
    "            print(f\"No JSON files found matching pattern 'structures_chunk_*.json' in {self.chunk_dir}\")\n",
    "            return\n",
    "        \n",
    "        with env.begin(write=True) as txn:\n",
    "            for json_file in tqdm(json_files, desc=\"Converting to LMDB\"):\n",
    "                with open(json_file, 'r') as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                \n",
    "                for pdb_id, structure in chunk_data.items():\n",
    "                    # Use msgpack for efficient serialization\n",
    "                    packed = msgpack.packb(structure, use_bin_type=True)\n",
    "                    txn.put(pdb_id.encode(), packed)\n",
    "        \n",
    "        env.close()\n",
    "        print(f\"Created LMDB database: {self.lmdb_dir}\")\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get single structure from LMDB\"\"\"\n",
    "        with self.env.begin() as txn:\n",
    "            packed = txn.get(pdb_id.encode())\n",
    "            if packed:\n",
    "                return msgpack.unpackb(packed, raw=False)\n",
    "        return None\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get batch of structures from LMDB - very fast\"\"\"\n",
    "        results = {}\n",
    "        with self.env.begin() as txn:\n",
    "            for pdb_id in pdb_ids:\n",
    "                packed = txn.get(pdb_id.encode())\n",
    "                if packed:\n",
    "                    results[pdb_id] = msgpack.unpackb(packed, raw=False)\n",
    "        return results\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ============= SOLUTION 4: Shared Memory Loader for Multi-Processing =============\n",
    "class SharedMemoryChunkLoader:\n",
    "    \"\"\"\n",
    "    Use shared memory for zero-copy access across processes\n",
    "    Best for multi-GPU training with multiple processes\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\"):\n",
    "        import multiprocessing as mp\n",
    "        from multiprocessing import shared_memory\n",
    "        \n",
    "        self.chunk_dir = Path(chunk_dir)\n",
    "        self.manager = mp.Manager()\n",
    "        self.shared_dict = self.manager.dict()\n",
    "        \n",
    "        # Load all data into shared memory\n",
    "        self._load_to_shared_memory()\n",
    "    \n",
    "    def _load_to_shared_memory(self):\n",
    "        \"\"\"Load all structures into shared memory\"\"\"\n",
    "        print(\"Loading structures into shared memory...\")\n",
    "        \n",
    "        json_files = list(self.chunk_dir.glob(\"chunk_*.json\"))\n",
    "        \n",
    "        for json_file in tqdm(json_files, desc=\"Loading to shared memory\"):\n",
    "            with open(json_file, 'r') as f:\n",
    "                chunk_data = json.load(f)\n",
    "                self.shared_dict.update(chunk_data)\n",
    "        \n",
    "        print(f\"Loaded {len(self.shared_dict)} structures into shared memory\")\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        return self.shared_dict.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        return {pdb_id: self.shared_dict[pdb_id] \n",
    "                for pdb_id in pdb_ids \n",
    "                if pdb_id in self.shared_dict}\n",
    "\n",
    "# ============= BENCHMARK FUNCTION =============\n",
    "def benchmark_loaders(chunk_dir=\"../data/structure_chunks/\", n_samples=1000):\n",
    "    \"\"\"\n",
    "    Benchmark different loader implementations\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BENCHMARKING CHUNK LOADERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get sample PDB IDs\n",
    "    with open(Path(chunk_dir) / \"chunk_metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Get some PDB IDs for testing\n",
    "    test_file = Path(chunk_dir) / \"structures_chunk_0000.json\"\n",
    "    with open(test_file, \"r\") as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    pdb_ids = list(sample_data.keys())[:n_samples]\n",
    "    batch_size = 32\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test original JSON loader\n",
    "    print(\"\\n1. Testing JSON Loader...\")\n",
    "    loader = StructureChunkLoader(chunk_dir)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(0, len(pdb_ids), batch_size):\n",
    "        batch = pdb_ids[i:i+batch_size]\n",
    "        _ = loader.get_batch(batch)\n",
    "    json_time = time.time() - start\n",
    "    results['JSON'] = json_time\n",
    "    print(f\"   Time: {json_time:.2f}s\")\n",
    "    \n",
    "    # Test Pickle loader\n",
    "    print(\"\\n2. Testing Pickle Loader...\")\n",
    "    loader = FastPickleChunkLoader(chunk_dir)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(0, len(pdb_ids), batch_size):\n",
    "        batch = pdb_ids[i:i+batch_size]\n",
    "        _ = loader.get_batch(batch)\n",
    "    pickle_time = time.time() - start\n",
    "    results['Pickle'] = pickle_time\n",
    "    print(f\"   Time: {pickle_time:.2f}s\")\n",
    "    print(f\"   Speedup: {json_time/pickle_time:.1f}x\")\n",
    "    \n",
    "    # Test HDF5 loader\n",
    "    try:\n",
    "        print(\"\\n3. Testing HDF5 Loader...\")\n",
    "        loader = HDF5ChunkLoader(chunk_dir)\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(0, len(pdb_ids), batch_size):\n",
    "            batch = pdb_ids[i:i+batch_size]\n",
    "            _ = loader.get_batch(batch)\n",
    "        hdf5_time = time.time() - start\n",
    "        results['HDF5'] = hdf5_time\n",
    "        print(f\"   Time: {hdf5_time:.2f}s\")\n",
    "        print(f\"   Speedup: {json_time/hdf5_time:.1f}x\")\n",
    "    except ImportError:\n",
    "        print(\"   HDF5 not available\")\n",
    "    \n",
    "    # Test LMDB loader\n",
    "    try:\n",
    "        print(\"\\n4. Testing LMDB Loader...\")\n",
    "        loader = LMDBChunkLoader(chunk_dir)\n",
    "        \n",
    "        start = time.time()\n",
    "        for i in range(0, len(pdb_ids), batch_size):\n",
    "            batch = pdb_ids[i:i+batch_size]\n",
    "            _ = loader.get_batch(batch)\n",
    "        lmdb_time = time.time() - start\n",
    "        results['LMDB'] = lmdb_time\n",
    "        print(f\"   Time: {lmdb_time:.2f}s\")\n",
    "        print(f\"   Speedup: {json_time/lmdb_time:.1f}x\")\n",
    "    except ImportError:\n",
    "        print(\"   LMDB not available\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY (loading {:,} structures in batches of {}):\".format(n_samples, batch_size))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "    baseline = sorted_results[-1][1]\n",
    "    \n",
    "    for method, time_taken in sorted_results:\n",
    "        speedup = baseline / time_taken\n",
    "        print(f\"{method:10s}: {time_taken:6.2f}s (speedup: {speedup:5.1f}x)\")\n",
    "    \n",
    "    print(\"\\nRECOMMENDATION:\")\n",
    "    fastest = sorted_results[0][0]\n",
    "    print(f\"Use {fastest} loader for {sorted_results[-1][1]/sorted_results[0][1]:.1f}x speedup!\")\n",
    "\n",
    "# ============= DROP-IN REPLACEMENT =============\n",
    "def get_optimized_loader(chunk_dir=\"../data/structure_chunks/\", method=\"auto\"):\n",
    "    \"\"\"\n",
    "    Get the best available loader\n",
    "    \n",
    "    Args:\n",
    "        method: \"auto\", \"pickle\", \"hdf5\", \"lmdb\", or \"json\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == \"auto\":\n",
    "        # Try loaders in order of preference\n",
    "        try:\n",
    "            import lmdb\n",
    "            print(\"Using LMDB loader (fastest)\")\n",
    "            return LMDBChunkLoader(chunk_dir)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            import h5py\n",
    "            print(\"Using HDF5 loader (very fast)\")\n",
    "            return HDF5ChunkLoader(chunk_dir)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        print(\"Using Pickle loader (fast)\")\n",
    "        return FastPickleChunkLoader(chunk_dir)\n",
    "    \n",
    "    elif method == \"pickle\":\n",
    "        return FastPickleChunkLoader(chunk_dir)\n",
    "    elif method == \"hdf5\":\n",
    "        return HDF5ChunkLoader(chunk_dir)\n",
    "    elif method == \"lmdb\":\n",
    "        return LMDBChunkLoader(chunk_dir)\n",
    "    elif method == \"json\":\n",
    "        return StructureChunkLoader(chunk_dir)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fab40c58-4564-47d5-aa52-9b9a8b45edb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LMDB loader (fastest)\n",
      "Loaded 50000 structures from LMDB\n"
     ]
    }
   ],
   "source": [
    "# Replace this:\n",
    "# chunk_loader = StructureChunkLoader(chunk_dir=\"../data/structure_chunks/\")\n",
    "\n",
    "# With this:\n",
    "chunk_loader = get_optimized_loader(chunk_dir=\"../data/structure_chunks/\", method=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92cff4-f3d6-4577-8456-e7a6e5c622b8",
   "metadata": {},
   "source": [
    "# Check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebfdbfee-557e-4900-9212-508b7386fdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_pdb_path</th>\n",
       "      <th>ligand_sdf_path</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pKi</th>\n",
       "      <th>source_file</th>\n",
       "      <th>is_experimental</th>\n",
       "      <th>resolution</th>\n",
       "      <th>pEC50</th>\n",
       "      <th>pKd (Wang, FEP)</th>\n",
       "      <th>pKd</th>\n",
       "      <th>...</th>\n",
       "      <th>LEnorm_pEC50</th>\n",
       "      <th>LE_pKd</th>\n",
       "      <th>LEnorm_pKd</th>\n",
       "      <th>LE_pIC50</th>\n",
       "      <th>LEnorm_pIC50</th>\n",
       "      <th>LE</th>\n",
       "      <th>LE_norm</th>\n",
       "      <th>carbon_le_1</th>\n",
       "      <th>carbon_lt_3</th>\n",
       "      <th>protein_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL214...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL214...</td>\n",
       "      <td>Cn1c(NC(C)(C)C)nc2c(-c3cc4c([nH]3)[C@@H](CC[NH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358550</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.358550</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>330070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CN(C)c1ccc(-c2csc3c(=O)cc(N4CCOCC4)oc23)cc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268850</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.268850</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>412364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL203...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL203...</td>\n",
       "      <td>O=C(NCC[NH+]1CCCCC1)Nc1ccc2ncnc(Nc3ccc(OCc4ccc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.144190</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.144190</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>359184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>[NH3+]C1(C(=O)N[C@H](CCO)c2ccc(Cl)cc2)CCN(c2nc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231693</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.231693</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>343265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>NS(=O)(=O)c1cnccc1-n1ccc(=O)cc1</td>\n",
       "      <td>7.737549</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455150</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>../data/raw/HiQBind/raw_data_hiq_sm/5luu/5luu_...</td>\n",
       "      <td>../data/raw/HiQBind/raw_data_hiq_sm/5luu/5luu_...</td>\n",
       "      <td>CC[C@H](O)N1CCC2NN[C@@H](C3CCCCC3)C2C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HiQBind</td>\n",
       "      <td>True</td>\n",
       "      <td>1.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.063989</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266526</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266526</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>150310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CNC(=O)c1ccc(-c2cnc3cnc(C(=O)N(C)c4ccc(Cl)cc4)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.183640</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.183640</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>413646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>Cc1ccc(-n2nnnc2-c2ccc3c(c2)C=CS(=O)(=O)O3)cc1</td>\n",
       "      <td>8.040959</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335040</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL358...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL358...</td>\n",
       "      <td>Cn1c(Nc2ccc(Br)cc2F)c(C(=O)NOC[C@H](O)CO)c2c1C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.251172</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.251172</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>322032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL371...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL371...</td>\n",
       "      <td>Cc1ccccc1OCC(=O)Nc1ccc2cncnc2c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.280633</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.280633</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>228540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        protein_pdb_path  \\\n",
       "0      ../data/raw/BindingNetv2/high/target_CHEMBL214...   \n",
       "1      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2      ../data/raw/BindingNetv2/high/target_CHEMBL203...   \n",
       "3      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "4      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "...                                                  ...   \n",
       "49995  ../data/raw/HiQBind/raw_data_hiq_sm/5luu/5luu_...   \n",
       "49996  ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "49997  ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "49998  ../data/raw/BindingNetv2/high/target_CHEMBL358...   \n",
       "49999  ../data/raw/BindingNetv2/high/target_CHEMBL371...   \n",
       "\n",
       "                                         ligand_sdf_path  \\\n",
       "0      ../data/raw/BindingNetv2/high/target_CHEMBL214...   \n",
       "1      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2      ../data/raw/BindingNetv2/high/target_CHEMBL203...   \n",
       "3      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "4      ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "...                                                  ...   \n",
       "49995  ../data/raw/HiQBind/raw_data_hiq_sm/5luu/5luu_...   \n",
       "49996  ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "49997  ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "49998  ../data/raw/BindingNetv2/high/target_CHEMBL358...   \n",
       "49999  ../data/raw/BindingNetv2/high/target_CHEMBL371...   \n",
       "\n",
       "                                                  smiles       pKi  \\\n",
       "0      Cn1c(NC(C)(C)C)nc2c(-c3cc4c([nH]3)[C@@H](CC[NH...       NaN   \n",
       "1            CN(C)c1ccc(-c2csc3c(=O)cc(N4CCOCC4)oc23)cc1       NaN   \n",
       "2      O=C(NCC[NH+]1CCCCC1)Nc1ccc2ncnc(Nc3ccc(OCc4ccc...       NaN   \n",
       "3      [NH3+]C1(C(=O)N[C@H](CCO)c2ccc(Cl)cc2)CCN(c2nc...       NaN   \n",
       "4                        NS(=O)(=O)c1cnccc1-n1ccc(=O)cc1  7.737549   \n",
       "...                                                  ...       ...   \n",
       "49995             CC[C@H](O)N1CCC2NN[C@@H](C3CCCCC3)C2C1       NaN   \n",
       "49996  CNC(=O)c1ccc(-c2cnc3cnc(C(=O)N(C)c4ccc(Cl)cc4)...       NaN   \n",
       "49997      Cc1ccc(-n2nnnc2-c2ccc3c(c2)C=CS(=O)(=O)O3)cc1  8.040959   \n",
       "49998  Cn1c(Nc2ccc(Br)cc2F)c(C(=O)NOC[C@H](O)CO)c2c1C...       NaN   \n",
       "49999                    Cc1ccccc1OCC(=O)Nc1ccc2cncnc2c1       NaN   \n",
       "\n",
       "        source_file  is_experimental  resolution  pEC50  pKd (Wang, FEP)  \\\n",
       "0      BindingNetv2            False         NaN    NaN              NaN   \n",
       "1      BindingNetv2            False         NaN    NaN              NaN   \n",
       "2      BindingNetv2            False         NaN    NaN              NaN   \n",
       "3      BindingNetv2            False         NaN    NaN              NaN   \n",
       "4      BindingNetv2            False         NaN    NaN              NaN   \n",
       "...             ...              ...         ...    ...              ...   \n",
       "49995       HiQBind             True        1.61    NaN              NaN   \n",
       "49996  BindingNetv2            False         NaN    NaN              NaN   \n",
       "49997  BindingNetv2            False         NaN    NaN              NaN   \n",
       "49998  BindingNetv2            False         NaN    NaN              NaN   \n",
       "49999  BindingNetv2            False         NaN    NaN              NaN   \n",
       "\n",
       "            pKd  ...  LEnorm_pEC50    LE_pKd  LEnorm_pKd  LE_pIC50  \\\n",
       "0           NaN  ...           NaN       NaN         NaN  0.358550   \n",
       "1           NaN  ...           NaN       NaN         NaN  0.268850   \n",
       "2           NaN  ...           NaN       NaN         NaN  0.144190   \n",
       "3           NaN  ...           NaN       NaN         NaN  0.231693   \n",
       "4           NaN  ...           NaN       NaN         NaN       NaN   \n",
       "...         ...  ...           ...       ...         ...       ...   \n",
       "49995  5.063989  ...           NaN  0.266526    0.000997       NaN   \n",
       "49996       NaN  ...           NaN       NaN         NaN  0.183640   \n",
       "49997       NaN  ...           NaN       NaN         NaN       NaN   \n",
       "49998       NaN  ...           NaN       NaN         NaN  0.251172   \n",
       "49999       NaN  ...           NaN       NaN         NaN  0.280633   \n",
       "\n",
       "      LEnorm_pIC50        LE   LE_norm carbon_le_1  carbon_lt_3  protein_id  \n",
       "0         0.000907  0.358550  0.000907       False        False      330070  \n",
       "1         0.000754  0.268850  0.000754       False        False      412364  \n",
       "2         0.000262  0.144190  0.000262       False        False      359184  \n",
       "3         0.000539  0.231693  0.000539       False        False      343265  \n",
       "4              NaN  0.455150  0.001811       False        False       37404  \n",
       "...            ...       ...       ...         ...          ...         ...  \n",
       "49995          NaN  0.266526  0.000997       False        False      150310  \n",
       "49996     0.000437  0.183640  0.000437       False        False      413646  \n",
       "49997          NaN  0.335040  0.000984       False        False       37691  \n",
       "49998     0.000534  0.251172  0.000534       False        False      322032  \n",
       "49999     0.000957  0.280633  0.000957       False        False      228540  \n",
       "\n",
       "[50000 rows x 39 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"protein_id\"] = df[\"standardized_protein_pdb\"].apply(\n",
    "    lambda p: os.path.splitext(os.path.basename(p))[0]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_clean = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "121f9e26-6af6-4c05-91cf-eadc37b9fae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dee7d6-4f79-4695-b8b4-00d5996a57fe",
   "metadata": {},
   "source": [
    "# Check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b64f5464-9652-4add-899b-c863ff3faf53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample protein IDs from dataframe:\n",
      "0    330070\n",
      "1    412364\n",
      "2    359184\n",
      "3    343265\n",
      "4     37404\n",
      "Name: protein_id, dtype: object\n",
      "\n",
      "Sample IDs available in chunk loader:\n",
      "['460134', '374409', '401207', '15175', '238712']\n",
      "\n",
      "Overlap: 50000 IDs match between dataframe and chunks\n",
      "Sample matching IDs: ['460134', '374409', '401207', '15175', '238712']\n"
     ]
    }
   ],
   "source": [
    "# Debug the ID mismatch\n",
    "print(\"Sample protein IDs from dataframe:\")\n",
    "print(df_clean[\"protein_id\"].head())\n",
    "\n",
    "print(\"\\nSample IDs available in chunk loader:\")\n",
    "if hasattr(chunk_loader, 'pdb_ids'):\n",
    "    print(list(chunk_loader.pdb_ids)[:5])\n",
    "elif hasattr(chunk_loader, 'pdb_to_chunk'):\n",
    "    print(list(chunk_loader.pdb_to_chunk.keys())[:5])\n",
    "\n",
    "# Check if there's any overlap\n",
    "df_ids = set(df_clean[\"protein_id\"].unique())\n",
    "if hasattr(chunk_loader, 'pdb_ids'):\n",
    "    chunk_ids = chunk_loader.pdb_ids\n",
    "elif hasattr(chunk_loader, 'pdb_to_chunk'):\n",
    "    chunk_ids = set(chunk_loader.pdb_to_chunk.keys())\n",
    "else:\n",
    "    chunk_ids = set()\n",
    "\n",
    "overlap = df_ids.intersection(chunk_ids)\n",
    "print(f\"\\nOverlap: {len(overlap)} IDs match between dataframe and chunks\")\n",
    "if len(overlap) > 0:\n",
    "    print(f\"Sample matching IDs: {list(overlap)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ce038-30c9-438f-a2a0-a1805138e8fd",
   "metadata": {},
   "source": [
    "# Build data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eab91c01-294e-44f1-a5d3-57a4b685265b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e40a3cd-121e-423f-b3ae-b8539a9cccdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized dataset building with multiple performance improvements:\n",
    "1. Vectorized operations instead of iterrows\n",
    "2. Parallel processing with multiprocessing\n",
    "3. Batch processing\n",
    "4. Feature caching\n",
    "5. Memory-efficient processing\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial, lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import List, Dict, Any, Optional\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import gc\n",
    "\n",
    "# ============= SOLUTION 1: Vectorized + Parallel Processing =============\n",
    "def build_mtl_dataset_parallel(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50'],\n",
    "    n_workers=None,\n",
    "    batch_size=100,\n",
    "    use_cache=True,\n",
    "    cache_dir=\"./feature_cache\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Highly optimized dataset building with parallel processing.\n",
    "    10-20x faster than iterrows approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_workers is None:\n",
    "        n_workers = min(mp.cpu_count() - 1, 8)\n",
    "    \n",
    "    print(f\"Building dataset with {n_workers} workers...\")\n",
    "    \n",
    "    # Pre-load all structures (already done in your code)\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Convert DataFrame to records for faster access\n",
    "    records = df_fold.to_dict('records')\n",
    "    \n",
    "    # Setup cache if enabled\n",
    "    feature_cache = FeatureCache(cache_dir) if use_cache else None\n",
    "    \n",
    "    # Process in batches for better memory management\n",
    "    data_list = []\n",
    "    \n",
    "    # Create batches\n",
    "    n_records = len(records)\n",
    "    batches = []\n",
    "    for i in range(0, n_records, batch_size):\n",
    "        batch_records = records[i:i+batch_size]\n",
    "        batch_structures = {r['protein_id']: structures_batch.get(r['protein_id']) \n",
    "                          for r in batch_records}\n",
    "        batches.append((batch_records, batch_structures))\n",
    "    \n",
    "    print(f\"Processing {len(batches)} batches...\")\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        # Submit all batches\n",
    "        futures = []\n",
    "        for batch_records, batch_structures in batches:\n",
    "            future = executor.submit(\n",
    "                process_batch,\n",
    "                batch_records,\n",
    "                batch_structures,\n",
    "                task_cols,\n",
    "                feature_cache\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Collect results with progress bar\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            batch_data = future.result()\n",
    "            data_list.extend(batch_data)\n",
    "    \n",
    "    print(f\"Successfully processed {len(data_list)} entries\")\n",
    "    \n",
    "    # Save cache if used\n",
    "    if feature_cache:\n",
    "        feature_cache.save()\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "def process_batch(batch_records, batch_structures, task_cols, feature_cache=None):\n",
    "    \"\"\"\n",
    "    Process a batch of records in parallel.\n",
    "    This function runs in a separate process.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    \n",
    "    for record in batch_records:\n",
    "        protein_id = record[\"protein_id\"]\n",
    "        \n",
    "        # Skip if no structure\n",
    "        if protein_id not in batch_structures or batch_structures[protein_id] is None:\n",
    "            continue\n",
    "        \n",
    "        # Get or compute protein features\n",
    "        if feature_cache:\n",
    "            protein_features = feature_cache.get_protein_features(\n",
    "                protein_id, \n",
    "                batch_structures[protein_id]\n",
    "            )\n",
    "        else:\n",
    "            protein_features = featurize_protein_graph(batch_structures[protein_id])\n",
    "        \n",
    "        # Get or compute drug features\n",
    "        if feature_cache:\n",
    "            drug_features = feature_cache.get_drug_features(\n",
    "                record[\"standardized_ligand_sdf\"]\n",
    "            )\n",
    "        else:\n",
    "            drug_features = featurize_drug(record[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in record and not pd.isna(record[task]):\n",
    "                task_values[task] = float(record[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein_features,\n",
    "            \"drug\": drug_features,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        batch_data.append(data_entry)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "# ============= SOLUTION 2: Vectorized Operations =============\n",
    "def build_mtl_dataset_vectorized(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "):\n",
    "    \"\"\"\n",
    "    Use vectorized pandas operations instead of iterrows.\n",
    "    5-10x faster than iterrows.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Building dataset with vectorized operations...\")\n",
    "    \n",
    "    # Load all structures at once\n",
    "    protein_ids = df_fold[\"protein_id\"].unique()\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids.tolist())\n",
    "    \n",
    "    # Create a mask for valid entries\n",
    "    valid_mask = df_fold[\"protein_id\"].apply(lambda x: x in structures_batch)\n",
    "    df_valid = df_fold[valid_mask].copy()\n",
    "    \n",
    "    print(f\"Processing {len(df_valid)} valid entries out of {len(df_fold)}\")\n",
    "    \n",
    "    # Vectorized processing using apply (much faster than iterrows)\n",
    "    def process_row(row):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        protein_json = structures_batch[protein_id]\n",
    "        \n",
    "        return {\n",
    "            \"protein\": featurize_protein_graph(protein_json),\n",
    "            \"drug\": featurize_drug(row[\"standardized_ligand_sdf\"]),\n",
    "            **{task: float(row[task]) if not pd.isna(row[task]) else np.nan \n",
    "               for task in task_cols}\n",
    "        }\n",
    "    \n",
    "    # Use pandas apply with progress bar\n",
    "    tqdm.pandas(desc=\"Processing rows\")\n",
    "    data_list = df_valid.progress_apply(process_row, axis=1).tolist()\n",
    "    \n",
    "    return MTL_DTA(df=df_valid, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= SOLUTION 3: Batch Processing with Threading =============\n",
    "def build_mtl_dataset_threaded(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50'],\n",
    "    n_threads=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Use threading for I/O-bound operations.\n",
    "    Good for featurization that involves file I/O.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Building dataset with {n_threads} threads...\")\n",
    "    \n",
    "    # Load structures\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Prepare data for threading\n",
    "    rows_with_structures = []\n",
    "    for idx, row in df_fold.iterrows():\n",
    "        if row[\"protein_id\"] in structures_batch:\n",
    "            rows_with_structures.append((idx, row, structures_batch[row[\"protein_id\"]]))\n",
    "    \n",
    "    print(f\"Processing {len(rows_with_structures)} entries...\")\n",
    "    \n",
    "    # Process with thread pool\n",
    "    data_list = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, row, structure in rows_with_structures:\n",
    "            future = executor.submit(\n",
    "                process_single_entry,\n",
    "                row, structure, task_cols\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Collect results\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            data_entry = future.result()\n",
    "            if data_entry:\n",
    "                data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "def process_single_entry(row, structure, task_cols):\n",
    "    \"\"\"Process a single entry.\"\"\"\n",
    "    try:\n",
    "        protein_features = featurize_protein_graph(structure)\n",
    "        drug_features = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein_features,\n",
    "            \"drug\": drug_features,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        \n",
    "        return data_entry\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============= FEATURE CACHING SYSTEM =============\n",
    "class FeatureCache:\n",
    "    \"\"\"\n",
    "    Cache computed features to avoid recomputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\"./feature_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.protein_cache_file = self.cache_dir / \"protein_features.pkl\"\n",
    "        self.drug_cache_file = self.cache_dir / \"drug_features.pkl\"\n",
    "        \n",
    "        # Load existing caches\n",
    "        self.protein_cache = self._load_cache(self.protein_cache_file)\n",
    "        self.drug_cache = self._load_cache(self.drug_cache_file)\n",
    "        \n",
    "    \n",
    "    def _load_cache(self, cache_file):\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _get_hash(self, obj):\n",
    "        \"\"\"Get hash of an object for caching.\"\"\"\n",
    "        if isinstance(obj, str):\n",
    "            return hashlib.md5(obj.encode()).hexdigest()\n",
    "        else:\n",
    "            return hashlib.md5(pickle.dumps(obj)).hexdigest()\n",
    "    \n",
    "    def get_protein_features(self, protein_id, structure):\n",
    "        \"\"\"Get or compute protein features.\"\"\"\n",
    "        if protein_id not in self.protein_cache:\n",
    "            self.protein_cache[protein_id] = featurize_protein_graph(structure)\n",
    "        return self.protein_cache[protein_id]\n",
    "    \n",
    "    def get_drug_features(self, sdf_string):\n",
    "        \"\"\"Get or compute drug features.\"\"\"\n",
    "        drug_hash = self._get_hash(sdf_string)\n",
    "        if drug_hash not in self.drug_cache:\n",
    "            self.drug_cache[drug_hash] = featurize_drug(sdf_string)\n",
    "        return self.drug_cache[drug_hash]\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save caches to disk.\"\"\"\n",
    "        with open(self.protein_cache_file, 'wb') as f:\n",
    "            pickle.dump(self.protein_cache, f)\n",
    "        with open(self.drug_cache_file, 'wb') as f:\n",
    "            pickle.dump(self.drug_cache, f)\n",
    "        print(f\"Saved cache: {len(self.protein_cache)} proteins, {len(self.drug_cache)} drugs\")\n",
    "\n",
    "# ============= ULTRA-FAST NUMPY-BASED PROCESSING =============\n",
    "def build_mtl_dataset_numpy(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "):\n",
    "    \"\"\"\n",
    "    Use numpy operations for maximum speed.\n",
    "    Best for numerical features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Building dataset with numpy operations...\")\n",
    "    \n",
    "    # Convert to numpy arrays for faster access\n",
    "    protein_ids = df_fold[\"protein_id\"].values\n",
    "    sdf_strings = df_fold[\"standardized_ligand_sdf\"].values\n",
    "    \n",
    "    # Load structures\n",
    "    unique_proteins = np.unique(protein_ids)\n",
    "    structures_batch = chunk_loader.get_batch(unique_proteins.tolist())\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    n_samples = len(df_fold)\n",
    "    task_values = np.full((n_samples, len(task_cols)), np.nan)\n",
    "    \n",
    "    # Fill task values using numpy\n",
    "    for i, task in enumerate(task_cols):\n",
    "        if task in df_fold.columns:\n",
    "            task_values[:, i] = df_fold[task].values\n",
    "    \n",
    "    # Process in batches\n",
    "    data_list = []\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for start_idx in tqdm(range(0, n_samples, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        \n",
    "        for i in range(start_idx, end_idx):\n",
    "            protein_id = protein_ids[i]\n",
    "            \n",
    "            if protein_id not in structures_batch:\n",
    "                continue\n",
    "            \n",
    "            data_entry = {\n",
    "                \"protein\": featurize_protein_graph(structures_batch[protein_id]),\n",
    "                \"drug\": featurize_drug(sdf_strings[i]),\n",
    "            }\n",
    "            \n",
    "            # Add task values\n",
    "            for j, task in enumerate(task_cols):\n",
    "                data_entry[task] = task_values[i, j]\n",
    "            \n",
    "            data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= JOBLIB PARALLEL PROCESSING =============\n",
    "def build_mtl_dataset_joblib(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50'],\n",
    "    n_jobs=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Use joblib for efficient parallel processing.\n",
    "    Often faster than multiprocessing for scientific computing.\n",
    "    \"\"\"\n",
    "    \n",
    "    from joblib import Parallel, delayed\n",
    "    \n",
    "    print(f\"Building dataset with joblib (n_jobs={n_jobs})...\")\n",
    "    \n",
    "    # Load structures\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Prepare data for parallel processing\n",
    "    rows_data = []\n",
    "    for idx, row in df_fold.iterrows():\n",
    "        if row[\"protein_id\"] in structures_batch:\n",
    "            rows_data.append((\n",
    "                row.to_dict(),\n",
    "                structures_batch[row[\"protein_id\"]],\n",
    "                task_cols\n",
    "            ))\n",
    "    \n",
    "    print(f\"Processing {len(rows_data)} entries...\")\n",
    "    \n",
    "    # Process in parallel using joblib\n",
    "    def process_row_joblib(row_dict, structure, task_cols):\n",
    "        try:\n",
    "            protein_features = featurize_protein_graph(structure)\n",
    "            drug_features = featurize_drug(row_dict[\"standardized_ligand_sdf\"])\n",
    "            \n",
    "            data_entry = {\n",
    "                \"protein\": protein_features,\n",
    "                \"drug\": drug_features,\n",
    "            }\n",
    "            \n",
    "            for task in task_cols:\n",
    "                if task in row_dict and not pd.isna(row_dict[task]):\n",
    "                    data_entry[task] = float(row_dict[task])\n",
    "                else:\n",
    "                    data_entry[task] = np.nan\n",
    "            \n",
    "            return data_entry\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Parallel execution\n",
    "    data_list = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "        delayed(process_row_joblib)(row_dict, structure, task_cols)\n",
    "        for row_dict, structure, task_cols in tqdm(rows_data)\n",
    "    )\n",
    "    \n",
    "    # Filter out None values\n",
    "    data_list = [d for d in data_list if d is not None]\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= BENCHMARK FUNCTION =============\n",
    "def benchmark_dataset_builders(df_fold, chunk_loader, task_cols):\n",
    "    \"\"\"\n",
    "    Benchmark different dataset building methods.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Take a sample for benchmarking\n",
    "    df_sample = df_fold.head(1000).copy()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test original method (iterrows)\n",
    "    print(\"\\n1. Testing original method (iterrows)...\")\n",
    "    start = time.time()\n",
    "    _ = build_mtl_dataset_optimized(df_sample, chunk_loader, task_cols)\n",
    "    original_time = time.time() - start\n",
    "    results['Original (iterrows)'] = original_time\n",
    "    print(f\"   Time: {original_time:.2f}s\")\n",
    "    \n",
    "    # Test vectorized method\n",
    "    print(\"\\n2. Testing vectorized method...\")\n",
    "    start = time.time()\n",
    "    _ = build_mtl_dataset_vectorized(df_sample, chunk_loader, task_cols)\n",
    "    vectorized_time = time.time() - start\n",
    "    results['Vectorized'] = vectorized_time\n",
    "    print(f\"   Time: {vectorized_time:.2f}s\")\n",
    "    print(f\"   Speedup: {original_time/vectorized_time:.1f}x\")\n",
    "    \n",
    "    # Test parallel method\n",
    "    print(\"\\n3. Testing parallel method...\")\n",
    "    start = time.time()\n",
    "    _ = build_mtl_dataset_parallel(df_sample, chunk_loader, task_cols, n_workers=4)\n",
    "    parallel_time = time.time() - start\n",
    "    results['Parallel'] = parallel_time\n",
    "    print(f\"   Time: {parallel_time:.2f}s\")\n",
    "    print(f\"   Speedup: {original_time/parallel_time:.1f}x\")\n",
    "    \n",
    "    # Test joblib method\n",
    "    try:\n",
    "        print(\"\\n4. Testing joblib method...\")\n",
    "        start = time.time()\n",
    "        _ = build_mtl_dataset_joblib(df_sample, chunk_loader, task_cols, n_jobs=4)\n",
    "        joblib_time = time.time() - start\n",
    "        results['Joblib'] = joblib_time\n",
    "        print(f\"   Time: {joblib_time:.2f}s\")\n",
    "        print(f\"   Speedup: {original_time/joblib_time:.1f}x\")\n",
    "    except ImportError:\n",
    "        print(\"   Joblib not available\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY (1000 samples):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "    baseline = sorted_results[-1][1]\n",
    "    \n",
    "    for method, time_taken in sorted_results:\n",
    "        speedup = baseline / time_taken\n",
    "        print(f\"{method:20s}: {time_taken:6.2f}s (speedup: {speedup:5.1f}x)\")\n",
    "    \n",
    "    fastest = sorted_results[0][0]\n",
    "    print(f\"\\nRECOMMENDATION: Use {fastest} for {sorted_results[-1][1]/sorted_results[0][1]:.1f}x speedup!\")\n",
    "\n",
    "# ============= DROP-IN REPLACEMENT =============\n",
    "def build_mtl_dataset_fast(\n",
    "    df_fold, \n",
    "    chunk_loader, \n",
    "    task_cols=['pKi', 'pEC50', 'pKd', 'pIC50'],\n",
    "    method='auto'\n",
    "):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for build_mtl_dataset_optimized.\n",
    "    Automatically selects the best method.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(df_fold)\n",
    "    \n",
    "    if method == 'auto':\n",
    "        if n_samples < 1000:\n",
    "            # Small dataset, use vectorized\n",
    "            return build_mtl_dataset_vectorized(df_fold, chunk_loader, task_cols)\n",
    "        elif n_samples < 10000:\n",
    "            # Medium dataset, use threading\n",
    "            return build_mtl_dataset_threaded(df_fold, chunk_loader, task_cols)\n",
    "        else:\n",
    "            # Large dataset, use parallel processing\n",
    "            return build_mtl_dataset_parallel(df_fold, chunk_loader, task_cols)\n",
    "    \n",
    "    elif method == 'vectorized':\n",
    "        return build_mtl_dataset_vectorized(df_fold, chunk_loader, task_cols)\n",
    "    elif method == 'parallel':\n",
    "        return build_mtl_dataset_parallel(df_fold, chunk_loader, task_cols)\n",
    "    elif method == 'threaded':\n",
    "        return build_mtl_dataset_threaded(df_fold, chunk_loader, task_cols)\n",
    "    elif method == 'joblib':\n",
    "        return build_mtl_dataset_joblib(df_fold, chunk_loader, task_cols)\n",
    "    elif method == 'numpy':\n",
    "        return build_mtl_dataset_numpy(df_fold, chunk_loader, task_cols)\n",
    "    else:\n",
    "        # Fallback to original\n",
    "        return build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols)\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ff16828-fa1a-468f-a24a-4ed3fe697c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# speed up StructureChunkLoader, so load json faster, same for pdb_structures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcb141-6913-49d6-a911-8b527358b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c428231-9174-4c6f-81c1-7bedc40f120e",
   "metadata": {},
   "source": [
    "### Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d24ecf6e-046a-42e8-90e7-8f8ef2b87525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-GPU training solution that works in Jupyter notebooks\n",
    "Avoids mp.spawn issues by using simpler approaches\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_mtl_dataset_parallel_fixed(df_fold, chunk_loader, task_cols):\n",
    "    \"\"\"Fixed parallel processing\"\"\"\n",
    "    \n",
    "    # Load structures first\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Prepare simple data for parallel processing\n",
    "    records = []\n",
    "    for idx, row in df_fold.iterrows():\n",
    "        if row[\"protein_id\"] in structures_batch:\n",
    "            records.append({\n",
    "                'protein_json': structures_batch[row[\"protein_id\"]],\n",
    "                'drug_sdf': row[\"standardized_ligand_sdf\"],\n",
    "                'tasks': {task: row[task] if task in row else np.nan \n",
    "                         for task in task_cols}\n",
    "            })\n",
    "    \n",
    "    # Process sequentially (featurization is fast enough)\n",
    "    data_list = []\n",
    "    for record in tqdm(records, desc=\"Featurizing\"):\n",
    "        try:\n",
    "            protein = featurize_protein_graph(record['protein_json'])\n",
    "            drug = featurize_drug(record['drug_sdf'])\n",
    "            \n",
    "            entry = {\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "            }\n",
    "            for task, value in record['tasks'].items():\n",
    "                entry[task] = float(value) if not pd.isna(value) else np.nan\n",
    "            \n",
    "            data_list.append(entry)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class CachedFeaturizer:\n",
    "    \"\"\"Cache computed features to disk\"\"\"\n",
    "    def __init__(self, cache_dir=\"./feature_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def get_or_compute(self, key, compute_fn, *args):\n",
    "        cache_file = self.cache_dir / f\"{key}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        result = compute_fn(*args)\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "        return result\n",
    "\n",
    "def build_mtl_dataset_ultra_fast(df_fold, chunk_loader, task_cols):\n",
    "    \"\"\"Ultra-fast dataset building with caching and batch processing\"\"\"\n",
    "    \n",
    "    cache = CachedFeaturizer()\n",
    "    \n",
    "    # Get unique proteins and drugs\n",
    "    unique_proteins = df_fold[\"protein_id\"].unique()\n",
    "    unique_drugs = df_fold[\"standardized_ligand_sdf\"].unique()\n",
    "    \n",
    "    print(f\"Processing {len(unique_proteins)} unique proteins, {len(unique_drugs)} unique drugs\")\n",
    "    \n",
    "    # Batch load all structures\n",
    "    structures_batch = chunk_loader.get_batch(unique_proteins.tolist())\n",
    "    \n",
    "    # Pre-compute all protein features\n",
    "    protein_features = {}\n",
    "    for pid in tqdm(unique_proteins, desc=\"Protein features\"):\n",
    "        if pid in structures_batch:\n",
    "            protein_features[pid] = cache.get_or_compute(\n",
    "                f\"protein_{pid}\",\n",
    "                featurize_protein_graph,\n",
    "                structures_batch[pid]\n",
    "            )\n",
    "    \n",
    "    # Pre-compute all drug features  \n",
    "    drug_features = {}\n",
    "    for drug_sdf in tqdm(unique_drugs, desc=\"Drug features\"):\n",
    "        drug_hash = hashlib.md5(drug_sdf.encode()).hexdigest()[:8]\n",
    "        drug_features[drug_sdf] = cache.get_or_compute(\n",
    "            f\"drug_{drug_hash}\",\n",
    "            featurize_drug,\n",
    "            drug_sdf\n",
    "        )\n",
    "    \n",
    "    # Now just lookup pre-computed features\n",
    "    data_list = []\n",
    "    for _, row in df_fold.iterrows():\n",
    "        pid = row[\"protein_id\"]\n",
    "        drug_sdf = row[\"standardized_ligand_sdf\"]\n",
    "        \n",
    "        if pid not in protein_features or drug_sdf not in drug_features:\n",
    "            continue\n",
    "            \n",
    "        entry = {\n",
    "            \"protein\": protein_features[pid],\n",
    "            \"drug\": drug_features[drug_sdf],\n",
    "        }\n",
    "        \n",
    "        for task in task_cols:\n",
    "            entry[task] = float(row[task]) if task in row and not pd.isna(row[task]) else np.nan\n",
    "            \n",
    "        data_list.append(entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "def build_mtl_dataset_ultra_fast(df_fold, chunk_loader, task_cols):\n",
    "    \"\"\"Version that skips cache checking if already complete\"\"\"\n",
    "    \n",
    "    cache = CachedFeaturizer()\n",
    "    \n",
    "    # Check if this fold is already fully cached\n",
    "    fold_hash = hashlib.md5(f\"{df_fold.index.tolist()}\".encode()).hexdigest()[:8]\n",
    "    fold_cache_file = cache.cache_dir / f\"fold_{fold_hash}_complete.pkl\"\n",
    "    \n",
    "    if fold_cache_file.exists():\n",
    "        print(f\"Loading pre-built dataset from cache...\")\n",
    "        with open(fold_cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(f\"Building dataset (will be cached for next time)...\")\n",
    "    \n",
    "    # Get unique proteins and drugs\n",
    "    unique_proteins = df_fold[\"protein_id\"].unique()\n",
    "    unique_drugs = df_fold[\"standardized_ligand_sdf\"].unique()\n",
    "    \n",
    "    # Only process items not in cache\n",
    "    protein_features = {}\n",
    "    proteins_to_process = []\n",
    "    for pid in unique_proteins:\n",
    "        cache_file = cache.cache_dir / f\"protein_{pid}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                protein_features[pid] = pickle.load(f)\n",
    "        else:\n",
    "            proteins_to_process.append(pid)\n",
    "    \n",
    "    if proteins_to_process:\n",
    "        print(f\"Processing {len(proteins_to_process)} new proteins...\")\n",
    "        structures_batch = chunk_loader.get_batch(proteins_to_process)\n",
    "        for pid in tqdm(proteins_to_process):\n",
    "            if pid in structures_batch:\n",
    "                protein_features[pid] = cache.get_or_compute(\n",
    "                    f\"protein_{pid}\",\n",
    "                    featurize_protein_graph,\n",
    "                    structures_batch[pid]\n",
    "                )\n",
    "    else:\n",
    "        print(f\"All {len(unique_proteins)} proteins already cached!\")\n",
    "    \n",
    "    # Same for drugs\n",
    "    drug_features = {}\n",
    "    drugs_to_process = []\n",
    "    for drug_sdf in unique_drugs:\n",
    "        drug_hash = hashlib.md5(drug_sdf.encode()).hexdigest()[:8]\n",
    "        cache_file = cache.cache_dir / f\"drug_{drug_hash}.pkl\"\n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                drug_features[drug_sdf] = pickle.load(f)\n",
    "        else:\n",
    "            drugs_to_process.append(drug_sdf)\n",
    "    \n",
    "    if drugs_to_process:\n",
    "        print(f\"Processing {len(drugs_to_process)} new drugs...\")\n",
    "        for drug_sdf in tqdm(drugs_to_process):\n",
    "            drug_hash = hashlib.md5(drug_sdf.encode()).hexdigest()[:8]\n",
    "            drug_features[drug_sdf] = cache.get_or_compute(\n",
    "                f\"drug_{drug_hash}\",\n",
    "                featurize_drug,\n",
    "                drug_sdf\n",
    "            )\n",
    "    else:\n",
    "        print(f\"All {len(unique_drugs)} drugs already cached!\")\n",
    "    \n",
    "    # Build dataset\n",
    "    data_list = []\n",
    "    for _, row in df_fold.iterrows():\n",
    "        pid = row[\"protein_id\"]\n",
    "        drug_sdf = row[\"standardized_ligand_sdf\"]\n",
    "        \n",
    "        if pid not in protein_features or drug_sdf not in drug_features:\n",
    "            continue\n",
    "            \n",
    "        entry = {\n",
    "            \"protein\": protein_features[pid],\n",
    "            \"drug\": drug_features[drug_sdf],\n",
    "        }\n",
    "        \n",
    "        for task in task_cols:\n",
    "            entry[task] = float(row[task]) if task in row and not pd.isna(row[task]) else np.nan\n",
    "            \n",
    "        data_list.append(entry)\n",
    "    \n",
    "    # Create and cache the dataset\n",
    "    dataset = MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "    \n",
    "    # Save complete dataset for instant loading next time\n",
    "    with open(fold_cache_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def precompute_all_features(df_full, chunk_loader):\n",
    "    \"\"\"Run this ONCE before training to cache all features\"\"\"\n",
    "    \n",
    "    cache = CachedFeaturizer()\n",
    "    \n",
    "    # Get ALL unique proteins and drugs from entire dataset\n",
    "    all_proteins = df_full[\"protein_id\"].unique()\n",
    "    all_drugs = df_full[\"standardized_ligand_sdf\"].unique()\n",
    "    \n",
    "    print(f\"Pre-computing features for {len(all_proteins)} proteins and {len(all_drugs)} drugs\")\n",
    "    \n",
    "    # Load all structures\n",
    "    structures = chunk_loader.get_batch(all_proteins.tolist())\n",
    "    \n",
    "    # Compute all protein features\n",
    "    for pid in tqdm(all_proteins, desc=\"All proteins\"):\n",
    "        if pid in structures:\n",
    "            cache.get_or_compute(f\"protein_{pid}\", featurize_protein_graph, structures[pid])\n",
    "    \n",
    "    # Compute all drug features\n",
    "    for drug_sdf in tqdm(all_drugs, desc=\"All drugs\"):\n",
    "        drug_hash = hashlib.md5(drug_sdf.encode()).hexdigest()[:8]\n",
    "        cache.get_or_compute(f\"drug_{drug_hash}\", featurize_drug, drug_sdf)\n",
    "    \n",
    "    print(\"✓ All features cached!\")\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============= SOLUTION 1: DataParallel (Works in Notebooks) =============\n",
    "def train_fold_dataparallel(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, batch_size=256, lr=0.0005, patience=20,\n",
    "    use_multiple_gpus=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Using standard DataParallel - works immediately in notebooks\n",
    "    Less efficient than DDP but much simpler and works everywhere\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\")\n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    \n",
    "    # Detect GPUs\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"  Available GPUs: {n_gpus}\")\n",
    "    \n",
    "    if use_multiple_gpus and n_gpus > 1:\n",
    "        device_ids = list(range(n_gpus))\n",
    "        print(f\"  Using GPUs: {device_ids}\")\n",
    "        # Adjust batch size for multiple GPUs\n",
    "        actual_batch_size = batch_size * n_gpus\n",
    "        print(f\"  Batch size per GPU: {batch_size}\")\n",
    "        print(f\"  Total batch size: {actual_batch_size}\")\n",
    "    else:\n",
    "        device_ids = [0]\n",
    "        actual_batch_size = batch_size\n",
    "        print(f\"  Using single GPU: 0\")\n",
    "        print(f\"  Batch size: {actual_batch_size}\")\n",
    "    \n",
    "    device = torch.device(f\"cuda:{device_ids[0]}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    # train_dataset = build_mtl_dataset_optimized(df_train, chunk_loader, task_cols)\n",
    "    # valid_dataset = build_mtl_dataset_optimized(df_valid, chunk_loader, task_cols)\n",
    "    # test_dataset = build_mtl_dataset_optimized(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "\n",
    "    \n",
    "    # For maximum speed:\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(\n",
    "        df_train, \n",
    "        chunk_loader, \n",
    "        task_cols\n",
    "    )\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(\n",
    "        df_valid, \n",
    "        chunk_loader, \n",
    "        task_cols\n",
    "    )\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(\n",
    "        df_test, \n",
    "        chunk_loader, \n",
    "        task_cols\n",
    "    )\n",
    "    print(\"\\nLoading datasets...\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "            batch_size=actual_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Keep at 0 in notebooks\n",
    "        pin_memory=False,  # Set to False if having issues\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=actual_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Keep at 0 in notebooks\n",
    "        pin_memory=False,  # Set to False if having issues\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,  \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Keep at 0 in notebooks\n",
    "        pin_memory=False,  # Set to False if having issues\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Initializing model...\")\n",
    "    model = MTL_DTAModel(\n",
    "        task_names=task_cols,\n",
    "        prot_emb_dim=1280,\n",
    "        prot_gcn_dims=[128, 256, 256],\n",
    "        prot_fc_dims=[1024, 128],\n",
    "        drug_node_in_dim=[66, 1],\n",
    "        drug_node_h_dims=[128, 64],\n",
    "        drug_fc_dims=[1024, 128],\n",
    "        mlp_dims=[1024, 512],\n",
    "        mlp_dropout=0.25\n",
    "    )\n",
    "    \n",
    "    # Apply DataParallel if using multiple GPUs\n",
    "    if use_multiple_gpus and n_gpus > 1:\n",
    "        model = nn.DataParallel(model, device_ids=device_ids)\n",
    "        print(f\"✓ Model wrapped with DataParallel\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"✓ Model loaded on GPU(s)\")\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges).to(device)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    pbar = tqdm(range(n_epochs), desc=f\"Training\", ncols=100)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # ========== TRAINING PHASE ==========\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), desc =\"batch\"):\n",
    "            try:\n",
    "                # Move batch to GPU\n",
    "                xd = batch['drug'].to(device, non_blocking=True)\n",
    "                xp = batch['protein'].to(device, non_blocking=True)\n",
    "                y = batch['y'].to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                n_train_batches += 1\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if batch_idx % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"\\nOOM at batch {batch_idx}, clearing cache and skipping...\")\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        avg_train_loss = train_loss / max(n_train_batches, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # ========== VALIDATION PHASE ==========\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        n_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device, non_blocking=True)\n",
    "                xp = batch['protein'].to(device, non_blocking=True)\n",
    "                y = batch['y'].to(device, non_blocking=True)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                n_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / max(n_val_batches, 1)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'train': f\"{avg_train_loss:.4f}\",\n",
    "            'val': f\"{avg_val_loss:.4f}\",\n",
    "            'best': f\"{best_val_loss:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            # Handle both DataParallel and regular model\n",
    "            if hasattr(model, 'module'):\n",
    "                best_model_state = model.module.state_dict()\n",
    "            else:\n",
    "                best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(best_model_state)\n",
    "        else:\n",
    "            model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # ========== EVALUATION PHASE ==========\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Collect predictions for each task\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_preds = pred[mask, i].cpu().numpy()\n",
    "                    task_trues = y[mask, i].cpu().numpy()\n",
    "                    task_predictions[task].extend(task_preds)\n",
    "                    task_targets[task].extend(task_trues)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del test_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "\n",
    "# ============= SOLUTION 2: Distributed Training via Script File =============\n",
    "def create_training_script(output_path=\"train_ddp.py\"):\n",
    "    \"\"\"\n",
    "    Create a standalone Python script for DDP training\n",
    "    Run this script outside the notebook with:\n",
    "    torchrun --nproc_per_node=4 train_ddp.py\n",
    "    \"\"\"\n",
    "    \n",
    "    script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Standalone DDP training script for GNN\n",
    "Run with: torchrun --nproc_per_node=4 train_ddp.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "# Add your other imports here\n",
    "\n",
    "def setup_ddp():\n",
    "    \"\"\"Initialize DDP from torchrun environment variables\"\"\"\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    return rank, world_size, local_rank\n",
    "\n",
    "def cleanup_ddp():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train_with_ddp():\n",
    "    rank, world_size, local_rank = setup_ddp()\n",
    "    \n",
    "    # Your model and training code here\n",
    "    # Use local_rank as the device\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = YourModel().to(device)\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "    \n",
    "    # Training loop...\n",
    "    \n",
    "    cleanup_ddp()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_with_ddp()\n",
    "'''\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(f\"Created training script: {output_path}\")\n",
    "    print(f\"Run with: torchrun --nproc_per_node=4 {output_path}\")\n",
    "\n",
    "# ============= SOLUTION 3: Gradient Accumulation for Large Batches =============\n",
    "def train_with_gradient_accumulation(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, base_batch_size=32, \n",
    "    accumulation_steps=4,  # Simulate batch_size = 32 * 4 = 128\n",
    "    lr=0.0005, patience=20,\n",
    "    gpu_id=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Use gradient accumulation to simulate larger batch sizes\n",
    "    This is memory efficient and works on single GPU\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    effective_batch_size = base_batch_size * accumulation_steps\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Gradient Accumulation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Base batch size: {base_batch_size}\")\n",
    "    print(f\"  Accumulation steps: {accumulation_steps}\")\n",
    "    print(f\"  Effective batch size: {effective_batch_size}\")\n",
    "    print(f\"  Using GPU: {gpu_id}\")\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = build_mtl_dataset_optimized(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_optimized(df_valid, chunk_loader, task_cols)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=base_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=base_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = MTL_DTAModel(\n",
    "        task_names=task_cols,\n",
    "        prot_emb_dim=1280,\n",
    "        prot_gcn_dims=[128, 256, 256],\n",
    "        prot_fc_dims=[1024, 128],\n",
    "        drug_node_in_dim=[66, 1],\n",
    "        drug_node_h_dims=[128, 64],\n",
    "        drug_fc_dims=[1024, 128],\n",
    "        mlp_dims=[1024, 512],\n",
    "        mlp_dropout=0.25\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges).to(device)\n",
    "    \n",
    "    # Training loop with gradient accumulation\n",
    "    print(\"\\nStarting training with gradient accumulation...\")\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(xd, xp)\n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            # Normalize loss by accumulation steps\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            train_loss += loss.item() * accumulation_steps\n",
    "            n_batches += 1\n",
    "            \n",
    "            # Update weights every accumulation_steps\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Handle remaining gradients\n",
    "        if n_batches % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation...\n",
    "        # [Add validation code similar to above]\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d79865bd-7415-4cea-a875-058a1b1d696f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing features for 50000 proteins and 50000 drugs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All proteins: 100%|██████████| 50000/50000 [24:45<00:00, 33.66it/s]\n",
      "All drugs: 100%|██████████| 50000/50000 [02:39<00:00, 313.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All features cached!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run once before training\n",
    "precompute_all_features(df_clean, chunk_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d1918-35a2-43ce-90f2-c53abff258de",
   "metadata": {},
   "source": [
    "# Multi gpu ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0c4c49c-c786-4572-89b5-c61903a4a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simplified multi-GPU training that actually works\n",
    "Each GPU processes independent batches, models sync at epoch end\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import math\n",
    "\n",
    "def collate_fn_mtl(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for MTL_DTA that properly batches the data\n",
    "    \"\"\"\n",
    "    # Batch the drug and protein graphs\n",
    "    drug_list = [item['drug'] for item in batch]\n",
    "    protein_list = [item['protein'] for item in batch]\n",
    "    \n",
    "    # Create batched graphs\n",
    "    drug_batch = Batch.from_data_list(drug_list)\n",
    "    protein_batch = Batch.from_data_list(protein_list)\n",
    "    \n",
    "    # Stack the target values\n",
    "    y_list = [item['y'] for item in batch]\n",
    "    y_batch = torch.stack(y_list)\n",
    "    \n",
    "    return {\n",
    "        'drug': drug_batch,\n",
    "        'protein': protein_batch,\n",
    "        'y': y_batch\n",
    "    }\n",
    "\n",
    "class SimpleMultiGPUTrainer:\n",
    "    \"\"\"\n",
    "    Simple approach: each GPU gets complete batches sequentially\n",
    "    Models sync at epoch end\n",
    "    \"\"\"\n",
    "    def __init__(self, model_class, model_kwargs, task_ranges, n_gpus=None):\n",
    "        self.n_gpus = n_gpus or torch.cuda.device_count()\n",
    "        self.task_ranges = task_ranges\n",
    "        \n",
    "        # Create models and optimizers for each GPU\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.devices = []\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            device = torch.device(f'cuda:{gpu_idx}')\n",
    "            self.devices.append(device)\n",
    "            \n",
    "            model = model_class(**model_kwargs).to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            self.optimizers.append(optimizer)\n",
    "        \n",
    "        print(f\"Created {self.n_gpus} models on GPUs\")\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train one epoch - batches go to GPUs in round-robin fashion\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        losses = []\n",
    "        gpu_batch_counts = [0] * self.n_gpus\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            # Assign batch to GPU in round-robin\n",
    "            gpu_idx = batch_idx % self.n_gpus\n",
    "            device = self.devices[gpu_idx]\n",
    "            model = self.models[gpu_idx]\n",
    "            optimizer = self.optimizers[gpu_idx]\n",
    "            \n",
    "            # Move batch to GPU\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Compute loss with mask for NaN values\n",
    "            mask = ~torch.isnan(y)\n",
    "            if mask.sum() > 0:\n",
    "                pred_masked = pred[mask]\n",
    "                y_masked = y[mask]\n",
    "                loss = torch.nn.functional.mse_loss(pred_masked, y_masked)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                gpu_batch_counts[gpu_idx] += 1\n",
    "        \n",
    "        # Sync models at end of epoch\n",
    "        self.sync_models()\n",
    "        \n",
    "        # Print GPU usage stats\n",
    "        for gpu_idx, count in enumerate(gpu_batch_counts):\n",
    "            print(f\"  GPU {gpu_idx}: processed {count} batches\")\n",
    "        \n",
    "        return np.mean(losses) if losses else 0.0\n",
    "    \n",
    "    def sync_models(self):\n",
    "        \"\"\"Average all model parameters\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get state dict from first model\n",
    "            avg_state = self.models[0].state_dict()\n",
    "            \n",
    "            # Average parameters from all models\n",
    "            for key in avg_state.keys():\n",
    "                params = []\n",
    "                for model in self.models:\n",
    "                    params.append(model.state_dict()[key].to('cuda:0'))\n",
    "                avg_state[key] = torch.stack(params).mean(dim=0)\n",
    "            \n",
    "            # Load averaged parameters to all models\n",
    "            for model in self.models:\n",
    "                model.load_state_dict(avg_state)\n",
    "    \n",
    "    def validate(self, valid_loader):\n",
    "        \"\"\"Validate using first model\"\"\"\n",
    "        model = self.models[0]\n",
    "        device = self.devices[0]\n",
    "        model.eval()\n",
    "        \n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                \n",
    "                mask = ~torch.isnan(y)\n",
    "                if mask.sum() > 0:\n",
    "                    loss = torch.nn.functional.mse_loss(pred[mask], y[mask])\n",
    "                    losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(losses) if losses else 0.0\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Return the first model\"\"\"\n",
    "        return self.models[0]\n",
    "\n",
    "\n",
    "def train_fold_simple_multi(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=30, batch_size=16, lr=0.0001, patience=20,\n",
    "    n_gpus=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple multi-GPU training that works\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Simple Multi-GPU\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\") \n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    print(f\"  Using {n_gpus} GPUs\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # Build datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Valid batches: {len(valid_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    model_kwargs = {\n",
    "        'task_names': task_cols,\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "    \n",
    "    trainer = SimpleMultiGPUTrainer(\n",
    "        model_class=MTL_DTAModel,\n",
    "        model_kwargs=model_kwargs,\n",
    "        task_ranges=task_ranges,\n",
    "        n_gpus=n_gpus\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_loss = trainer.train_epoch(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = trainer.validate(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = trainer.get_model().state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Cleanup\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.get_model().load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    model = trainer.get_model()\n",
    "    device = trainer.devices[0]\n",
    "    model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Collect predictions\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_predictions[task].extend(pred[mask, i].cpu().numpy())\n",
    "                    task_targets[task].extend(y[mask, i].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5130d-cdea-46ca-ad48-2e96347ef513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80f3ffc3-82cb-4183-abbf-3ed32730eee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GRAPH NEURAL NETWORK TRAINING - NOTEBOOK-COMPATIBLE MULTI-GPU\n",
      "======================================================================\n",
      "GPUs available: 8\n",
      "\n",
      "Configuration:\n",
      "  Batch size per GPU: 32\n",
      "  Epochs: 200\n",
      "  Learning rate: 0.0001\n",
      "  Patience: 300\n",
      "  Folds: 5\n",
      "\n",
      "Starting 5-fold cross-validation...\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "FOLD 1/5 - Simple Multi-GPU\n",
      "============================================================\n",
      "  Train: 36000 samples\n",
      "  Valid: 4000 samples\n",
      "  Test:  10000 samples\n",
      "  Using 8 GPUs\n",
      "  Batch size: 32\n",
      "\n",
      "Building datasets...\n",
      "Loading pre-built dataset from cache...\n",
      "Loading pre-built dataset from cache...\n",
      "Loading pre-built dataset from cache...\n",
      "✓ DataLoaders created\n",
      "  Train batches: 1125\n",
      "  Valid batches: 125\n",
      "  Test batches: 313\n",
      "Created 8 models on GPUs\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 1/200: Train=4.7145, Val=48.9230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [05:12<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 2/200: Train=12.4613, Val=25.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 3/200: Train=2.8764, Val=1.5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:58<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 4/200: Train=1.6973, Val=1.5354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 5/200: Train=1.6682, Val=1.5175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 6/200: Train=1.6628, Val=1.5104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [05:06<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 7/200: Train=1.6615, Val=1.5202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:52<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 14/200: Train=1.5720, Val=1.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:53<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 15/200: Train=1.5766, Val=1.4288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:54<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 16/200: Train=1.5680, Val=1.4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 726/1125 [03:22<01:42,  3.89it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 23/200: Train=1.5664, Val=1.4013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 24/200: Train=1.5425, Val=1.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:58<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 25/200: Train=1.5447, Val=1.4078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 648/1125 [02:50<02:07,  3.75it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [05:08<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 32/200: Train=1.5242, Val=1.3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 33/200: Train=1.5300, Val=1.3761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:54<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 34/200: Train=1.5117, Val=1.3852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 369/1125 [01:35<03:13,  3.91it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 41/200: Train=1.5044, Val=1.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [05:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 42/200: Train=1.5014, Val=1.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 43/200: Train=1.5050, Val=1.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 121/1125 [00:31<04:23,  3.81it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 50/200: Train=1.4772, Val=1.3690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 51/200: Train=1.4770, Val=1.3514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 931/1125 [04:14<00:56,  3.43it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:58<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 58/200: Train=1.4435, Val=1.3430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 59/200: Train=1.4662, Val=1.3362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 60/200: Train=1.4586, Val=1.3441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 550/1125 [02:23<02:32,  3.76it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:54<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 65/200: Train=1.4308, Val=1.3273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 66/200: Train=1.4432, Val=1.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 1122/1125 [05:07<00:00,  3.92it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 69/200: Train=1.4432, Val=1.3230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 452/1125 [01:59<02:54,  3.86it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 74/200: Train=1.4285, Val=1.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:53<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 75/200: Train=1.4313, Val=1.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 957/1125 [04:10<00:44,  3.80it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:55<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 78/200: Train=1.4063, Val=1.3087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 175/1125 [00:45<04:31,  3.50it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 83/200: Train=1.4148, Val=1.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 128/200: Train=1.3138, Val=1.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:59<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 129/200: Train=1.3123, Val=1.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:56<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 130/200: Train=1.3064, Val=1.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 131/200: Train=1.3136, Val=1.2632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1125/1125 [05:07<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0: processed 141 batches\n",
      "  GPU 1: processed 141 batches\n",
      "  GPU 2: processed 141 batches\n",
      "  GPU 3: processed 141 batches\n",
      "  GPU 4: processed 141 batches\n",
      "  GPU 5: processed 140 batches\n",
      "  GPU 6: processed 140 batches\n",
      "  GPU 7: processed 140 batches\n",
      "Epoch 132/200: Train=1.3157, Val=1.2550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 1074/1125 [04:41<00:13,  3.81it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============= MAIN EXECUTION FOR NOTEBOOKS =============\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GRAPH NEURAL NETWORK TRAINING - NOTEBOOK-COMPATIBLE MULTI-GPU\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32  # Per GPU batch size\n",
    "N_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "PATIENCE = 300\n",
    "N_FOLDS = 5\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch size per GPU: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {N_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Patience: {PATIENCE}\")\n",
    "print(f\"  Folds: {N_FOLDS}\")\n",
    "\n",
    "# Initialize results storage\n",
    "cv_results = {\n",
    "    task: {\n",
    "        'r2_list': [],\n",
    "        'rmse_list': [],\n",
    "        'all_predictions': [],\n",
    "        'all_targets': []\n",
    "    } for task in task_cols\n",
    "}\n",
    "\n",
    "# K-Fold cross-validation\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nStarting {N_FOLDS}-fold cross-validation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df_clean)):\n",
    "    # Split data\n",
    "    df_train = df_clean.iloc[train_idx].reset_index(drop=True)\n",
    "    df_test = df_clean.iloc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Create validation set (10% of training)\n",
    "    valid_size = int(0.1 * len(df_train))\n",
    "    df_valid = df_train.sample(n=valid_size, random_state=42)\n",
    "    df_train = df_train.drop(df_valid.index).reset_index(drop=True)\n",
    "    \n",
    "    # Train fold using DataParallel (works in notebooks)\n",
    "    fold_results, train_losses, val_losses = train_fold_simple_multi(\n",
    "    fold_idx, N_FOLDS,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    N_EPOCHS, BATCH_SIZE, LEARNING_RATE, PATIENCE,\n",
    "    n_gpus=torch.cuda.device_count()\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    for task in task_cols:\n",
    "        if task in fold_results:\n",
    "            cv_results[task]['r2_list'].append(fold_results[task]['r2'])\n",
    "            cv_results[task]['rmse_list'].append(fold_results[task]['rmse'])\n",
    "            cv_results[task]['all_predictions'].extend(fold_results[task]['predictions'])\n",
    "            cv_results[task]['all_targets'].extend(fold_results[task]['targets'])\n",
    "    \n",
    "    # Clean up after each fold\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Print final cross-validation results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for task in task_cols:\n",
    "    if cv_results[task]['r2_list']:\n",
    "        mean_r2 = np.mean(cv_results[task]['r2_list'])\n",
    "        std_r2 = np.std(cv_results[task]['r2_list'])\n",
    "        mean_rmse = np.mean(cv_results[task]['rmse_list'])\n",
    "        std_rmse = np.std(cv_results[task]['rmse_list'])\n",
    "        \n",
    "        print(f\"{task:20s} | RMSE: {mean_rmse:.3f}±{std_rmse:.3f} | R²: {mean_r2:.3f}±{std_r2:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Training completed successfully!\")\n",
    "\n",
    "# Optional: Create a script for DDP training outside notebook\n",
    "# create_training_script(\"train_ddp.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d17f8-2789-4bb9-9776-f70b6dc3fe67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Epoch 57/100: Train=1.3756, Val=1.2902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c17cc-9119-4d68-b8e1-a3e16a4e774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 28/100: Train=1.4705, Val=1.3626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241854a-41f6-44ec-9747-e1d1731266c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 100/100: Train=1.2507, Val=1.2538\n",
    "\n",
    "Evaluating on test set...\n",
    "Testing: 100%|██████████| 157/157 [00:49<00:00,  3.19it/s]\n",
    "\n",
    "==================================================\n",
    "Fold 1 Results:\n",
    "==================================================\n",
    "pKi                  | RMSE:  1.200 | R²:  0.298 | n= 2053\n",
    "pEC50                | RMSE:  1.099 | R²:  0.296 | n=  410\n",
    "pKd                  | RMSE:  1.360 | R²:  0.212 | n=  346\n",
    "pKd (Wang, FEP)      | RMSE:  0.688 | R²:  0.420 | n=   30\n",
    "pIC50                | RMSE:  1.125 | R²:  0.266 | n= 5674\n",
    "potency              | RMSE:  1.033 | R²:  0.365 | n= 1486\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3279c1e-015a-41a4-a779-701eabb7c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "5min per epoch = 500 minutes = 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707b7a-fe1e-49ff-8741-8f81f7387bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16dc82-0fd4-479a-a4a2-9ce00968a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6460e8a-45a5-449d-9802-23e6971f3488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376d5d4-8b03-4470-9b56-f8be83eb9996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafc55e-d2d8-4983-8c56-1b8db955d01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5bebf-0f35-417a-a8b7-b6de5f960b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e39a49-1d26-48f2-8d2b-bbfa682df5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab99e01-f5d0-4a20-8306-b87f400c5a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-GPU training with distributed batches and epoch-level synchronization\n",
    "Each GPU processes its own batch subset, losses are combined on GPU 0 at epoch end\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "class MultiGPUDistributedBatchTrainer:\n",
    "    \"\"\"\n",
    "    Custom multi-GPU trainer that:\n",
    "    1. Maintains separate model copies on each GPU\n",
    "    2. Distributes batches across GPUs\n",
    "    3. Aggregates losses on GPU 0 at epoch end\n",
    "    4. Syncs models after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_kwargs, n_gpus=None, master_gpu=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: Class of the model (e.g., MTL_DTAModel)\n",
    "            model_kwargs: Dictionary of model initialization parameters\n",
    "            n_gpus: Number of GPUs to use (None = use all available)\n",
    "            master_gpu: Index of the master GPU (default 0)\n",
    "        \"\"\"\n",
    "        self.n_gpus = n_gpus or torch.cuda.device_count()\n",
    "        self.master_gpu = master_gpu\n",
    "        self.model_class = model_class\n",
    "        self.model_kwargs = model_kwargs\n",
    "        \n",
    "        # Create model on each GPU\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.devices = []\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            device = torch.device(f'cuda:{gpu_idx}')\n",
    "            self.devices.append(device)\n",
    "            \n",
    "            # Create model on this GPU\n",
    "            model = model_class(**model_kwargs).to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            # Create optimizer for this model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            self.optimizers.append(optimizer)\n",
    "        \n",
    "        print(f\"Initialized {self.n_gpus} models on GPUs: {list(range(self.n_gpus))}\")\n",
    "        print(f\"Master GPU: {self.master_gpu}\")\n",
    "    \n",
    "    def sync_models_from_master(self):\n",
    "        \"\"\"Copy master model weights to all other GPUs\"\"\"\n",
    "        master_state = self.models[self.master_gpu].state_dict()\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            if gpu_idx != self.master_gpu:\n",
    "                # Copy state dict to other GPU\n",
    "                self.models[gpu_idx].load_state_dict(master_state)\n",
    "    \n",
    "    def distribute_batch(self, batch, batch_size):\n",
    "        \"\"\"\n",
    "        Distribute a batch across multiple GPUs\n",
    "        \n",
    "        Returns:\n",
    "            List of sub-batches, one for each GPU\n",
    "        \"\"\"\n",
    "        # Calculate sub-batch size for each GPU\n",
    "        samples_per_gpu = batch_size // self.n_gpus\n",
    "        remainder = batch_size % self.n_gpus\n",
    "        \n",
    "        sub_batches = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            # Add one extra sample to first GPUs if remainder exists\n",
    "            gpu_batch_size = samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "            \n",
    "            if gpu_batch_size > 0:\n",
    "                # Extract sub-batch\n",
    "                sub_batch = {\n",
    "                    'drug': batch['drug'][start_idx:start_idx + gpu_batch_size],\n",
    "                    'protein': batch['protein'][start_idx:start_idx + gpu_batch_size],\n",
    "                    'y': batch['y'][start_idx:start_idx + gpu_batch_size]\n",
    "                }\n",
    "                sub_batches.append(sub_batch)\n",
    "                start_idx += gpu_batch_size\n",
    "            else:\n",
    "                sub_batches.append(None)\n",
    "        \n",
    "        return sub_batches\n",
    "    \n",
    "    def forward_on_gpu(self, gpu_idx, sub_batch, criterion):\n",
    "        \"\"\"\n",
    "        Forward pass on a specific GPU\n",
    "        \n",
    "        Returns:\n",
    "            loss value and gradients\n",
    "        \"\"\"\n",
    "        if sub_batch is None:\n",
    "            return 0.0, None\n",
    "        \n",
    "        device = self.devices[gpu_idx]\n",
    "        model = self.models[gpu_idx]\n",
    "        \n",
    "        # Move data to GPU\n",
    "        xd = sub_batch['drug'].to(device, non_blocking=True)\n",
    "        xp = sub_batch['protein'].to(device, non_blocking=True)\n",
    "        y = sub_batch['y'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(xd, xp)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        return loss, (xd, xp, y, pred)\n",
    "    \n",
    "    def train_epoch(self, train_loader, criterion, epoch_idx):\n",
    "        \"\"\"\n",
    "        Train one epoch with distributed batches\n",
    "        \"\"\"\n",
    "        # Set all models to training mode\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        batch_losses_per_gpu = [[] for _ in range(self.n_gpus)]\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch_idx}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            batch_size = batch['y'].shape[0]\n",
    "            \n",
    "            # Distribute batch across GPUs\n",
    "            sub_batches = self.distribute_batch(batch, batch_size)\n",
    "            \n",
    "            # Process each sub-batch on its GPU\n",
    "            gpu_losses = []\n",
    "            gpu_data = []\n",
    "            \n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                if sub_batches[gpu_idx] is not None:\n",
    "                    loss, data = self.forward_on_gpu(gpu_idx, sub_batches[gpu_idx], criterion)\n",
    "                    gpu_losses.append(loss)\n",
    "                    gpu_data.append((gpu_idx, loss, data))\n",
    "                    batch_losses_per_gpu[gpu_idx].append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "            \n",
    "            # Backward pass on each GPU\n",
    "            for gpu_idx, loss, _ in gpu_data:\n",
    "                if loss > 0:\n",
    "                    self.optimizers[gpu_idx].zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    self.optimizers[gpu_idx].step()\n",
    "            \n",
    "            # Record batch loss (average across GPUs)\n",
    "            if gpu_losses:\n",
    "                avg_batch_loss = sum([l.item() if hasattr(l, 'item') else l for l in gpu_losses]) / len(gpu_losses)\n",
    "                epoch_losses.append(avg_batch_loss)\n",
    "                pbar.set_postfix({'loss': f'{avg_batch_loss:.4f}'})\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # At end of epoch: aggregate losses and sync models\n",
    "        epoch_avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "        \n",
    "        # Average gradients across all GPUs and update master model\n",
    "        self.aggregate_and_update_master()\n",
    "        \n",
    "        # Sync all models from master\n",
    "        self.sync_models_from_master()\n",
    "        \n",
    "        return epoch_avg_loss, batch_losses_per_gpu\n",
    "    \n",
    "    def aggregate_and_update_master(self):\n",
    "        \"\"\"\n",
    "        Average parameters from all GPUs and update master model\n",
    "        This is more sophisticated than just copying - it averages the learned updates\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_state = master_model.state_dict()\n",
    "        \n",
    "        # Average parameters from all models\n",
    "        averaged_state = {}\n",
    "        for key in master_state.keys():\n",
    "            # Sum parameters from all GPUs\n",
    "            param_sum = None\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                param = self.models[gpu_idx].state_dict()[key].to(self.devices[self.master_gpu])\n",
    "                if param_sum is None:\n",
    "                    param_sum = param.clone()\n",
    "                else:\n",
    "                    param_sum += param\n",
    "            \n",
    "            # Average\n",
    "            averaged_state[key] = param_sum / self.n_gpus\n",
    "        \n",
    "        # Update master model with averaged parameters\n",
    "        master_model.load_state_dict(averaged_state)\n",
    "    \n",
    "    def validate(self, valid_loader, criterion):\n",
    "        \"\"\"\n",
    "        Validate using only the master model\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_device = self.devices[self.master_gpu]\n",
    "        \n",
    "        master_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(master_device)\n",
    "                xp = batch['protein'].to(master_device)\n",
    "                y = batch['y'].to(master_device)\n",
    "                \n",
    "                pred = master_model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(val_losses) if val_losses else 0.0\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the master model\"\"\"\n",
    "        return self.models[self.master_gpu]\n",
    "\n",
    "\n",
    "def train_fold_distributed_batch(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, batch_size=256, lr=0.0005, patience=20,\n",
    "    n_gpus=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a fold using distributed batch multi-GPU approach\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Distributed Batch Multi-GPU\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\")\n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    \n",
    "    # Detect GPUs\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    n_gpus = min(n_gpus or available_gpus, available_gpus)\n",
    "    print(f\"  Using {n_gpus} GPUs out of {available_gpus} available\")\n",
    "    \n",
    "    # Adjust batch size for multiple GPUs\n",
    "    # Each GPU will process batch_size/n_gpus samples\n",
    "    print(f\"  Total batch size: {batch_size}\")\n",
    "    print(f\"  Batch size per GPU: ~{batch_size // n_gpus}\")\n",
    "    \n",
    "    # Build datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    \n",
    "    # Initialize multi-GPU trainer\n",
    "    model_kwargs = {\n",
    "        'task_names': task_cols,\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "    \n",
    "    trainer = MultiGPUDistributedBatchTrainer(\n",
    "        model_class=MTL_DTAModel,\n",
    "        model_kwargs=model_kwargs,\n",
    "        n_gpus=n_gpus,\n",
    "        master_gpu=0\n",
    "    )\n",
    "    \n",
    "    # Create criterion on master GPU\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges).to(trainer.devices[trainer.master_gpu])\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting distributed batch training...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train epoch\n",
    "        train_loss, gpu_losses = trainer.train_epoch(train_loader, criterion, epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = trainer.validate(valid_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "        \n",
    "        # Print per-GPU statistics\n",
    "        for gpu_idx, losses in enumerate(gpu_losses):\n",
    "            if losses:\n",
    "                avg_gpu_loss = np.mean(losses)\n",
    "                print(f\"  GPU {gpu_idx}: avg loss = {avg_gpu_loss:.4f} ({len(losses)} batches)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = trainer.get_best_model().state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.get_best_model().load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    master_model = trainer.get_best_model()\n",
    "    master_device = trainer.devices[trainer.master_gpu]\n",
    "    master_model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(master_device)\n",
    "            xp = batch['protein'].to(master_device)\n",
    "            y = batch['y'].to(master_device)\n",
    "            \n",
    "            pred = master_model(xd, xp)\n",
    "            \n",
    "            # Collect predictions for each task\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_preds = pred[mask, i].cpu().numpy()\n",
    "                    task_trues = y[mask, i].cpu().numpy()\n",
    "                    task_predictions[task].extend(task_preds)\n",
    "                    task_targets[task].extend(task_trues)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140ab64-4fcc-4152-8ab2-7e10b565e1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-GPU training with distributed batches and epoch-level synchronization\n",
    "Each GPU processes its own batch subset, losses are combined on GPU 0 at epoch end\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "class MultiGPUDistributedBatchTrainer:\n",
    "    \"\"\"\n",
    "    Custom multi-GPU trainer that:\n",
    "    1. Maintains separate model copies on each GPU\n",
    "    2. Distributes batches across GPUs\n",
    "    3. Aggregates losses on GPU 0 at epoch end\n",
    "    4. Syncs models after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_kwargs, n_gpus=None, master_gpu=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: Class of the model (e.g., MTL_DTAModel)\n",
    "            model_kwargs: Dictionary of model initialization parameters\n",
    "            n_gpus: Number of GPUs to use (None = use all available)\n",
    "            master_gpu: Index of the master GPU (default 0)\n",
    "        \"\"\"\n",
    "        self.n_gpus = n_gpus or torch.cuda.device_count()\n",
    "        self.master_gpu = master_gpu\n",
    "        self.model_class = model_class\n",
    "        self.model_kwargs = model_kwargs\n",
    "        \n",
    "        # Create model on each GPU\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.devices = []\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            device = torch.device(f'cuda:{gpu_idx}')\n",
    "            self.devices.append(device)\n",
    "            \n",
    "            # Create model on this GPU\n",
    "            model = model_class(**model_kwargs).to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            # Create optimizer for this model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            self.optimizers.append(optimizer)\n",
    "        \n",
    "        print(f\"Initialized {self.n_gpus} models on GPUs: {list(range(self.n_gpus))}\")\n",
    "        print(f\"Master GPU: {self.master_gpu}\")\n",
    "    \n",
    "    def sync_models_from_master(self):\n",
    "        \"\"\"Copy master model weights to all other GPUs\"\"\"\n",
    "        master_state = self.models[self.master_gpu].state_dict()\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            if gpu_idx != self.master_gpu:\n",
    "                # Copy state dict to other GPU\n",
    "                self.models[gpu_idx].load_state_dict(master_state)\n",
    "    \n",
    "    def distribute_batch(self, batch, batch_size):\n",
    "        \"\"\"\n",
    "        Distribute a batch across multiple GPUs\n",
    "        Handles both regular tensors and torch_geometric Batch objects\n",
    "        \n",
    "        Returns:\n",
    "            List of sub-batches, one for each GPU\n",
    "        \"\"\"\n",
    "        import torch_geometric\n",
    "        \n",
    "        # Check if we're dealing with geometric data\n",
    "        if isinstance(batch['drug'], list):\n",
    "            # Handle list of Data objects from DataLoader\n",
    "            sub_batches = []\n",
    "            samples_per_gpu = len(batch['drug']) // self.n_gpus\n",
    "            remainder = len(batch['drug']) % self.n_gpus\n",
    "            \n",
    "            start_idx = 0\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                gpu_batch_size = samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if gpu_batch_size > 0:\n",
    "                    end_idx = start_idx + gpu_batch_size\n",
    "                    \n",
    "                    # Create Batch objects for geometric data\n",
    "                    drug_list = batch['drug'][start_idx:end_idx]\n",
    "                    protein_list = batch['protein'][start_idx:end_idx]\n",
    "                    \n",
    "                    drug_batch = torch_geometric.data.Batch.from_data_list(drug_list)\n",
    "                    protein_batch = torch_geometric.data.Batch.from_data_list(protein_list)\n",
    "                    \n",
    "                    # Handle y values\n",
    "                    y_tensor = torch.stack([batch['y'][i] for i in range(start_idx, end_idx)])\n",
    "                    \n",
    "                    sub_batch = {\n",
    "                        'drug': drug_batch,\n",
    "                        'protein': protein_batch,\n",
    "                        'y': y_tensor\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                    start_idx = end_idx\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "                    \n",
    "        elif hasattr(batch['drug'], 'batch'):\n",
    "            # Handle pre-batched geometric data\n",
    "            # Get batch assignments\n",
    "            drug_batch_tensor = batch['drug'].batch\n",
    "            protein_batch_tensor = batch['protein'].batch\n",
    "            \n",
    "            sub_batches = []\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                # Calculate which samples go to this GPU\n",
    "                samples_per_gpu = batch_size // self.n_gpus\n",
    "                remainder = batch_size % self.n_gpus\n",
    "                \n",
    "                start_sample = gpu_idx * samples_per_gpu + min(gpu_idx, remainder)\n",
    "                end_sample = start_sample + samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if start_sample < batch_size:\n",
    "                    # Extract nodes belonging to these samples\n",
    "                    drug_mask = (drug_batch_tensor >= start_sample) & (drug_batch_tensor < end_sample)\n",
    "                    protein_mask = (protein_batch_tensor >= start_sample) & (protein_batch_tensor < end_sample)\n",
    "                    \n",
    "                    # Create sub-batch (this is simplified - may need more sophisticated slicing)\n",
    "                    sub_batch = {\n",
    "                        'drug': batch['drug'],  # Keep full batch for now\n",
    "                        'protein': batch['protein'],  # Keep full batch for now\n",
    "                        'y': batch['y'][start_sample:end_sample]\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "        else:\n",
    "            # Handle regular tensor batches\n",
    "            samples_per_gpu = batch_size // self.n_gpus\n",
    "            remainder = batch_size % self.n_gpus\n",
    "            \n",
    "            sub_batches = []\n",
    "            start_idx = 0\n",
    "            \n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                gpu_batch_size = samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if gpu_batch_size > 0:\n",
    "                    sub_batch = {\n",
    "                        'drug': batch['drug'][start_idx:start_idx + gpu_batch_size],\n",
    "                        'protein': batch['protein'][start_idx:start_idx + gpu_batch_size],\n",
    "                        'y': batch['y'][start_idx:start_idx + gpu_batch_size]\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                    start_idx += gpu_batch_size\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "        \n",
    "        return sub_batches\n",
    "    \n",
    "    def forward_on_gpu(self, gpu_idx, sub_batch, criterion):\n",
    "        \"\"\"\n",
    "        Forward pass on a specific GPU\n",
    "        \n",
    "        Returns:\n",
    "            loss value and gradients\n",
    "        \"\"\"\n",
    "        if sub_batch is None:\n",
    "            return 0.0, None\n",
    "        \n",
    "        device = self.devices[gpu_idx]\n",
    "        model = self.models[gpu_idx]\n",
    "        \n",
    "        # Move data to GPU\n",
    "        xd = sub_batch['drug'].to(device, non_blocking=True)\n",
    "        xp = sub_batch['protein'].to(device, non_blocking=True)\n",
    "        y = sub_batch['y'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(xd, xp)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        return loss, (xd, xp, y, pred)\n",
    "    \n",
    "    def train_epoch(self, train_loader, criterion, epoch_idx):\n",
    "        \"\"\"\n",
    "        Train one epoch with distributed batches\n",
    "        \"\"\"\n",
    "        # Set all models to training mode\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        batch_losses_per_gpu = [[] for _ in range(self.n_gpus)]\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch_idx}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Determine batch size based on data type\n",
    "            if isinstance(batch['y'], list):\n",
    "                batch_size = len(batch['y'])\n",
    "            else:\n",
    "                batch_size = batch['y'].shape[0]\n",
    "            \n",
    "            # Distribute batch across GPUs\n",
    "            sub_batches = self.distribute_batch(batch, batch_size)\n",
    "            \n",
    "            # Process each sub-batch on its GPU\n",
    "            gpu_losses = []\n",
    "            gpu_data = []\n",
    "            \n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                if sub_batches[gpu_idx] is not None:\n",
    "                    loss, data = self.forward_on_gpu(gpu_idx, sub_batches[gpu_idx], criterion)\n",
    "                    gpu_losses.append(loss)\n",
    "                    gpu_data.append((gpu_idx, loss, data))\n",
    "                    batch_losses_per_gpu[gpu_idx].append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "            \n",
    "            # Backward pass on each GPU\n",
    "            for gpu_idx, loss, _ in gpu_data:\n",
    "                if loss > 0:\n",
    "                    self.optimizers[gpu_idx].zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    self.optimizers[gpu_idx].step()\n",
    "            \n",
    "            # Record batch loss (average across GPUs)\n",
    "            if gpu_losses:\n",
    "                avg_batch_loss = sum([l.item() if hasattr(l, 'item') else l for l in gpu_losses]) / len(gpu_losses)\n",
    "                epoch_losses.append(avg_batch_loss)\n",
    "                pbar.set_postfix({'loss': f'{avg_batch_loss:.4f}'})\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # At end of epoch: aggregate losses and sync models\n",
    "        epoch_avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "        \n",
    "        # Average gradients across all GPUs and update master model\n",
    "        self.aggregate_and_update_master()\n",
    "        \n",
    "        # Sync all models from master\n",
    "        self.sync_models_from_master()\n",
    "        \n",
    "        return epoch_avg_loss, batch_losses_per_gpu\n",
    "    \n",
    "    def aggregate_and_update_master(self):\n",
    "        \"\"\"\n",
    "        Average parameters from all GPUs and update master model\n",
    "        This is more sophisticated than just copying - it averages the learned updates\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_state = master_model.state_dict()\n",
    "        \n",
    "        # Average parameters from all models\n",
    "        averaged_state = {}\n",
    "        for key in master_state.keys():\n",
    "            # Sum parameters from all GPUs\n",
    "            param_sum = None\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                param = self.models[gpu_idx].state_dict()[key].to(self.devices[self.master_gpu])\n",
    "                if param_sum is None:\n",
    "                    param_sum = param.clone()\n",
    "                else:\n",
    "                    param_sum += param\n",
    "            \n",
    "            # Average\n",
    "            averaged_state[key] = param_sum / self.n_gpus\n",
    "        \n",
    "        # Update master model with averaged parameters\n",
    "        master_model.load_state_dict(averaged_state)\n",
    "    \n",
    "    def validate(self, valid_loader, criterion):\n",
    "        \"\"\"\n",
    "        Validate using only the master model\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_device = self.devices[self.master_gpu]\n",
    "        \n",
    "        master_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(master_device)\n",
    "                xp = batch['protein'].to(master_device)\n",
    "                y = batch['y'].to(master_device)\n",
    "                \n",
    "                pred = master_model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(val_losses) if val_losses else 0.0\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the master model\"\"\"\n",
    "        return self.models[self.master_gpu]\n",
    "\n",
    "\n",
    "def train_fold_distributed_batch(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, batch_size=256, lr=0.0005, patience=20,\n",
    "    n_gpus=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a fold using distributed batch multi-GPU approach\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Distributed Batch Multi-GPU\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\")\n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    \n",
    "    # Detect GPUs\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    n_gpus = min(n_gpus or available_gpus, available_gpus)\n",
    "    print(f\"  Using {n_gpus} GPUs out of {available_gpus} available\")\n",
    "    \n",
    "    # Adjust batch size for multiple GPUs\n",
    "    # Each GPU will process batch_size/n_gpus samples\n",
    "    print(f\"  Total batch size: {batch_size}\")\n",
    "    print(f\"  Batch size per GPU: ~{batch_size // n_gpus}\")\n",
    "    \n",
    "    # Build datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    \n",
    "    # Initialize multi-GPU trainer\n",
    "    model_kwargs = {\n",
    "        'task_names': task_cols,\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "    \n",
    "    trainer = MultiGPUDistributedBatchTrainer(\n",
    "        model_class=MTL_DTAModel,\n",
    "        model_kwargs=model_kwargs,\n",
    "        n_gpus=n_gpus,\n",
    "        master_gpu=0\n",
    "    )\n",
    "    \n",
    "    # Create criterion on master GPU\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges).to(trainer.devices[trainer.master_gpu])\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting distributed batch training...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train epoch\n",
    "        train_loss, gpu_losses = trainer.train_epoch(train_loader, criterion, epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = trainer.validate(valid_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "        \n",
    "        # Print per-GPU statistics\n",
    "        for gpu_idx, losses in enumerate(gpu_losses):\n",
    "            if losses:\n",
    "                avg_gpu_loss = np.mean(losses)\n",
    "                print(f\"  GPU {gpu_idx}: avg loss = {avg_gpu_loss:.4f} ({len(losses)} batches)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = trainer.get_best_model().state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.get_best_model().load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    master_model = trainer.get_best_model()\n",
    "    master_device = trainer.devices[trainer.master_gpu]\n",
    "    master_model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(master_device)\n",
    "            xp = batch['protein'].to(master_device)\n",
    "            y = batch['y'].to(master_device)\n",
    "            \n",
    "            pred = master_model(xd, xp)\n",
    "            \n",
    "            # Collect predictions for each task\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_preds = pred[mask, i].cpu().numpy()\n",
    "                    task_trues = y[mask, i].cpu().numpy()\n",
    "                    task_predictions[task].extend(task_preds)\n",
    "                    task_targets[task].extend(task_trues)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your existing setup code...\n",
    "    \n",
    "    # Use the distributed batch training\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df_clean)):\n",
    "        df_train = df_clean.iloc[train_idx].reset_index(drop=True)\n",
    "        df_test = df_clean.iloc[test_idx].reset_index(drop=True)\n",
    "        \n",
    "        valid_size = int(0.1 * len(df_train))\n",
    "        df_valid = df_train.sample(n=valid_size, random_state=42)\n",
    "        df_train = df_train.drop(df_valid.index).reset_index(drop=True)\n",
    "        \n",
    "        # Use distributed batch training\n",
    "        fold_results, train_losses, val_losses = train_fold_distributed_batch(\n",
    "            fold_idx, N_FOLDS,\n",
    "            df_train, df_valid, df_test,\n",
    "            chunk_loader, task_cols, task_ranges,\n",
    "            N_EPOCHS, BATCH_SIZE, LEARNING_RATE, PATIENCE,\n",
    "            n_gpus=4  # Use 4 GPUs\n",
    "        )\n",
    "        \n",
    "        # Store results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab883d2-2fa0-46f0-8e68-661b031e44af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-GPU training with distributed batches and epoch-level synchronization\n",
    "Each GPU processes its own batch subset, losses are combined on GPU 0 at epoch end\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "class MultiGPUDistributedBatchTrainer:\n",
    "    \"\"\"\n",
    "    Custom multi-GPU trainer that:\n",
    "    1. Maintains separate model copies on each GPU\n",
    "    2. Distributes batches across GPUs\n",
    "    3. Aggregates losses on GPU 0 at epoch end\n",
    "    4. Syncs models after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_kwargs, n_gpus=None, master_gpu=0, criterion_class=None, criterion_kwargs=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: Class of the model (e.g., MTL_DTAModel)\n",
    "            model_kwargs: Dictionary of model initialization parameters\n",
    "            n_gpus: Number of GPUs to use (None = use all available)\n",
    "            master_gpu: Index of the master GPU (default 0)\n",
    "            criterion_class: Loss function class\n",
    "            criterion_kwargs: Loss function initialization parameters\n",
    "        \"\"\"\n",
    "        self.n_gpus = n_gpus or torch.cuda.device_count()\n",
    "        self.master_gpu = master_gpu\n",
    "        self.model_class = model_class\n",
    "        self.model_kwargs = model_kwargs\n",
    "        \n",
    "        # Create model on each GPU\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.devices = []\n",
    "        self.criterions = []\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            device = torch.device(f'cuda:{gpu_idx}')\n",
    "            self.devices.append(device)\n",
    "            \n",
    "            # Create model on this GPU\n",
    "            model = model_class(**model_kwargs).to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            # Create optimizer for this model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            self.optimizers.append(optimizer)\n",
    "            \n",
    "            # Create criterion on this GPU\n",
    "            if criterion_class and criterion_kwargs:\n",
    "                criterion = criterion_class(**criterion_kwargs).to(device)\n",
    "                self.criterions.append(criterion)\n",
    "        \n",
    "        print(f\"Initialized {self.n_gpus} models on GPUs: {list(range(self.n_gpus))}\")\n",
    "        print(f\"Master GPU: {self.master_gpu}\")\n",
    "    \n",
    "    def sync_models_from_master(self):\n",
    "        \"\"\"Copy master model weights to all other GPUs\"\"\"\n",
    "        master_state = self.models[self.master_gpu].state_dict()\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            if gpu_idx != self.master_gpu:\n",
    "                # Copy state dict to other GPU\n",
    "                self.models[gpu_idx].load_state_dict(master_state)\n",
    "    \n",
    "    def distribute_batch(self, batch, batch_size):\n",
    "        \"\"\"\n",
    "        Distribute a batch across multiple GPUs\n",
    "        Handles both regular tensors and torch_geometric Batch objects\n",
    "        \n",
    "        Returns:\n",
    "            List of sub-batches, one for each GPU\n",
    "        \"\"\"\n",
    "        import torch_geometric\n",
    "        \n",
    "        # Check if we're dealing with geometric data\n",
    "        if isinstance(batch['drug'], list):\n",
    "            # Handle list of Data objects from DataLoader\n",
    "            sub_batches = []\n",
    "            samples_per_gpu = len(batch['drug']) // self.n_gpus\n",
    "            remainder = len(batch['drug']) % self.n_gpus\n",
    "            \n",
    "            start_idx = 0\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                gpu_batch_size = samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if gpu_batch_size > 0:\n",
    "                    end_idx = start_idx + gpu_batch_size\n",
    "                    \n",
    "                    # Create Batch objects for geometric data\n",
    "                    drug_list = batch['drug'][start_idx:end_idx]\n",
    "                    protein_list = batch['protein'][start_idx:end_idx]\n",
    "                    \n",
    "                    drug_batch = torch_geometric.data.Batch.from_data_list(drug_list)\n",
    "                    protein_batch = torch_geometric.data.Batch.from_data_list(protein_list)\n",
    "                    \n",
    "                    # Handle y values\n",
    "                    y_tensor = torch.stack([batch['y'][i] for i in range(start_idx, end_idx)])\n",
    "                    \n",
    "                    sub_batch = {\n",
    "                        'drug': drug_batch,\n",
    "                        'protein': protein_batch,\n",
    "                        'y': y_tensor\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                    start_idx = end_idx\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "                    \n",
    "        elif hasattr(batch['drug'], 'batch'):\n",
    "            # Handle pre-batched geometric data\n",
    "            # Get batch assignments\n",
    "            drug_batch_tensor = batch['drug'].batch\n",
    "            protein_batch_tensor = batch['protein'].batch\n",
    "            \n",
    "            sub_batches = []\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                # Calculate which samples go to this GPU\n",
    "                samples_per_gpu = batch_size // self.n_gpus\n",
    "                remainder = batch_size % self.n_gpus\n",
    "                \n",
    "                start_sample = gpu_idx * samples_per_gpu + min(gpu_idx, remainder)\n",
    "                end_sample = start_sample + samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if start_sample < batch_size:\n",
    "                    # Extract nodes belonging to these samples\n",
    "                    drug_mask = (drug_batch_tensor >= start_sample) & (drug_batch_tensor < end_sample)\n",
    "                    protein_mask = (protein_batch_tensor >= start_sample) & (protein_batch_tensor < end_sample)\n",
    "                    \n",
    "                    # Create sub-batch (this is simplified - may need more sophisticated slicing)\n",
    "                    sub_batch = {\n",
    "                        'drug': batch['drug'],  # Keep full batch for now\n",
    "                        'protein': batch['protein'],  # Keep full batch for now\n",
    "                        'y': batch['y'][start_sample:end_sample]\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "        else:\n",
    "            # Handle regular tensor batches\n",
    "            samples_per_gpu = batch_size // self.n_gpus\n",
    "            remainder = batch_size % self.n_gpus\n",
    "            \n",
    "            sub_batches = []\n",
    "            start_idx = 0\n",
    "            \n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                gpu_batch_size = samples_per_gpu + (1 if gpu_idx < remainder else 0)\n",
    "                \n",
    "                if gpu_batch_size > 0:\n",
    "                    sub_batch = {\n",
    "                        'drug': batch['drug'][start_idx:start_idx + gpu_batch_size],\n",
    "                        'protein': batch['protein'][start_idx:start_idx + gpu_batch_size],\n",
    "                        'y': batch['y'][start_idx:start_idx + gpu_batch_size]\n",
    "                    }\n",
    "                    sub_batches.append(sub_batch)\n",
    "                    start_idx += gpu_batch_size\n",
    "                else:\n",
    "                    sub_batches.append(None)\n",
    "        \n",
    "        return sub_batches\n",
    "    \n",
    "    def forward_on_gpu(self, gpu_idx, sub_batch, criterion=None):\n",
    "        \"\"\"\n",
    "        Forward pass on a specific GPU\n",
    "        \n",
    "        Returns:\n",
    "            loss value and gradients\n",
    "        \"\"\"\n",
    "        if sub_batch is None:\n",
    "            return 0.0, None\n",
    "        \n",
    "        device = self.devices[gpu_idx]\n",
    "        model = self.models[gpu_idx]\n",
    "        \n",
    "        # Move data to GPU\n",
    "        xd = sub_batch['drug'].to(device, non_blocking=True)\n",
    "        xp = sub_batch['protein'].to(device, non_blocking=True)\n",
    "        y = sub_batch['y'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(xd, xp)\n",
    "        \n",
    "        # Use the criterion for this GPU\n",
    "        if self.criterions:\n",
    "            loss = self.criterions[gpu_idx](pred, y)\n",
    "        else:\n",
    "            # Fallback if no criterions provided\n",
    "            loss = torch.nn.functional.mse_loss(pred[~torch.isnan(y)], y[~torch.isnan(y)])\n",
    "        \n",
    "        return loss, (xd, xp, y, pred)\n",
    "    \n",
    "    def train_epoch(self, train_loader, criterion, epoch_idx):\n",
    "        \"\"\"\n",
    "        Train one epoch with distributed batches\n",
    "        \"\"\"\n",
    "        # Set all models to training mode\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        batch_losses_per_gpu = [[] for _ in range(self.n_gpus)]\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch_idx}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Determine batch size based on data type\n",
    "            if isinstance(batch['y'], list):\n",
    "                batch_size = len(batch['y'])\n",
    "            else:\n",
    "                batch_size = batch['y'].shape[0]\n",
    "            \n",
    "            # Distribute batch across GPUs\n",
    "            sub_batches = self.distribute_batch(batch, batch_size)\n",
    "            \n",
    "            # Process each sub-batch on its GPU\n",
    "            gpu_losses = []\n",
    "            gpu_data = []\n",
    "            \n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                if sub_batches[gpu_idx] is not None:\n",
    "                    loss, data = self.forward_on_gpu(gpu_idx, sub_batches[gpu_idx], criterion)\n",
    "                    gpu_losses.append(loss)\n",
    "                    gpu_data.append((gpu_idx, loss, data))\n",
    "                    batch_losses_per_gpu[gpu_idx].append(loss.item() if hasattr(loss, 'item') else loss)\n",
    "            \n",
    "            # Backward pass on each GPU\n",
    "            for gpu_idx, loss, _ in gpu_data:\n",
    "                if loss > 0:\n",
    "                    self.optimizers[gpu_idx].zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    self.optimizers[gpu_idx].step()\n",
    "            \n",
    "            # Record batch loss (average across GPUs)\n",
    "            if gpu_losses:\n",
    "                avg_batch_loss = sum([l.item() if hasattr(l, 'item') else l for l in gpu_losses]) / len(gpu_losses)\n",
    "                epoch_losses.append(avg_batch_loss)\n",
    "                pbar.set_postfix({'loss': f'{avg_batch_loss:.4f}'})\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # At end of epoch: aggregate losses and sync models\n",
    "        epoch_avg_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "        \n",
    "        # Average gradients across all GPUs and update master model\n",
    "        self.aggregate_and_update_master()\n",
    "        \n",
    "        # Sync all models from master\n",
    "        self.sync_models_from_master()\n",
    "        \n",
    "        return epoch_avg_loss, batch_losses_per_gpu\n",
    "    \n",
    "    def aggregate_and_update_master(self):\n",
    "        \"\"\"\n",
    "        Average parameters from all GPUs and update master model\n",
    "        This is more sophisticated than just copying - it averages the learned updates\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_state = master_model.state_dict()\n",
    "        \n",
    "        # Average parameters from all models\n",
    "        averaged_state = {}\n",
    "        for key in master_state.keys():\n",
    "            # Sum parameters from all GPUs\n",
    "            param_sum = None\n",
    "            for gpu_idx in range(self.n_gpus):\n",
    "                param = self.models[gpu_idx].state_dict()[key].to(self.devices[self.master_gpu])\n",
    "                if param_sum is None:\n",
    "                    param_sum = param.clone()\n",
    "                else:\n",
    "                    param_sum += param\n",
    "            \n",
    "            # Average\n",
    "            averaged_state[key] = param_sum / self.n_gpus\n",
    "        \n",
    "        # Update master model with averaged parameters\n",
    "        master_model.load_state_dict(averaged_state)\n",
    "    \n",
    "    def validate(self, valid_loader, criterion):\n",
    "        \"\"\"\n",
    "        Validate using only the master model\n",
    "        \"\"\"\n",
    "        master_model = self.models[self.master_gpu]\n",
    "        master_device = self.devices[self.master_gpu]\n",
    "        \n",
    "        master_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(master_device)\n",
    "                xp = batch['protein'].to(master_device)\n",
    "                y = batch['y'].to(master_device)\n",
    "                \n",
    "                pred = master_model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(val_losses) if val_losses else 0.0\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the master model\"\"\"\n",
    "        return self.models[self.master_gpu]\n",
    "\n",
    "\n",
    "def train_fold_distributed_batch(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, batch_size=256, lr=0.0005, patience=20,\n",
    "    n_gpus=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a fold using distributed batch multi-GPU approach\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Distributed Batch Multi-GPU\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\")\n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    \n",
    "    # Detect GPUs\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    n_gpus = min(n_gpus or available_gpus, available_gpus)\n",
    "    print(f\"  Using {n_gpus} GPUs out of {available_gpus} available\")\n",
    "    \n",
    "    # Adjust batch size for multiple GPUs\n",
    "    # Each GPU will process batch_size/n_gpus samples\n",
    "    print(f\"  Total batch size: {batch_size}\")\n",
    "    print(f\"  Batch size per GPU: ~{batch_size // n_gpus}\")\n",
    "    \n",
    "    # Build datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    \n",
    "    # Initialize multi-GPU trainer\n",
    "    model_kwargs = {\n",
    "        'task_names': task_cols,\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "    \n",
    "    trainer = MultiGPUDistributedBatchTrainer(\n",
    "        model_class=MTL_DTAModel,\n",
    "        model_kwargs=model_kwargs,\n",
    "        n_gpus=n_gpus,\n",
    "        master_gpu=0,\n",
    "        criterion_class=MaskedMSELoss,\n",
    "        criterion_kwargs={'task_ranges': task_ranges}\n",
    "    )\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting distributed batch training...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train epoch (no need to pass criterion anymore)\n",
    "        train_loss, gpu_losses = trainer.train_epoch(train_loader, None, epoch + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation (no need to pass criterion)\n",
    "        val_loss = trainer.validate(valid_loader, None)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "        \n",
    "        # Print per-GPU statistics\n",
    "        for gpu_idx, losses in enumerate(gpu_losses):\n",
    "            if losses:\n",
    "                avg_gpu_loss = np.mean(losses)\n",
    "                print(f\"  GPU {gpu_idx}: avg loss = {avg_gpu_loss:.4f} ({len(losses)} batches)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = trainer.get_best_model().state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.get_best_model().load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    master_model = trainer.get_best_model()\n",
    "    master_device = trainer.devices[trainer.master_gpu]\n",
    "    master_model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(master_device)\n",
    "            xp = batch['protein'].to(master_device)\n",
    "            y = batch['y'].to(master_device)\n",
    "            \n",
    "            pred = master_model(xd, xp)\n",
    "            \n",
    "            # Collect predictions for each task\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_preds = pred[mask, i].cpu().numpy()\n",
    "                    task_trues = y[mask, i].cpu().numpy()\n",
    "                    task_predictions[task].extend(task_preds)\n",
    "                    task_targets[task].extend(task_trues)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simplified multi-GPU training that actually works\n",
    "Each GPU processes independent batches, models sync at epoch end\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import math\n",
    "\n",
    "def collate_fn_mtl(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for MTL_DTA that properly batches the data\n",
    "    \"\"\"\n",
    "    # Batch the drug and protein graphs\n",
    "    drug_list = [item['drug'] for item in batch]\n",
    "    protein_list = [item['protein'] for item in batch]\n",
    "    \n",
    "    # Create batched graphs\n",
    "    drug_batch = Batch.from_data_list(drug_list)\n",
    "    protein_batch = Batch.from_data_list(protein_list)\n",
    "    \n",
    "    # Stack the target values\n",
    "    y_list = [item['y'] for item in batch]\n",
    "    y_batch = torch.stack(y_list)\n",
    "    \n",
    "    return {\n",
    "        'drug': drug_batch,\n",
    "        'protein': protein_batch,\n",
    "        'y': y_batch\n",
    "    }\n",
    "\n",
    "class SimpleMultiGPUTrainer:\n",
    "    \"\"\"\n",
    "    Simple approach: each GPU gets complete batches sequentially\n",
    "    Models sync at epoch end\n",
    "    \"\"\"\n",
    "    def __init__(self, model_class, model_kwargs, task_ranges, n_gpus=None):\n",
    "        self.n_gpus = n_gpus or torch.cuda.device_count()\n",
    "        self.task_ranges = task_ranges\n",
    "        \n",
    "        # Create models and optimizers for each GPU\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.devices = []\n",
    "        \n",
    "        for gpu_idx in range(self.n_gpus):\n",
    "            device = torch.device(f'cuda:{gpu_idx}')\n",
    "            self.devices.append(device)\n",
    "            \n",
    "            model = model_class(**model_kwargs).to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            self.optimizers.append(optimizer)\n",
    "        \n",
    "        print(f\"Created {self.n_gpus} models on GPUs\")\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train one epoch - batches go to GPUs in round-robin fashion\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        losses = []\n",
    "        gpu_batch_counts = [0] * self.n_gpus\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            # Assign batch to GPU in round-robin\n",
    "            gpu_idx = batch_idx % self.n_gpus\n",
    "            device = self.devices[gpu_idx]\n",
    "            model = self.models[gpu_idx]\n",
    "            optimizer = self.optimizers[gpu_idx]\n",
    "            \n",
    "            # Move batch to GPU\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Compute loss with mask for NaN values\n",
    "            mask = ~torch.isnan(y)\n",
    "            if mask.sum() > 0:\n",
    "                pred_masked = pred[mask]\n",
    "                y_masked = y[mask]\n",
    "                loss = torch.nn.functional.mse_loss(pred_masked, y_masked)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                gpu_batch_counts[gpu_idx] += 1\n",
    "        \n",
    "        # Sync models at end of epoch\n",
    "        self.sync_models()\n",
    "        \n",
    "        # Print GPU usage stats\n",
    "        for gpu_idx, count in enumerate(gpu_batch_counts):\n",
    "            print(f\"  GPU {gpu_idx}: processed {count} batches\")\n",
    "        \n",
    "        return np.mean(losses) if losses else 0.0\n",
    "    \n",
    "    def sync_models(self):\n",
    "        \"\"\"Average all model parameters\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get state dict from first model\n",
    "            avg_state = self.models[0].state_dict()\n",
    "            \n",
    "            # Average parameters from all models\n",
    "            for key in avg_state.keys():\n",
    "                params = []\n",
    "                for model in self.models:\n",
    "                    params.append(model.state_dict()[key].to('cuda:0'))\n",
    "                avg_state[key] = torch.stack(params).mean(dim=0)\n",
    "            \n",
    "            # Load averaged parameters to all models\n",
    "            for model in self.models:\n",
    "                model.load_state_dict(avg_state)\n",
    "    \n",
    "    def validate(self, valid_loader):\n",
    "        \"\"\"Validate using first model\"\"\"\n",
    "        model = self.models[0]\n",
    "        device = self.devices[0]\n",
    "        model.eval()\n",
    "        \n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                \n",
    "                mask = ~torch.isnan(y)\n",
    "                if mask.sum() > 0:\n",
    "                    loss = torch.nn.functional.mse_loss(pred[mask], y[mask])\n",
    "                    losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(losses) if losses else 0.0\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Return the first model\"\"\"\n",
    "        return self.models[0]\n",
    "\n",
    "\n",
    "def train_fold_simple_multi(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=30, batch_size=16, lr=0.0001, patience=20,\n",
    "    n_gpus=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple multi-GPU training that works\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Simple Multi-GPU\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\") \n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    print(f\"  Using {n_gpus} GPUs\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # Build datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_ultra_fast(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_ultra_fast(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_ultra_fast(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn_mtl\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Valid batches: {len(valid_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    model_kwargs = {\n",
    "        'task_names': task_cols,\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "    \n",
    "    trainer = SimpleMultiGPUTrainer(\n",
    "        model_class=MTL_DTAModel,\n",
    "        model_kwargs=model_kwargs,\n",
    "        task_ranges=task_ranges,\n",
    "        n_gpus=n_gpus\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        train_loss = trainer.train_epoch(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = trainer.validate(valid_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = trainer.get_model().state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Cleanup\n",
    "        if epoch % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.get_model().load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    model = trainer.get_model()\n",
    "    device = trainer.devices[0]\n",
    "    model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Collect predictions\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_predictions[task].extend(pred[mask, i].cpu().numpy())\n",
    "                    task_targets[task].extend(y[mask, i].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n",
    "fold_results, train_losses, val_losses = train_fold_simple_multi(\n",
    "    fold_idx, N_FOLDS,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    N_EPOCHS, BATCH_SIZE, LEARNING_RATE, PATIENCE,\n",
    "    n_gpus=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da2704-f03a-487d-b592-0d5f7cc0cbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the distributed batch training\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df_clean)):\n",
    "    df_train = df_clean.iloc[train_idx].reset_index(drop=True)\n",
    "    df_test = df_clean.iloc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    valid_size = int(0.1 * len(df_train))\n",
    "    df_valid = df_train.sample(n=valid_size, random_state=42)\n",
    "    df_train = df_train.drop(df_valid.index).reset_index(drop=True)\n",
    "    \n",
    "    # Use distributed batch training\n",
    "    fold_results, train_losses, val_losses = train_fold_distributed_batch(\n",
    "        fold_idx, N_FOLDS,\n",
    "        df_train, df_valid, df_test,\n",
    "        chunk_loader, task_cols, task_ranges,\n",
    "        N_EPOCHS, BATCH_SIZE, LEARNING_RATE, PATIENCE,\n",
    "        n_gpus=4  # Use 4 GPUs\n",
    "    )\n",
    "    \n",
    "    # Store results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac0735-e9bf-440a-a994-3ec8dccd02f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50362e1c-0c2b-4039-b649-4232da43f569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de9cf4-fc35-4e45-9d0a-c3671704ae62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7053e5-902f-43a7-9ad4-5d24ccfa2cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307b3a9-dc35-4d9d-b27b-1a8883aac5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3411d020-9d26-4474-8dbe-681ba852c65b",
   "metadata": {},
   "source": [
    "50k complex is:\n",
    "- PDB structure: 8 Go RAM (can be cleaned between each fold)\n",
    "- Build MTL dataset:\n",
    "    - 130 Train\n",
    "    - 20 Valid\n",
    "    - 30 Test\n",
    "\n",
    "- Total:\n",
    "166 for 50k\n",
    "Need to train in two sessions for 1M\n",
    "\n",
    "\n",
    "- so bottle neck is NOT the pdb structure, it is the building dataset, that is really to heavy, can't have 500k in memory\n",
    "- per epoch it would be way too long\n",
    "\n",
    "Max for me is 1360 RAM = 135 Go max for 50 k if 10\n",
    "For the 1 Mil., can do first 500k additional and then do 500 exp + additional so fine tuned on the good data in a way\n",
    "\n",
    "Need to correct multiple points ! \n",
    "\n",
    "1 layer per head\n",
    "one chain only ( esm per chain and go in the graph )\n",
    "size ? load and store faster ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54409a-8ab2-41aa-a7e7-ed2882ee40cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "rldif118",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python (rldif118)",
   "language": "python",
   "name": "rldif118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
