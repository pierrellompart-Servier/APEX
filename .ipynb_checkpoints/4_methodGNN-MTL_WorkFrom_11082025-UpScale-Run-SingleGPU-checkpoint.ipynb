{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc203eb-e8d2-47e0-9034-1edf69bc2983",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71a36e-c25c-463d-bd25-d4e12a1cc4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= MODIFICATIONS FOR MULTI-TASK LEARNING =============\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. MASKED MSE LOSS WITH TASK WEIGHTING\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked MSE loss that handles NaN values and applies task-specific weighting\n",
    "    based on the inverse of the range of each task.\n",
    "    \"\"\"\n",
    "    def __init__(self, task_ranges=None):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.task_ranges = task_ranges\n",
    "        if task_ranges is not None:\n",
    "            # Calculate task weights based on inverse range\n",
    "            weights = []\n",
    "            for range_val in task_ranges.values():\n",
    "                weights.append(1.0 / range_val if range_val > 0 else 1.0)\n",
    "            total_weight = sum(weights)\n",
    "            self.task_weights = torch.tensor([w / total_weight for w in weights])\n",
    "        else:\n",
    "            self.task_weights = None\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: [batch_size, n_tasks]\n",
    "        target: [batch_size, n_tasks]\n",
    "        \"\"\"\n",
    "        # Create mask for non-NaN values\n",
    "        mask = ~torch.isnan(target)\n",
    "        \n",
    "        # Calculate MSE only for non-NaN values\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # Apply mask\n",
    "        pred_masked = pred[mask]\n",
    "        target_masked = target[mask]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        se = (pred_masked - target_masked) ** 2\n",
    "        \n",
    "        # If we have task weights, apply them\n",
    "        if self.task_weights is not None:\n",
    "            # Expand mask to get task indices\n",
    "            task_indices = torch.where(mask)[1]\n",
    "            weights = self.task_weights.to(pred.device)[task_indices]\n",
    "            weighted_se = se * weights\n",
    "            loss = weighted_se.mean()\n",
    "        else:\n",
    "            loss = se.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "# 2. MODIFIED DTA MODEL FOR MULTI-TASK LEARNING\n",
    "class MTL_DTAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            task_names=['pKi', 'pEC50', 'pKd', 'pIC50'],  # List of tasks\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=[128, 256, 256],\n",
    "            prot_fc_dims=[1024, 128],\n",
    "            drug_node_in_dim=[66, 1], drug_node_h_dims=[128, 64],\n",
    "            drug_edge_in_dim=[16, 1], drug_edge_h_dims=[32, 1],            \n",
    "            drug_fc_dims=[1024, 128],\n",
    "            mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super(MTL_DTAModel, self).__init__()\n",
    "        \n",
    "        self.task_names = task_names\n",
    "        self.n_tasks = len(task_names)\n",
    "        \n",
    "        # Same encoders as before\n",
    "        self.drug_model = DrugGVPModel(\n",
    "            node_in_dim=drug_node_in_dim, node_h_dim=drug_node_h_dims,\n",
    "            edge_in_dim=drug_edge_in_dim, edge_h_dim=drug_edge_h_dims,\n",
    "        )\n",
    "        drug_emb_dim = drug_node_h_dims[0]\n",
    "        \n",
    "        self.prot_model = Prot3DGraphModel(\n",
    "            d_pretrained_emb=prot_emb_dim, d_gcn=prot_gcn_dims\n",
    "        )\n",
    "        prot_emb_dim = prot_gcn_dims[-1]\n",
    "        \n",
    "        self.drug_fc = self.get_fc_layers(\n",
    "            [drug_emb_dim] + drug_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "       \n",
    "        self.prot_fc = self.get_fc_layers(\n",
    "            [prot_emb_dim] + prot_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Shared representation layers\n",
    "        self.shared_fc = self.get_fc_layers(\n",
    "            [drug_fc_dims[-1] + prot_fc_dims[-1]] + mlp_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Task-specific heads (one for each task)\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            task: nn.Linear(mlp_dims[-1], 1) for task in task_names\n",
    "        })\n",
    "    \n",
    "    def get_fc_layers(self, hidden_sizes,\n",
    "            dropout=0, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True):\n",
    "        act_fn = torch.nn.LeakyReLU()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:])):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if not no_last_activation or i != len(hidden_sizes) - 2:\n",
    "                layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                if not no_last_dropout or i != len(hidden_sizes) - 2:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            if batchnorm and i != len(hidden_sizes) - 2:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xd, xp):\n",
    "        # Encode drug and protein\n",
    "        xd = self.drug_model(xd)\n",
    "        xp = self.prot_model(xp)\n",
    "        \n",
    "        # Process through FC layers\n",
    "        xd = self.drug_fc(xd)\n",
    "        xp = self.prot_fc(xp)\n",
    "        \n",
    "        # Concatenate and process through shared layers\n",
    "        x = torch.cat([xd, xp], dim=1)\n",
    "        shared_repr = self.shared_fc(x)\n",
    "        \n",
    "        # Generate predictions for each task\n",
    "        outputs = []\n",
    "        for task in self.task_names:\n",
    "            task_pred = self.task_heads[task](shared_repr)\n",
    "            outputs.append(task_pred)\n",
    "        \n",
    "        # Stack outputs: [batch_size, n_tasks]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# 3. MODIFIED DTA DATASET CLASS\n",
    "class MTL_DTA(data.Dataset):\n",
    "    def __init__(self, df=None, data_list=None, task_cols=None, onthefly=False,\n",
    "                prot_featurize_fn=None, drug_featurize_fn=None):\n",
    "        super(MTL_DTA, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.data_list = data_list\n",
    "        self.task_cols = task_cols or ['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "        self.onthefly = onthefly\n",
    "        self.prot_featurize_fn = prot_featurize_fn\n",
    "        self.drug_featurize_fn = drug_featurize_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.onthefly:\n",
    "            drug = self.drug_featurize_fn(\n",
    "                self.data_list[idx]['drug'],\n",
    "                name=self.data_list[idx]['drug_name']\n",
    "            )\n",
    "            prot = self.prot_featurize_fn(\n",
    "                self.data_list[idx]['protein'],\n",
    "                name=self.data_list[idx]['protein_name']\n",
    "            )\n",
    "        else:\n",
    "            drug = self.data_list[idx]['drug']\n",
    "            prot = self.data_list[idx]['protein']\n",
    "        \n",
    "        # Get multi-task targets\n",
    "        y_multi = []\n",
    "        for task in self.task_cols:\n",
    "            val = self.data_list[idx].get(task, np.nan)\n",
    "            y_multi.append(val if not pd.isna(val) else np.nan)\n",
    "        \n",
    "        y = torch.tensor(y_multi, dtype=torch.float32)\n",
    "        \n",
    "        item = {'drug': drug, 'protein': prot, 'y': y}\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Dataset builder for DTA class\n",
    "def build_dataset(df_fold, pdb_structures, exp_cols = \"pKi\", is_pred = False):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        if is_pred == True:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": 0\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": float(row[exp_cols]),\n",
    "            })\n",
    "    return DTA(df=df_fold, data_list=data_list)\n",
    "\n",
    "\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 5. MODIFIED TRAINING LOOP\n",
    "def train_mtl_model(model, train_loader, valid_loader, task_cols, task_ranges, \n",
    "                    n_epochs=100, lr=0.0005, device='cuda', patience=20):\n",
    "    \"\"\"\n",
    "    Training loop for multi-task learning model\n",
    "    \n",
    "    Args:\n",
    "        task_cols: List of task column names\n",
    "        task_ranges: Dict mapping task names to their value ranges\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges)\n",
    "    stopper = EarlyStopping(patience=patience, higher_better=False)\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)  # [batch_size, n_tasks]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)  # [batch_size, n_tasks]\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_n_batches = 0\n",
    "        task_metrics = {task: {'mse': 0, 'n': 0} for task in task_cols}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                val_n_batches += 1\n",
    "                \n",
    "                # Calculate per-task metrics\n",
    "                for i, task in enumerate(task_cols):\n",
    "                    mask = ~torch.isnan(y[:, i])\n",
    "                    if mask.sum() > 0:\n",
    "                        task_mse = F.mse_loss(pred[mask, i], y[mask, i])\n",
    "                        task_metrics[task]['mse'] += task_mse.item()\n",
    "                        task_metrics[task]['n'] += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / n_batches\n",
    "        avg_val_loss = val_loss / val_n_batches if val_n_batches > 0 else float('inf')\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Valid Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        for task in task_cols:\n",
    "            if task_metrics[task]['n'] > 0:\n",
    "                avg_task_mse = task_metrics[task]['mse'] / task_metrics[task]['n']\n",
    "                print(f\"  {task} MSE: {avg_task_mse:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if stopper.update(avg_val_loss):\n",
    "            best_model = model.state_dict()\n",
    "        if stopper.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. EXAMPLE USAGE\n",
    "def prepare_mtl_experiment(df, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Prepare data for multi-task learning\n",
    "    \"\"\"\n",
    "    # Calculate task ranges for weighting\n",
    "    task_ranges = {}\n",
    "    for task in task_cols:\n",
    "        if task in df.columns:\n",
    "            valid_values = df[task].dropna()\n",
    "            if len(valid_values) > 0:\n",
    "                task_ranges[task] = valid_values.max() - valid_values.min()\n",
    "            else:\n",
    "                task_ranges[task] = 1.0\n",
    "        else:\n",
    "            task_ranges[task] = 1.0\n",
    "    \n",
    "    print(\"Task ranges for weighting:\")\n",
    "    for task, range_val in task_ranges.items():\n",
    "        weight = 1.0 / range_val if range_val > 0 else 1.0\n",
    "        normalized_weight = weight / sum(1.0/r if r > 0 else 1.0 for r in task_ranges.values())\n",
    "        print(f\"  {task}: range={range_val:.2f}, weight={normalized_weight:.4f}\")\n",
    "    \n",
    "    return task_ranges\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def structureJSON(df, esm_model):\n",
    "    structure_dict = {}\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        pdb_path = row[\"standardized_protein_pdb\"]\n",
    "        try:\n",
    "\n",
    "            pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "\n",
    "            structure = parser.get_structure(pdb_id, pdb_path)\n",
    "            seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "            if seq is None:\n",
    "                available = [c.id for c in structure[0]]\n",
    "                print(f\"[SKIP] {pdb_id}: no usable chain found (available: {available})\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Stack in order: N, CA, C, O --> [L, 4, 3]\n",
    "            coords_stacked = []\n",
    "            for i in range(len(coords[\"N\"])):\n",
    "                coord_group = []\n",
    "                for atom in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "                    coord_group.append(coords[atom][i])\n",
    "                coords_stacked.append(coord_group)\n",
    "\n",
    "            if coords_stacked is None:\n",
    "                print(f\"[SKIP] {pdb_id}: no usable coords found (available: {pdb_path})\")\n",
    "                continue\n",
    "\n",
    "                \n",
    "            embedding = get_esm_embedding(seq, esm_model)\n",
    "            torch.save(embedding, f\"esm_embeddings/{pdb_id}.pt\")\n",
    "\n",
    "            if coords_stacked != None and embedding != None:\n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": f\"esm_embeddings/{pdb_id}.pt\"\n",
    "\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"../data/pockets_structure.json\", \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to pockets_structure.json\")\n",
    "\n",
    "    return(structure_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d52c7-479c-45ce-b88a-5bb69ed2a292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from Bio.PDB import PDBParser  # make sure Biopython is installed\n",
    "\n",
    "# Assumes you already have:\n",
    "# - extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "# - get_esm_embedding(seq, esm_model)\n",
    "\n",
    "ATOMS = (\"N\", \"CA\", \"C\", \"O\")\n",
    "EMBED_DIR = Path(\"esm_embeddings\")\n",
    "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _stack_backbone(coords):\n",
    "    # coords: dict with keys \"N\",\"CA\",\"C\",\"O\", each a list of [x,y,z]\n",
    "    L = len(coords[\"N\"])\n",
    "    return [[coords[a][i] for a in ATOMS] for i in range(L)]\n",
    "\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Worker: parse PDB, extract seq/coords/chain, return tuple or a skip marker.\n",
    "    Runs in a separate process; initializes its own parser.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "\n",
    "        seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "        if seq is None:\n",
    "            available = [c.id for c in structure[0]]\n",
    "            return (\"skip\", pdb_id, f\"no usable chain (available: {available})\")\n",
    "\n",
    "        if not coords or any(k not in coords for k in ATOMS) or len(coords[\"N\"]) == 0:\n",
    "            return (\"skip\", pdb_id, \"no usable coords\")\n",
    "\n",
    "        coords_stacked = _stack_backbone(coords)\n",
    "        if not coords_stacked:\n",
    "            return (\"skip\", pdb_id, \"empty coords after stacking\")\n",
    "\n",
    "        return (\"ok\", pdb_id, seq, coords_stacked, chain_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"error\", os.path.basename(pdb_path).split('.')[0], str(e))\n",
    "\n",
    "def structureJSON(df, esm_model, max_workers=None, embed_batch_size=8, out_json=\"../data/pockets_structure.json\"):\n",
    "    structure_dict = {}\n",
    "\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    results = []\n",
    "\n",
    "    # Phase 1: parallel PDB parsing + coordinate extraction\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"PDB -> seq/coords\"):\n",
    "            status_tuple = fut.result()\n",
    "            results.append(status_tuple)\n",
    "\n",
    "    # Log skips/errors (fast)\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            _, pdb_id, msg = r\n",
    "            print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "        elif tag == \"error\":\n",
    "            _, pdb_id, err = r\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "\n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "\n",
    "    # Phase 2: embeddings on a single device (GPU/CPU) to avoid per-process model copies\n",
    "    # Optionally batch if your get_esm_embedding supports lists; otherwise do per-sequence.\n",
    "    # Here we do per-sequence by default; simple and safe.\n",
    "    for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, desc=\"ESM embeddings\"):\n",
    "        try:\n",
    "            embedding = get_esm_embedding(seq, esm_model)  # ensure this returns a tensor\n",
    "            torch.save(embedding, EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "\n",
    "            structure_dict[pdb_id] = {\n",
    "                \"name\": pdb_id,\n",
    "                \"UniProt_id\": \"UNKNOWN\",\n",
    "                \"PDB_id\": pdb_id,\n",
    "                \"chain\": chain_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords\": coords_stacked,         # [[N,CA,C,O], ...], each as [x,y,z]\n",
    "                \"embed\": str(EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to {os.path.basename(out_json)}\")\n",
    "    return structure_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "def structureJSON_chunked(df, esm_model, max_workers=None, embed_batch_size=8, \n",
    "                          chunk_size=100000, out_dir=\"../data/structure_chunks/\"):\n",
    "    \"\"\"\n",
    "    Process structures in chunks to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model: ESM model for embeddings\n",
    "        max_workers: Number of parallel workers\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(EMBED_DIR, exist_ok=True)\n",
    "    \n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"Processing {total_pdbs} PDBs in {num_chunks} chunks of max {chunk_size} each\")\n",
    "    \n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"\\n=== Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_paths)} PDBs) ===\")\n",
    "        \n",
    "        structure_dict = {}\n",
    "        results = []\n",
    "        \n",
    "        # Phase 1: Parallel PDB parsing for this chunk\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_process_pdb_path, p): p for p in chunk_paths}\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                          desc=f\"Chunk {chunk_idx + 1} - PDB parsing\"):\n",
    "                status_tuple = fut.result()\n",
    "                results.append(status_tuple)\n",
    "        \n",
    "        # Log errors for this chunk\n",
    "        for r in results:\n",
    "            tag = r[0]\n",
    "            if tag == \"skip\":\n",
    "                _, pdb_id, msg = r\n",
    "                print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "            elif tag == \"error\":\n",
    "                _, pdb_id, err = r\n",
    "                print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "        \n",
    "        # Keep only successful items\n",
    "        ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                    for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                    for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "        \n",
    "        # Phase 2: ESM embeddings for this chunk\n",
    "        for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, \n",
    "                                                          desc=f\"Chunk {chunk_idx + 1} - ESM embeddings\"):\n",
    "            try:\n",
    "                embedding = get_esm_embedding(seq, esm_model)\n",
    "                embed_path = EMBED_DIR / f\"{pdb_id}.pt\"\n",
    "                torch.save(embedding, embed_path)\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": str(embed_path)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Save this chunk\n",
    "        chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "        chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "        with open(chunk_path, \"w\") as f:\n",
    "            json.dump(structure_dict, f, indent=2)\n",
    "        \n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": chunk_idx,\n",
    "            \"filename\": chunk_filename,\n",
    "            \"path\": chunk_path,\n",
    "            \"num_structures\": len(structure_dict),\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        \n",
    "        print(f\"✅ Chunk {chunk_idx + 1} saved: {len(structure_dict)} structures to {chunk_filename}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ All chunks processed. Metadata saved to {metadata_path}\")\n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            # Quick scan to build index (could be saved in metadata for efficiency)\n",
    "            with open(chunk_path, \"r\") as f:\n",
    "                chunk_data = json.load(f)\n",
    "                for pdb_id in chunk_data.keys():\n",
    "                    self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b9fa7-c897-4d2f-aac7-7b6d9a91019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "\n",
    "def process_single_chunk(args):\n",
    "    \"\"\"\n",
    "    Process a single chunk of PDB files independently.\n",
    "    This function is designed to be run in parallel.\n",
    "    \n",
    "    Args:\n",
    "        args: tuple of (chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name)\n",
    "    \n",
    "    Returns:\n",
    "        dict with chunk processing results\n",
    "    \"\"\"\n",
    "    chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name = args\n",
    "    \n",
    "    # Import inside function for multiprocessing\n",
    "    from transformers import EsmModel, EsmTokenizer\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    print(f\"\\n[Chunk {chunk_idx}] Starting processing of {len(pdb_paths)} PDBs\")\n",
    "    \n",
    "    # Load ESM model for this process\n",
    "    print(f\"[Chunk {chunk_idx}] Loading ESM model...\")\n",
    "    tokenizer = EsmTokenizer.from_pretrained(esm_model_name)\n",
    "    esm_model = EsmModel.from_pretrained(esm_model_name)\n",
    "    esm_model.eval()\n",
    "    \n",
    "    # Move to GPU if available (each process gets its own GPU memory)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        # For multi-GPU, assign different chunks to different GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        gpu_id = chunk_idx % num_gpus\n",
    "        device = torch.device(f'cuda:{gpu_id}')\n",
    "        esm_model = esm_model.to(device)\n",
    "        print(f\"[Chunk {chunk_idx}] Using GPU {gpu_id}\")\n",
    "    else:\n",
    "        print(f\"[Chunk {chunk_idx}] Using CPU\")\n",
    "    \n",
    "    structure_dict = {}\n",
    "    results = []\n",
    "    \n",
    "    # Phase 1: Parallel PDB parsing within this chunk\n",
    "    max_workers = min(8, cpu_count() // 4)  # Limit workers per chunk\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                      desc=f\"Chunk {chunk_idx} - PDB parsing\", position=chunk_idx):\n",
    "            try:\n",
    "                status_tuple = fut.result(timeout=30)  # Add timeout\n",
    "                results.append(status_tuple)\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] Error processing PDB: {e}\")\n",
    "    \n",
    "    # Log errors\n",
    "    error_count = 0\n",
    "    skip_count = 0\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            skip_count += 1\n",
    "        elif tag == \"error\":\n",
    "            error_count += 1\n",
    "    \n",
    "    if error_count > 0 or skip_count > 0:\n",
    "        print(f\"[Chunk {chunk_idx}] Skipped: {skip_count}, Errors: {error_count}\")\n",
    "    \n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "    \n",
    "    # Phase 2: ESM embeddings (batch processing for efficiency)\n",
    "    print(f\"[Chunk {chunk_idx}] Computing ESM embeddings for {len(ok_items)} proteins...\")\n",
    "    \n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in batches to optimize GPU usage\n",
    "    batch_size = 8\n",
    "    for i in tqdm(range(0, len(ok_items), batch_size), \n",
    "                  desc=f\"Chunk {chunk_idx} - ESM embeddings\", position=chunk_idx):\n",
    "        batch = ok_items[i:i+batch_size]\n",
    "        \n",
    "        for pdb_id, seq, coords_stacked, chain_id in batch:\n",
    "            try:\n",
    "                # Compute embedding\n",
    "                with torch.no_grad():\n",
    "                    embedding = get_esm_embedding(seq, esm_model, tokenizer, device)\n",
    "                \n",
    "                # Save embedding\n",
    "                embed_path = os.path.join(embed_dir, f\"{pdb_id}.pt\")\n",
    "                torch.save(embedding.cpu(), embed_path)  # Save on CPU to free GPU memory\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": embed_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Periodic garbage collection\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save chunk\n",
    "    chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "    chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "    with open(chunk_path, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"[Chunk {chunk_idx}] ✅ Completed: {len(structure_dict)} structures saved\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del esm_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"chunk_idx\": chunk_idx,\n",
    "        \"filename\": chunk_filename,\n",
    "        \"path\": chunk_path,\n",
    "        \"num_structures\": len(structure_dict),\n",
    "        \"num_errors\": error_count,\n",
    "        \"num_skipped\": skip_count\n",
    "    }\n",
    "\n",
    "\n",
    "def get_esm_embedding(seq, esm_model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Get ESM embedding for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        seq: Protein sequence\n",
    "        esm_model: ESM model\n",
    "        tokenizer: ESM tokenizer\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = esm_model(**inputs)\n",
    "        # Use mean pooling over sequence length\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def structureJSON_chunked(df, esm_model_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "                         chunk_size=100000, max_chunks_parallel=4,\n",
    "                         out_dir=\"../data/structure_chunks/\",\n",
    "                         embed_dir=\"../data/embeddings/\"):\n",
    "    \"\"\"\n",
    "    Process structures in parallel chunks to avoid memory issues and maximize speed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model_name: Name of ESM model to use\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        max_chunks_parallel: Maximum number of chunks to process in parallel\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "        embed_dir: Directory to save embeddings\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique PDB paths (avoid duplicates)\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].unique().tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Processing {total_pdbs} unique PDBs in {num_chunks} chunks\")\n",
    "    print(f\"Chunk size: {chunk_size}, Parallel chunks: {max_chunks_parallel}\")\n",
    "    print(f\"=\" * 80)\n",
    "    \n",
    "    # Prepare chunk arguments\n",
    "    chunk_args = []\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        chunk_args.append((\n",
    "            chunk_idx,\n",
    "            chunk_paths,\n",
    "            out_dir,\n",
    "            embed_dir,\n",
    "            esm_model_name\n",
    "        ))\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    chunk_results = []\n",
    "    \n",
    "    # Determine optimal number of parallel processes\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    if num_gpus > 0:\n",
    "        # If we have GPUs, process one chunk per GPU\n",
    "        parallel_chunks = min(max_chunks_parallel, num_gpus, num_chunks)\n",
    "        print(f\"Using {num_gpus} GPUs, processing {parallel_chunks} chunks in parallel\")\n",
    "    else:\n",
    "        # CPU only - limit parallelism to avoid memory issues\n",
    "        parallel_chunks = min(max_chunks_parallel, cpu_count() // 4, num_chunks)\n",
    "        print(f\"Using CPU only, processing {parallel_chunks} chunks in parallel\")\n",
    "    \n",
    "    # Process in batches of parallel chunks\n",
    "    for batch_start in range(0, num_chunks, parallel_chunks):\n",
    "        batch_end = min(batch_start + parallel_chunks, num_chunks)\n",
    "        batch_args = chunk_args[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\nProcessing chunk batch {batch_start+1}-{batch_end} of {num_chunks}\")\n",
    "        \n",
    "        if len(batch_args) == 1:\n",
    "            # Single chunk - process directly\n",
    "            result = process_single_chunk(batch_args[0])\n",
    "            chunk_results.append(result)\n",
    "        else:\n",
    "            # Multiple chunks - use multiprocessing\n",
    "            with Pool(processes=len(batch_args)) as pool:\n",
    "                results = pool.map(process_single_chunk, batch_args)\n",
    "                chunk_results.extend(results)\n",
    "        \n",
    "        # Garbage collection between batches\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create metadata\n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"total_structures\": sum(r[\"num_structures\"] for r in chunk_results),\n",
    "        \"total_errors\": sum(r[\"num_errors\"] for r in chunk_results),\n",
    "        \"total_skipped\": sum(r[\"num_skipped\"] for r in chunk_results),\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Add chunk info with proper indices\n",
    "    start_idx = 0\n",
    "    for result in sorted(chunk_results, key=lambda x: x[\"chunk_idx\"]):\n",
    "        end_idx = start_idx + result[\"num_structures\"]\n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": result[\"chunk_idx\"],\n",
    "            \"filename\": result[\"filename\"],\n",
    "            \"path\": result[\"path\"],\n",
    "            \"num_structures\": result[\"num_structures\"],\n",
    "            \"num_errors\": result[\"num_errors\"],\n",
    "            \"num_skipped\": result[\"num_skipped\"],\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✅ Processing complete!\")\n",
    "    print(f\"  - Total structures: {chunk_metadata['total_structures']}\")\n",
    "    print(f\"  - Total errors: {chunk_metadata['total_errors']}\")\n",
    "    print(f\"  - Total skipped: {chunk_metadata['total_skipped']}\")\n",
    "    print(f\"  - Metadata saved: {metadata_path}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "# ============= Helper function for PDB processing =============\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Process a single PDB file to extract sequence and coordinates.\n",
    "    This function runs in a separate process.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (status, pdb_id, data...) where status is \"ok\", \"skip\", or \"error\"\n",
    "    \"\"\"\n",
    "    from Bio.PDB import PDBParser, is_aa\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    pdb_id = os.path.splitext(os.path.basename(pdb_path))[0]\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "        \n",
    "        # Get first model\n",
    "        model = structure[0]\n",
    "        \n",
    "        # Process each chain\n",
    "        for chain in model:\n",
    "            residues = [r for r in chain if is_aa(r)]\n",
    "            if len(residues) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract sequence\n",
    "            seq = ''.join([seq1(r.resname) for r in residues])\n",
    "            \n",
    "            # Extract coordinates [N, CA, C, O] for each residue\n",
    "            coords = []\n",
    "            for residue in residues:\n",
    "                try:\n",
    "                    n_coord = residue['N'].coord.tolist()\n",
    "                    ca_coord = residue['CA'].coord.tolist()\n",
    "                    c_coord = residue['C'].coord.tolist()\n",
    "                    o_coord = residue['O'].coord.tolist()\n",
    "                    coords.append([n_coord, ca_coord, c_coord, o_coord])\n",
    "                except:\n",
    "                    # Missing atoms - use zeros\n",
    "                    coords.append([[0,0,0], [0,0,0], [0,0,0], [0,0,0]])\n",
    "            \n",
    "            return (\"ok\", pdb_id, seq, coords, chain.id)\n",
    "        \n",
    "        return (\"skip\", pdb_id, \"No valid chains found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (\"error\", pdb_id, str(e))\n",
    "\n",
    "\n",
    "# ============= Optimized Chunk Loader (same as before) =============\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded metadata: {self.metadata['total_structures']} structures in {self.metadata['num_chunks']} chunks\")\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, \"r\") as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                    for pdb_id in chunk_data.keys():\n",
    "                        self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "            else:\n",
    "                print(f\"Warning: Chunk file not found: {chunk_path}\")\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "            gc.collect()  # Force garbage collection\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ce606-ee95-4850-bb99-c3f6095e9858",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6b3cd-b54d-42d2-85f0-035da7ebd23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parser = PDBParser(QUIET=True)\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Defne tasks to train on\n",
    "task_cols = ['pKi', 'pEC50', 'pKd', 'pKd (Wang, FEP)', 'pIC50', 'potency']\n",
    "\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\", engine=\"fastparquet\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "col_nan = [\"standardized_protein_pdb\", \"standardized_ligand_sdf\"] + task_cols\n",
    "# df = df[df['is_experimental'] == True]\n",
    "df = df.dropna(how = \"all\", subset=col_nan)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[df[\"standardized_protein_pdb\"].isna()==False]\n",
    "df = df[df[\"standardized_ligand_sdf\"].isna()==False]\n",
    "df = df[:50000]\n",
    "\n",
    "# Calculate task ranges from your dataframe\n",
    "task_ranges = prepare_mtl_experiment(df, task_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bff598-24c9-4d26-924c-5dc700614db9",
   "metadata": {},
   "source": [
    "# Load ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309da2a6-a78f-4aa2-8413-63bee8a064e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load ESM-2 model\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "esm_model = EsmModel.from_pretrained(model_name)\n",
    "esm_model.eval().cuda() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e8321-de80-48c4-9172-9d5463eb7e4b",
   "metadata": {},
   "source": [
    "# Generate structure dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd8943-eb35-41f4-87ad-f0f551acc609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Load and use the chunks later\n",
    "from parallel_structure_processing_optimized import StructureChunkLoader\n",
    "\n",
    "# Create a chunk loader (caches 2 chunks in memory at a time)\n",
    "chunk_loader = StructureChunkLoader(\n",
    "    chunk_dir=\"../data/structure_chunks/\",\n",
    "    cache_size=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92cff4-f3d6-4577-8456-e7a6e5c622b8",
   "metadata": {},
   "source": [
    "# Check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69276b-e6fe-4d5e-8e9a-50e4fce72edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/structure_chunks/structures_chunk_0000.json\", \"r\") as f:\n",
    "    pdb_structures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4454dd4-cd2b-46e2-baa1-f7d080c363e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here need to load all the pdb_structures in one .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1237a8d-3128-407d-8a13-4b4e80e2d4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a lookup set\n",
    "pdb_keys = set(pdb_structures.keys())\n",
    "\n",
    "# Add a canonical protein id column from the protein PDB path\n",
    "df[\"protein_id\"] = df[\"standardized_protein_pdb\"].apply(\n",
    "    lambda p: os.path.splitext(os.path.basename(p))[0]\n",
    ")\n",
    "\n",
    "# Keep only rows with a matching protein structure\n",
    "df_clean = df[df[\"protein_id\"].isin(pdb_keys)].reset_index(drop=True)\n",
    "\n",
    "print(\"Available over total:\", len(df_clean), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73186e05-d5dd-4013-8980-620058cfa164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df), len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74eedf-5dd1-40c5-ad1b-1c81e3d68b8f",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dee7d6-4f79-4695-b8b4-00d5996a57fe",
   "metadata": {},
   "source": [
    "# Check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b9148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a lookup set\n",
    "pdb_keys = set(pdb_structures.keys())\n",
    "\n",
    "# Add a canonical protein id column from the protein PDB path\n",
    "df[\"protein_id\"] = df[\"standardized_protein_pdb\"].apply(\n",
    "    lambda p: os.path.splitext(os.path.basename(p))[0]\n",
    ")\n",
    "\n",
    "# Keep only rows with a matching protein structure\n",
    "df_clean = df[df[\"protein_id\"].isin(pdb_keys)].reset_index(drop=True)\n",
    "\n",
    "print(\"Available over total:\", len(df_clean), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ce038-30c9-438f-a2a0-a1805138e8fd",
   "metadata": {},
   "source": [
    "# Build data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab91c01-294e-44f1-a5d3-57a4b685265b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2a448-627e-4913-a056-406618dba843",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1b652-23da-4d58-b8ab-ef782faf8dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fixed training for Graph Neural Networks\n",
    "DataParallel doesn't work well with graph batches, so we'll use single GPU\n",
    "but with optimized batching\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clean up\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def train_fold_single_gpu(\n",
    "    fold_idx, n_folds,\n",
    "    df_train, df_valid, df_test,\n",
    "    chunk_loader, task_cols, task_ranges,\n",
    "    n_epochs=100, batch_size=256, lr=0.0005, patience=20,\n",
    "    device_id=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Training function using single GPU (Graph NNs don't work well with DataParallel)\n",
    "    \n",
    "    Parameters:\n",
    "    - device_id: which GPU to use (0-7 for your 8 GPUs)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds} - Using GPU {device_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Train: {len(df_train)} samples\")\n",
    "    print(f\"  Valid: {len(df_valid)} samples\")\n",
    "    print(f\"  Test:  {len(df_test)} samples\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nBuilding datasets...\")\n",
    "    train_dataset = build_mtl_dataset_optimized(df_train, chunk_loader, task_cols)\n",
    "    valid_dataset = build_mtl_dataset_optimized(df_valid, chunk_loader, task_cols)\n",
    "    test_dataset = build_mtl_dataset_optimized(df_test, chunk_loader, task_cols)\n",
    "    \n",
    "    # Create data loaders - num_workers=0 to avoid file issues\n",
    "    train_loader = torch_geometric.loader.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    valid_loader = torch_geometric.loader.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = torch_geometric.loader.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DataLoaders created\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    print(f\"✓ Using device: {device}\")\n",
    "    \n",
    "    # Create model - NO DataParallel for graph models\n",
    "    print(\"Initializing model...\")\n",
    "    model = MTL_DTAModel(\n",
    "        task_names=task_cols,\n",
    "        prot_emb_dim=1280,\n",
    "        prot_gcn_dims=[128, 256, 256],\n",
    "        prot_fc_dims=[1024, 128],\n",
    "        drug_node_in_dim=[66, 1],\n",
    "        drug_node_h_dims=[128, 64],\n",
    "        drug_fc_dims=[1024, 128],\n",
    "        mlp_dims=[1024, 512],\n",
    "        mlp_dropout=0.25\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"✓ Model loaded on GPU {device_id}\")\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges).to(device)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    pbar = tqdm(range(n_epochs), desc=f\"Training\", ncols=100)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # ========== TRAINING PHASE ==========\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), desc = \"Batch iter.\"):\n",
    "            # Move batch to GPU\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n_train_batches += 1\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_train_loss = train_loss / n_train_batches if n_train_batches > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # ========== VALIDATION PHASE ==========\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        n_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                n_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / n_val_batches if n_val_batches > 0 else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'train': f\"{avg_train_loss:.4f}\",\n",
    "            'val': f\"{avg_val_loss:.4f}\",\n",
    "            'best': f\"{best_val_loss:.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # ========== EVALUATION PHASE ==========\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    model.eval()\n",
    "    \n",
    "    task_predictions = {task: [] for task in task_cols}\n",
    "    task_targets = {task: [] for task in task_cols}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            pred = model(xd, xp)\n",
    "            \n",
    "            # Collect predictions for each task\n",
    "            for i, task in enumerate(task_cols):\n",
    "                mask = ~torch.isnan(y[:, i])\n",
    "                if mask.sum() > 0:\n",
    "                    task_preds = pred[mask, i].cpu().numpy()\n",
    "                    task_trues = y[mask, i].cpu().numpy()\n",
    "                    task_predictions[task].extend(task_preds)\n",
    "                    task_targets[task].extend(task_trues)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    fold_results = {}\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for task in task_cols:\n",
    "        if len(task_predictions[task]) > 0:\n",
    "            preds = np.array(task_predictions[task])\n",
    "            targets = np.array(task_targets[task])\n",
    "            \n",
    "            r2 = r2_score(targets, preds)\n",
    "            rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "            \n",
    "            fold_results[task] = {\n",
    "                'predictions': preds,\n",
    "                'targets': targets,\n",
    "                'r2': r2,\n",
    "                'rmse': rmse\n",
    "            }\n",
    "            \n",
    "            print(f\"{task:20s} | RMSE: {rmse:6.3f} | R²: {r2:6.3f} | n={len(preds):5d}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del optimizer\n",
    "    del train_loader\n",
    "    del valid_loader\n",
    "    del test_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return fold_results, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a622fa-8bdd-493e-8f3b-b42b559e99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============= MAIN TRAINING SCRIPT =============\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GRAPH NEURAL NETWORK TRAINING - OPTIMIZED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 64     # Can use larger batch on single GPU\n",
    "N_EPOCHS = 100       \n",
    "LEARNING_RATE = 0.0001\n",
    "PATIENCE = 30\n",
    "N_FOLDS = 5\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {N_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Patience: {PATIENCE}\")\n",
    "print(f\"  Folds: {N_FOLDS}\")\n",
    "\n",
    "# Initialize results\n",
    "cv_results = {\n",
    "    task: {\n",
    "        'r2_list': [],\n",
    "        'rmse_list': [],\n",
    "        'all_predictions': [],\n",
    "        'all_targets': []\n",
    "    } for task in task_cols\n",
    "}\n",
    "\n",
    "# K-Fold cross-validation\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nStarting {N_FOLDS}-fold cross-validation...\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220bd289-96b2-4c60-9e92-6bbe348777a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# We'll rotate through GPUs for different folds\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df_clean)):\n",
    "    # Select GPU for this fold (rotate through available GPUs)\n",
    "    gpu_id = fold_idx % n_gpus\n",
    "    \n",
    "    # Split data\n",
    "    df_train = df_clean.iloc[train_idx].reset_index(drop=True)\n",
    "    df_test = df_clean.iloc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Create validation set (10% of training)\n",
    "    valid_size = int(0.1 * len(df_train))\n",
    "    df_valid = df_train.sample(n=valid_size, random_state=42)\n",
    "    df_train = df_train.drop(df_valid.index).reset_index(drop=True)\n",
    "    \n",
    "    # Train fold on selected GPU\n",
    "    fold_results, train_losses, val_losses = train_fold_single_gpu(\n",
    "        fold_idx, N_FOLDS,\n",
    "        df_train, df_valid, df_test,\n",
    "        chunk_loader, task_cols, task_ranges,\n",
    "        N_EPOCHS, BATCH_SIZE, LEARNING_RATE, PATIENCE,\n",
    "        device_id=gpu_id\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    for task in task_cols:\n",
    "        if task in fold_results:\n",
    "            cv_results[task]['r2_list'].append(fold_results[task]['r2'])\n",
    "            cv_results[task]['rmse_list'].append(fold_results[task]['rmse'])\n",
    "            cv_results[task]['all_predictions'].extend(fold_results[task]['predictions'])\n",
    "            cv_results[task]['all_targets'].extend(fold_results[task]['targets'])\n",
    "    \n",
    "    # Clean up after each fold\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42ddea-464b-46fc-9cdc-9e1c3c5c3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========== FINAL SUMMARY ==========\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CROSS-VALIDATION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "summary_results = []\n",
    "for task in task_cols:\n",
    "    if len(cv_results[task]['r2_list']) > 0:\n",
    "        avg_r2 = np.mean(cv_results[task]['r2_list'])\n",
    "        std_r2 = np.std(cv_results[task]['r2_list'])\n",
    "        avg_rmse = np.mean(cv_results[task]['rmse_list'])\n",
    "        std_rmse = np.std(cv_results[task]['rmse_list'])\n",
    "        n_samples = len(cv_results[task]['all_targets'])\n",
    "        \n",
    "        summary_results.append({\n",
    "            'Task': task,\n",
    "            'R² (mean±std)': f\"{avg_r2:.3f}±{std_r2:.3f}\",\n",
    "            'RMSE (mean±std)': f\"{avg_rmse:.3f}±{std_rmse:.3f}\",\n",
    "            'N samples': n_samples\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{task}:\")\n",
    "        print(f\"  R²:    {avg_r2:.3f} ± {std_r2:.3f}\")\n",
    "        print(f\"  RMSE:  {avg_rmse:.3f} ± {std_rmse:.3f}\")\n",
    "        print(f\"  Total samples: {n_samples}\")\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========== VISUALIZATION ==========\n",
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "# Individual task plots\n",
    "n_tasks_with_data = sum(1 for task in task_cols if len(cv_results[task]['all_targets']) > 0)\n",
    "n_cols = min(3, n_tasks_with_data)\n",
    "n_rows = (n_tasks_with_data + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "if n_tasks_with_data == 1:\n",
    "    axes = [axes]\n",
    "elif n_rows == 1:\n",
    "    axes = axes\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for task in task_cols:\n",
    "    if len(cv_results[task]['all_targets']) > 0:\n",
    "        ax = axes[plot_idx] if n_tasks_with_data > 1 else axes\n",
    "        \n",
    "        targets = np.array(cv_results[task]['all_targets'])\n",
    "        preds = np.array(cv_results[task]['all_predictions'])\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(targets, preds, alpha=0.4, s=10, color='blue')\n",
    "        \n",
    "        # Diagonal line\n",
    "        min_val = min(targets.min(), preds.min())\n",
    "        max_val = max(targets.max(), preds.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=1, alpha=0.7)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        overall_r2 = r2_score(targets, preds)\n",
    "        overall_rmse = math.sqrt(mean_squared_error(targets, preds))\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel(f'Experimental {task}')\n",
    "        ax.set_ylabel(f'Predicted {task}')\n",
    "        ax.set_title(f'{task}\\nR²={overall_r2:.3f}, RMSE={overall_rmse:.3f}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "if n_tasks_with_data > 1:\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'{N_FOLDS}-Fold Cross-Validation Results', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"✓ Results saved in 'cv_results' variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54755f0-81bb-4668-adfb-fd933d7633a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336553d4-d207-4e86-9331-aad16fac2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d6656-a449-4869-b2fa-5be5a35db232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307b3a9-dc35-4d9d-b27b-1a8883aac5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3411d020-9d26-4474-8dbe-681ba852c65b",
   "metadata": {},
   "source": [
    "50k complex is:\n",
    "- PDB structure: 8 Go RAM (can be cleaned between each fold)\n",
    "- Build MTL dataset:\n",
    "    - 130 Train\n",
    "    - 20 Valid\n",
    "    - 30 Test\n",
    "\n",
    "- Total:\n",
    "166 for 50k\n",
    "Need to train in two sessions for 1M\n",
    "\n",
    "\n",
    "- so bottle neck is NOT the pdb structure, it is the building dataset, that is really to heavy, can't have 500k in memory\n",
    "- per epoch it would be way too long\n",
    "\n",
    "Max for me is 1360 RAM = 135 Go max for 50 k if 10\n",
    "For the 1 Mil., can do first 500k additional and then do 500 exp + additional so fine tuned on the good data in a way\n",
    "\n",
    "Need to correct multiple points ! \n",
    "\n",
    "1 layer per head\n",
    "one chain only ( esm per chain and go in the graph )\n",
    "size ? load and store faster ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54409a-8ab2-41aa-a7e7-ed2882ee40cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "rldif118",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python (rldif118)",
   "language": "python",
   "name": "rldif118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
