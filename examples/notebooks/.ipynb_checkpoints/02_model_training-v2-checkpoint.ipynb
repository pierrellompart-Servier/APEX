{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete MTL-GNN-DTA Pipeline: Data Preparation, Training, and Analysis\n",
    "\n",
    "This comprehensive notebook provides the complete pipeline for:\n",
    "1. Data preparation and standardization\n",
    "2. Model training with multi-task learning\n",
    "3. Model evaluation and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy import stats\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Crippen, Lipinski, rdMolDescriptors, QED, Draw\n",
    "from rdkit.Chem.rdPartialCharges import ComputeGasteigerCharges\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "# Protein processing\n",
    "from Bio.PDB import PDBParser, PDBIO, Select, is_aa\n",
    "from Bio.SeqUtils import seq1\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile, Modeller, element as elem\n",
    "\n",
    "# PyTorch and PyG\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "# Setup\n",
    "N_PROC = cpu_count() - 1\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "POLAR_HEAVY = {7, 8, 15, 16}  # N, O, P, S\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(f\"MTL-GNN-DTA Complete Pipeline\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Using {N_PROC} CPU cores\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data preparation\n",
    "\n",
    "def dg_to_kd(delta_g_kcal, temp_k=298.15):\n",
    "    R = 1.987e-3\n",
    "    kd_molar = -math.log10(math.exp(delta_g_kcal / (R * temp_k)))\n",
    "    return kd_molar\n",
    "\n",
    "def keep_only_polar_hydrogens(mol):\n",
    "    h_to_remove = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetAtomicNum() == 1:\n",
    "            neighbors = atom.GetNeighbors()\n",
    "            if neighbors and neighbors[0].GetAtomicNum() not in POLAR_HEAVY:\n",
    "                h_to_remove.append(atom.GetIdx())\n",
    "    \n",
    "    if h_to_remove:\n",
    "        em = Chem.EditableMol(mol)\n",
    "        for idx in sorted(h_to_remove, reverse=True):\n",
    "            em.RemoveAtom(idx)\n",
    "        mol = em.GetMol()\n",
    "    \n",
    "    mol.UpdatePropertyCache(strict=False)\n",
    "    targets = [a.GetIdx() for a in mol.GetAtoms()\n",
    "               if a.GetAtomicNum() in POLAR_HEAVY and a.GetNumImplicitHs() > 0]\n",
    "    if targets:\n",
    "        mol = Chem.AddHs(mol, addCoords=True, onlyOnAtoms=targets)\n",
    "    \n",
    "    return mol\n",
    "\n",
    "def standardize_smiles_from_sdf(sdf_path):\n",
    "    try:\n",
    "        mol = Chem.MolFromMolFile(sdf_path, removeHs=False, sanitize=False)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        sanitize_result = Chem.SanitizeMol(mol, catchErrors=True)\n",
    "        if sanitize_result != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "            mol = rdMolStandardize.Cleanup(mol)\n",
    "            if mol is None:\n",
    "                return None\n",
    "        \n",
    "        mol = keep_only_polar_hydrogens(mol)\n",
    "        mol = rdMolStandardize.Normalizer().normalize(mol)\n",
    "        mol = rdMolStandardize.FragmentParent(mol)\n",
    "        mol = rdMolStandardize.TautomerEnumerator().Canonicalize(mol)\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            atom.SetIsotope(0)\n",
    "        \n",
    "        Chem.AssignStereochemistry(mol, force=True, cleanIt=True)\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=True, canonical=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compute_props(smiles):\n",
    "    if not isinstance(smiles, str) or smiles.strip() == '':\n",
    "        return {k: None for k in ['InChIKey', 'MolWt', 'HeavyAtomCount', 'QED', \n",
    "                                  'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', \n",
    "                                  'TPSA', 'LogP']}\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {k: None for k in ['InChIKey', 'MolWt', 'HeavyAtomCount', 'QED', \n",
    "                                  'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', \n",
    "                                  'TPSA', 'LogP']}\n",
    "    return {\n",
    "        'InChIKey': Chem.MolToInchiKey(mol),\n",
    "        'MolWt': Descriptors.MolWt(mol),\n",
    "        'HeavyAtomCount': mol.GetNumHeavyAtoms(),\n",
    "        'QED': QED.qed(mol),\n",
    "        'NumHDonors': Lipinski.NumHDonors(mol),\n",
    "        'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
    "        'NumRotatableBonds': Lipinski.NumRotatableBonds(mol),\n",
    "        'TPSA': rdMolDescriptors.CalcTPSA(mol),\n",
    "        'LogP': Crippen.MolLogP(mol)\n",
    "    }\n",
    "\n",
    "def clean_protein_structure(pdb_path, output_path, ph=7.4, remove_water=True):\n",
    "    try:\n",
    "        fixer = PDBFixer(filename=pdb_path)\n",
    "        fixer.findMissingResidues()\n",
    "        fixer.findNonstandardResidues()\n",
    "        fixer.replaceNonstandardResidues()\n",
    "        fixer.removeHeterogens(keepWater=not remove_water)\n",
    "        fixer.findMissingAtoms()\n",
    "        fixer.addMissingAtoms()\n",
    "        fixer.addMissingHydrogens(pH=ph)\n",
    "        \n",
    "        mod = Modeller(fixer.topology, fixer.positions)\n",
    "        to_delete = []\n",
    "        for atom in mod.topology.atoms():\n",
    "            if atom.element == elem.hydrogen:\n",
    "                for bond in mod.topology.bonds():\n",
    "                    a1, a2 = bond\n",
    "                    if (a1 == atom and a2.element == elem.carbon) or \\\n",
    "                       (a2 == atom and a1.element == elem.carbon):\n",
    "                        to_delete.append(atom)\n",
    "                        break\n",
    "        \n",
    "        if to_delete:\n",
    "            mod.delete(to_delete)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            PDBFile.writeFile(mod.topology, mod.positions, f)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def standardize_ligand(sdf_path, output_path):\n",
    "    try:\n",
    "        mol = Chem.MolFromMolFile(sdf_path, removeHs=False, sanitize=False)\n",
    "        if mol is None:\n",
    "            return False\n",
    "        \n",
    "        Chem.SanitizeMol(mol)\n",
    "        mol = rdMolStandardize.Cleanup(mol)\n",
    "        mol = keep_only_polar_hydrogens(mol)\n",
    "        Chem.AssignStereochemistry(mol, cleanIt=False, force=True)\n",
    "        \n",
    "        if mol.GetNumConformers() == 0:\n",
    "            AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "        \n",
    "        ComputeGasteigerCharges(mol)\n",
    "        \n",
    "        writer = Chem.SDWriter(output_path)\n",
    "        writer.write(mol)\n",
    "        writer.close()\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "data_dir = Path(\"../data/curated/exp/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create sample data if no real data available\n",
    "print(\"Creating sample dataset for demonstration...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "df_combined = pd.DataFrame({\n",
    "    'protein_pdb_path': [f'protein_{i:04d}.pdb' for i in range(n_samples)],\n",
    "    'ligand_sdf_path': [f'ligand_{i:04d}.sdf' for i in range(n_samples)],\n",
    "    'smiles': ['CC(C)CC1=CC=C(C=C1)C(C)C(O)=O'] * n_samples,\n",
    "    'pKi': np.random.normal(7.0, 1.5, n_samples),\n",
    "    'pKd': np.random.normal(7.2, 1.3, n_samples),\n",
    "    'pIC50': np.random.normal(6.8, 1.4, n_samples),\n",
    "    'pEC50': np.random.normal(6.5, 1.2, n_samples),\n",
    "    'resolution': np.random.uniform(1.5, 2.5, n_samples),\n",
    "    'source_file': 'sample_data',\n",
    "    'is_experimental': True\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "for col in ['pKi', 'pKd', 'pIC50', 'pEC50']:\n",
    "    mask = np.random.random(n_samples) < 0.2\n",
    "    df_combined.loc[mask, col] = np.nan\n",
    "\n",
    "df_combined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df_combined.shape}\")\n",
    "print(f\"Columns: {df_combined.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data filtering\n",
    "print(\"\\nFiltering data...\")\n",
    "print(f\"Starting samples: {len(df_combined)}\")\n",
    "\n",
    "task_cols = ['pKi', 'pKd', 'pIC50', 'pEC50']\n",
    "for col in task_cols:\n",
    "    if col in df_combined.columns:\n",
    "        df_combined = df_combined[df_combined[col].isna() | ((df_combined[col] > 3) & (df_combined[col] < 15))]\n",
    "\n",
    "if 'resolution' in df_combined.columns:\n",
    "    df_combined = df_combined[df_combined['resolution'].isna() | ((df_combined['resolution'] > 0) & (df_combined['resolution'] < 3))]\n",
    "\n",
    "print(f\"Samples after filtering: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize SMILES and compute properties\n",
    "print(\"\\nStandardizing SMILES and computing properties...\")\n",
    "\n",
    "# For demonstration, use existing SMILES\n",
    "df_combined['std_smiles'] = df_combined['smiles'].apply(\n",
    "    lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x), canonical=True) \n",
    "    if pd.notna(x) and Chem.MolFromSmiles(x) else None\n",
    ")\n",
    "\n",
    "df_combined = df_combined[df_combined['std_smiles'].notna()]\n",
    "\n",
    "# Compute molecular properties\n",
    "props = Parallel(n_jobs=min(4, N_PROC))(\n",
    "    delayed(compute_props)(smi) for smi in tqdm(df_combined['std_smiles'].tolist(), desc=\"Properties\")\n",
    ")\n",
    "props_df = pd.DataFrame(props)\n",
    "df_combined = pd.concat([df_combined.reset_index(drop=True), props_df], axis=1)\n",
    "\n",
    "print(f\"Computed properties for {len(df_combined)} molecules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ligand efficiency\n",
    "print(\"\\nCalculating ligand efficiency...\")\n",
    "\n",
    "for col in task_cols:\n",
    "    if col in df_combined.columns:\n",
    "        le_col = f'LE_{col}'\n",
    "        df_combined[le_col] = df_combined.apply(\n",
    "            lambda row: row[col] / row['HeavyAtomCount']\n",
    "            if pd.notnull(row[col]) and pd.notnull(row['HeavyAtomCount']) and row['HeavyAtomCount'] > 0\n",
    "            else None,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "le_cols = [c for c in df_combined.columns if c.startswith(\"LE_\")]\n",
    "if le_cols:\n",
    "    df_combined['LE'] = df_combined[le_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "print(\"Ligand efficiency calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-based filtering\n",
    "print(\"\\nApplying quality filters...\")\n",
    "\n",
    "def count_carbon_atoms(smiles):\n",
    "    if pd.isna(smiles):\n",
    "        return 0\n",
    "    return smiles.count('C') + smiles.count('c')\n",
    "\n",
    "df_combined['carbon_count'] = df_combined['std_smiles'].apply(count_carbon_atoms)\n",
    "\n",
    "# Apply filters\n",
    "bad_filter = (\n",
    "    (df_combined['carbon_count'] < 4) |\n",
    "    (df_combined['HeavyAtomCount'] < 5) |\n",
    "    (df_combined['HeavyAtomCount'] > 75) |\n",
    "    (df_combined['MolWt'] > 1000)\n",
    ")\n",
    "\n",
    "if 'LE' in df_combined.columns:\n",
    "    bad_filter |= (df_combined['LE'] <= 0.05) | (df_combined['LE'] >= 0.7)\n",
    "\n",
    "df_good = df_combined[~bad_filter].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered out {bad_filter.sum()} samples\")\n",
    "print(f\"Remaining good samples: {len(df_good)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split\n",
    "print(\"\\nSplitting data...\")\n",
    "\n",
    "train_val_df, test_df = train_test_split(df_good, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate task ranges for multi-task learning\n",
    "task_ranges = {}\n",
    "task_weights = {}\n",
    "\n",
    "for task in task_cols:\n",
    "    if task in train_df.columns:\n",
    "        valid_values = train_df[task].dropna()\n",
    "        if len(valid_values) > 0:\n",
    "            task_range = valid_values.max() - valid_values.min()\n",
    "            task_ranges[task] = task_range\n",
    "            task_weights[task] = 1.0 / task_range if task_range > 0 else 1.0\n",
    "\n",
    "if task_weights:\n",
    "    total_weight = sum(task_weights.values())\n",
    "    task_weights = {k: v/total_weight for k, v in task_weights.items()}\n",
    "\n",
    "print(\"\\nTask ranges and weights:\")\n",
    "for task in task_ranges:\n",
    "    print(f\"  {task}: range={task_ranges[task]:.2f}, weight={task_weights[task]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_parquet(output_dir / \"train_data.parquet\", index=False)\n",
    "val_df.to_parquet(output_dir / \"val_data.parquet\", index=False)\n",
    "test_df.to_parquet(output_dir / \"test_data.parquet\", index=False)\n",
    "\n",
    "with open(output_dir / \"task_ranges.json\", 'w') as f:\n",
    "    json.dump(task_ranges, f, indent=2)\n",
    "\n",
    "print(f\"\\nData saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GVP layers for drug encoder\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.s, self.v = dims\n",
    "        self.scalar_norm = nn.LayerNorm(self.s) if self.s else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.v:\n",
    "            return self.scalar_norm(x) if self.scalar_norm else x\n",
    "        s, v = x\n",
    "        if self.scalar_norm:\n",
    "            s = self.scalar_norm(s)\n",
    "        return s, v\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims, h_dim=None, activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super().__init__()\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.vector_gate = vector_gate\n",
    "        \n",
    "        if self.vi:\n",
    "            self.h_dim = h_dim or max(self.vi, self.vo) \n",
    "            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n",
    "            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n",
    "            if self.vo:\n",
    "                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n",
    "        else:\n",
    "            self.ws = nn.Linear(self.si, self.so)\n",
    "        \n",
    "        self.scalar_act, self.vector_act = activations\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.vi:\n",
    "            s, v = x\n",
    "            v = torch.transpose(v, -1, -2)\n",
    "            vh = self.wh(v) \n",
    "            vn = torch.norm(vh, dim=-2)\n",
    "            s = self.ws(torch.cat([s, vn], -1))\n",
    "            if self.vo:\n",
    "                v = self.wv(vh)\n",
    "                v = torch.transpose(v, -1, -2)\n",
    "        else:\n",
    "            s = self.ws(x)\n",
    "            v = None\n",
    "        \n",
    "        if self.scalar_act:\n",
    "            s = self.scalar_act(s)\n",
    "        \n",
    "        return (s, v) if self.vo else s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Drug Encoder\n",
    "\n",
    "class DrugGCN(nn.Module):\n",
    "    def __init__(self, node_in_dim=66, node_h_dims=[128, 256, 128], fc_dims=[1024, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_embedding = nn.Linear(node_in_dim, node_h_dims[0])\n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        in_dim = node_h_dims[0]\n",
    "        for out_dim in node_h_dims[1:]:\n",
    "            self.gcn_layers.append(GCNConv(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = node_h_dims[-1] * 2\n",
    "        for out_dim in fc_dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(dim) for dim in node_h_dims[1:]])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_embedding(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i, gcn in enumerate(self.gcn_layers):\n",
    "            x = gcn(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x_mean = global_mean_pool(x, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        for i, fc in enumerate(self.fc_layers):\n",
    "            x = fc(x)\n",
    "            if i < len(self.fc_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Protein Encoder\n",
    "\n",
    "class ProteinGCN(nn.Module):\n",
    "    def __init__(self, emb_dim=1280, gcn_dims=[128, 256, 256], fc_dims=[1024, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        in_dim = emb_dim\n",
    "        for out_dim in gcn_dims:\n",
    "            self.gcn_layers.append(GCNConv(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = gcn_dims[-1]\n",
    "        for out_dim in fc_dims:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(dim) for dim in gcn_dims])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for i, gcn in enumerate(self.gcn_layers):\n",
    "            x = gcn(x, edge_index)\n",
    "            x = self.bn_layers[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        for i, fc in enumerate(self.fc_layers):\n",
    "            x = fc(x)\n",
    "            if i < len(self.fc_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MTL-DTA Model\n",
    "\n",
    "class MTL_DTAModel(nn.Module):\n",
    "    def __init__(self, task_names, prot_emb_dim=1280, prot_gcn_dims=[128, 256, 256],\n",
    "                 prot_fc_dims=[1024, 128], drug_node_in_dim=66, drug_node_h_dims=[128, 256, 128],\n",
    "                 drug_fc_dims=[1024, 128], mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.task_names = task_names\n",
    "        self.n_tasks = len(task_names)\n",
    "        \n",
    "        self.protein_encoder = ProteinGCN(prot_emb_dim, prot_gcn_dims, prot_fc_dims, mlp_dropout)\n",
    "        self.drug_encoder = DrugGCN(drug_node_in_dim, drug_node_h_dims, drug_fc_dims, mlp_dropout)\n",
    "        \n",
    "        prot_out_dim = prot_fc_dims[-1]\n",
    "        drug_out_dim = drug_fc_dims[-1]\n",
    "        combined_dim = prot_out_dim + drug_out_dim\n",
    "        \n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        in_dim = combined_dim\n",
    "        for out_dim in mlp_dims:\n",
    "            self.shared_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            task: nn.Linear(mlp_dims[-1], 1) for task in task_names\n",
    "        })\n",
    "        \n",
    "        self.dropout = nn.Dropout(mlp_dropout)\n",
    "        self.bn = nn.BatchNorm1d(combined_dim)\n",
    "    \n",
    "    def forward(self, drug_batch, protein_batch):\n",
    "        drug_repr = self.drug_encoder(\n",
    "            drug_batch.x,\n",
    "            drug_batch.edge_index,\n",
    "            drug_batch.edge_attr if hasattr(drug_batch, 'edge_attr') else None,\n",
    "            drug_batch.batch\n",
    "        )\n",
    "        \n",
    "        protein_repr = self.protein_encoder(\n",
    "            protein_batch.x,\n",
    "            protein_batch.edge_index,\n",
    "            protein_batch.batch\n",
    "        )\n",
    "        \n",
    "        combined = torch.cat([drug_repr, protein_repr], dim=1)\n",
    "        combined = self.bn(combined)\n",
    "        combined = F.relu(combined)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        for layer in self.shared_layers:\n",
    "            combined = layer(combined)\n",
    "            combined = F.relu(combined)\n",
    "            combined = self.dropout(combined)\n",
    "        \n",
    "        predictions = []\n",
    "        for task in self.task_names:\n",
    "            pred = self.task_heads[task](combined)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return torch.cat(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self, task_ranges=None):\n",
    "        super().__init__()\n",
    "        self.task_ranges = task_ranges or {}\n",
    "        \n",
    "        if self.task_ranges:\n",
    "            weights = []\n",
    "            for task_range in self.task_ranges.values():\n",
    "                weights.append(1.0 / task_range if task_range > 0 else 1.0)\n",
    "            total_weight = sum(weights)\n",
    "            self.weights = torch.tensor([w / total_weight for w in weights])\n",
    "        else:\n",
    "            self.weights = None\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mask = ~torch.isnan(target)\n",
    "        \n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        task_losses = []\n",
    "        for i in range(target.shape[1]):\n",
    "            task_mask = mask[:, i]\n",
    "            if task_mask.sum() > 0:\n",
    "                task_pred = pred[task_mask, i]\n",
    "                task_target = target[task_mask, i]\n",
    "                task_loss = F.mse_loss(task_pred, task_target)\n",
    "                \n",
    "                if self.weights is not None:\n",
    "                    task_loss = task_loss * self.weights[i].to(pred.device)\n",
    "                \n",
    "                task_losses.append(task_loss)\n",
    "        \n",
    "        if len(task_losses) == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        return torch.stack(task_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Extraction Functions\n",
    "\n",
    "def atom_features(atom):\n",
    "    atom_types = ['C', 'N', 'O', 'S', 'F', 'P', 'Cl', 'Br', 'I', 'H']\n",
    "    features = []\n",
    "    \n",
    "    # One-hot encoding for atom type\n",
    "    atom_type = [0] * len(atom_types)\n",
    "    if atom.GetSymbol() in atom_types:\n",
    "        atom_type[atom_types.index(atom.GetSymbol())] = 1\n",
    "    features.extend(atom_type)\n",
    "    \n",
    "    # Degree\n",
    "    degree = [0] * 6\n",
    "    if atom.GetDegree() < 6:\n",
    "        degree[atom.GetDegree()] = 1\n",
    "    features.extend(degree)\n",
    "    \n",
    "    # Hybridization\n",
    "    hybridizations = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ]\n",
    "    hybridization = [0] * len(hybridizations)\n",
    "    if atom.GetHybridization() in hybridizations:\n",
    "        hybridization[hybridizations.index(atom.GetHybridization())] = 1\n",
    "    features.extend(hybridization)\n",
    "    \n",
    "    # Implicit valence\n",
    "    impl_valence = [0] * 6\n",
    "    if atom.GetImplicitValence() < 6:\n",
    "        impl_valence[atom.GetImplicitValence()] = 1\n",
    "    features.extend(impl_valence)\n",
    "    \n",
    "    # Other features\n",
    "    features.append(atom.GetFormalCharge())\n",
    "    features.append(atom.GetNumRadicalElectrons())\n",
    "    features.append(int(atom.GetIsAromatic()))\n",
    "    features.append(int(atom.IsInRing()))\n",
    "    features.append(int(atom.HasProp('_ChiralityPossible')))\n",
    "    features.append(atom.GetMass())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def bond_features(bond):\n",
    "    bond_type = bond.GetBondType()\n",
    "    features = [\n",
    "        int(bond_type == Chem.rdchem.BondType.SINGLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.DOUBLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.TRIPLE),\n",
    "        int(bond_type == Chem.rdchem.BondType.AROMATIC),\n",
    "        int(bond.GetIsConjugated()),\n",
    "        int(bond.IsInRing())\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Node features\n",
    "    node_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        node_features.append(atom_features(atom))\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    \n",
    "    # Edge features\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.extend([[i, j], [j, i]])\n",
    "        bond_feat = bond_features(bond)\n",
    "        edge_features.extend([bond_feat, bond_feat])\n",
    "    \n",
    "    if len(edge_indices) > 0:\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.zeros((0, 6), dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def create_protein_graph(seq_len=100, emb_dim=1280):\n",
    "    # Create mock protein graph for demonstration\n",
    "    x = torch.randn(seq_len, emb_dim)\n",
    "    \n",
    "    # Create distance-based edges (mock)\n",
    "    edge_list = []\n",
    "    for i in range(seq_len):\n",
    "        for j in range(max(0, i-5), min(seq_len, i+6)):\n",
    "            if i != j:\n",
    "                edge_list.append([i, j])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t() if edge_list else torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset Class\n",
    "\n",
    "class MTL_DTA_Dataset(Dataset):\n",
    "    def __init__(self, df, task_cols):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.task_cols = task_cols\n",
    "        self.n_tasks = len(task_cols)\n",
    "        \n",
    "        # Pre-compute graphs\n",
    "        self.drug_graphs = []\n",
    "        self.protein_graphs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing samples\"):\n",
    "            # Drug graph\n",
    "            drug_graph = mol_to_graph(row['std_smiles'])\n",
    "            if drug_graph is None:\n",
    "                continue\n",
    "            \n",
    "            # Protein graph (mock for demonstration)\n",
    "            protein_graph = create_protein_graph()\n",
    "            \n",
    "            # Targets\n",
    "            y = torch.zeros(self.n_tasks)\n",
    "            for i, task in enumerate(self.task_cols):\n",
    "                if task in row and not pd.isna(row[task]):\n",
    "                    y[i] = float(row[task])\n",
    "                else:\n",
    "                    y[i] = float('nan')\n",
    "            \n",
    "            self.drug_graphs.append(drug_graph)\n",
    "            self.protein_graphs.append(protein_graph)\n",
    "            self.targets.append(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drug_graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'drug': self.drug_graphs[idx],\n",
    "            'protein': self.protein_graphs[idx],\n",
    "            'y': self.targets[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"\\nCreating datasets and data loaders...\")\n",
    "\n",
    "train_dataset = MTL_DTA_Dataset(train_df, task_cols)\n",
    "val_dataset = MTL_DTA_Dataset(val_df, task_cols)\n",
    "test_dataset = MTL_DTA_Dataset(test_df, task_cols)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    drugs = [item['drug'] for item in batch]\n",
    "    proteins = [item['protein'] for item in batch]\n",
    "    ys = torch.stack([item['y'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'drug': Batch.from_data_list(drugs),\n",
    "        'protein': Batch.from_data_list(proteins),\n",
    "        'y': ys\n",
    "    }\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "\n",
    "model = MTL_DTAModel(\n",
    "    task_names=task_cols,\n",
    "    prot_emb_dim=1280,\n",
    "    prot_gcn_dims=[128, 256, 256],\n",
    "    prot_fc_dims=[1024, 128],\n",
    "    drug_node_in_dim=66,\n",
    "    drug_node_h_dims=[128, 256, 128],\n",
    "    drug_fc_dims=[1024, 128],\n",
    "    mlp_dims=[1024, 512],\n",
    "    mlp_dropout=0.25\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "criterion = MaskedMSELoss(task_ranges=task_ranges).to(DEVICE)\n",
    "\n",
    "print(\"\\nTraining setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        drug_batch = batch['drug'].to(device)\n",
    "        protein_batch = batch['protein'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(drug_batch, protein_batch)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            drug_batch = batch['drug'].to(device)\n",
    "            protein_batch = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            predictions = model(drug_batch, protein_batch)\n",
    "            loss = criterion(predictions, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_targets.append(y.cpu())\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    for i, task in enumerate(task_cols):\n",
    "        mask = ~np.isnan(all_targets[:, i])\n",
    "        if mask.sum() > 0:\n",
    "            pred = all_predictions[mask, i]\n",
    "            target = all_targets[mask, i]\n",
    "            metrics[f'{task}_r2'] = r2_score(target, pred)\n",
    "            metrics[f'{task}_rmse'] = np.sqrt(mean_squared_error(target, pred))\n",
    "    \n",
    "    return total_loss / n_batches, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_epochs = 20  # Reduced for demonstration\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_metrics_history = []\n",
    "val_metrics_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_metrics = validate(model, val_loader, criterion, DEVICE)\n",
    "    val_losses.append(val_loss)\n",
    "    val_metrics_history.append(val_metrics)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    avg_r2 = np.mean([v for k, v in val_metrics.items() if 'r2' in k])\n",
    "    avg_rmse = np.mean([v for k, v in val_metrics.items() if 'rmse' in k])\n",
    "    print(f\"Val Avg R2: {avg_r2:.4f}, Avg RMSE: {avg_rmse:.4f}\")\n",
    "    \n",
    "    # Check for best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"✓ New best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nLoaded best model with validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax.plot(val_losses, label='Val Loss', marker='s')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# R2 curves\n",
    "ax = axes[1]\n",
    "for task in task_cols:\n",
    "    r2_values = [metrics.get(f'{task}_r2', np.nan) for metrics in val_metrics_history]\n",
    "    if not all(np.isnan(r2_values)):\n",
    "        ax.plot(r2_values, label=f'{task} R²', marker='o')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('Validation R² by Task')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        drug_batch = batch['drug'].to(DEVICE)\n",
    "        protein_batch = batch['protein'].to(DEVICE)\n",
    "        y = batch['y']\n",
    "        \n",
    "        predictions = model(drug_batch, protein_batch)\n",
    "        test_predictions.append(predictions.cpu())\n",
    "        test_targets.append(y)\n",
    "\n",
    "test_predictions = torch.cat(test_predictions, dim=0).numpy()\n",
    "test_targets = torch.cat(test_targets, dim=0).numpy()\n",
    "\n",
    "# Calculate test metrics\n",
    "test_results = {}\n",
    "for i, task in enumerate(task_cols):\n",
    "    mask = ~np.isnan(test_targets[:, i])\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    pred = test_predictions[mask, i]\n",
    "    target = test_targets[mask, i]\n",
    "    \n",
    "    test_results[task] = {\n",
    "        'n_samples': mask.sum(),\n",
    "        'r2': r2_score(target, pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(target, pred)),\n",
    "        'mae': mean_absolute_error(target, pred),\n",
    "        'pearson': stats.pearsonr(target, pred)[0],\n",
    "        'spearman': stats.spearmanr(target, pred)[0]\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for task, metrics in test_results.items():\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  Samples: {metrics['n_samples']}\")\n",
    "    print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  Pearson: {metrics['pearson']:.4f}\")\n",
    "    print(f\"  Spearman: {metrics['spearman']:.4f}\")\n",
    "\n",
    "# Overall metrics\n",
    "overall_r2 = np.mean([m['r2'] for m in test_results.values()])\n",
    "overall_rmse = np.mean([m['rmse'] for m in test_results.values()])\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Average R²: {overall_r2:.4f}\")\n",
    "print(f\"  Average RMSE: {overall_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for predictions vs actual\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (i, task) in enumerate(zip(range(len(task_cols)), task_cols)):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    mask = ~np.isnan(test_targets[:, i])\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        pred = test_predictions[mask, i]\n",
    "        target = test_targets[mask, i]\n",
    "        \n",
    "        ax.scatter(target, pred, alpha=0.5, s=10)\n",
    "        \n",
    "        # Add diagonal line\n",
    "        min_val = min(target.min(), pred.min())\n",
    "        max_val = max(target.max(), pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        \n",
    "        # Add regression line\n",
    "        z = np.polyfit(target, pred, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(target, p(target), 'g-', alpha=0.5, lw=2)\n",
    "        \n",
    "        ax.set_xlabel(f'Actual {task}')\n",
    "        ax.set_ylabel(f'Predicted {task}')\n",
    "        ax.set_title(f'{task}: R²={test_results[task][\"r2\"]:.3f}, RMSE={test_results[task][\"rmse\"]:.3f}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Test Set Predictions vs Actual Values', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (i, task) in enumerate(zip(range(len(task_cols)), task_cols)):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    mask = ~np.isnan(test_targets[:, i])\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        pred = test_predictions[mask, i]\n",
    "        target = test_targets[mask, i]\n",
    "        errors = pred - target\n",
    "        \n",
    "        ax.hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(0, color='red', linestyle='--', lw=2)\n",
    "        ax.axvline(errors.mean(), color='green', linestyle='--', lw=2, label=f'Mean: {errors.mean():.3f}')\n",
    "        \n",
    "        ax.set_xlabel('Prediction Error')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(f'{task} Error Distribution (Std: {errors.std():.3f})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Prediction Error Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using attention weights\n",
    "print(\"\\nAnalyzing model components...\")\n",
    "\n",
    "# Get model component sizes\n",
    "component_params = {\n",
    "    'Protein Encoder': sum(p.numel() for p in model.protein_encoder.parameters()),\n",
    "    'Drug Encoder': sum(p.numel() for p in model.drug_encoder.parameters()),\n",
    "    'Shared Layers': sum(p.numel() for p in model.shared_layers.parameters()),\n",
    "    'Task Heads': sum(p.numel() for p in model.task_heads.parameters())\n",
    "}\n",
    "\n",
    "# Plot component sizes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Pie chart of parameters\n",
    "ax1.pie(component_params.values(), labels=component_params.keys(), autopct='%1.1f%%')\n",
    "ax1.set_title('Model Parameter Distribution')\n",
    "\n",
    "# Bar chart of parameters\n",
    "ax2.bar(component_params.keys(), component_params.values())\n",
    "ax2.set_ylabel('Number of Parameters')\n",
    "ax2.set_title('Parameters by Component')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for comp, params in component_params.items():\n",
    "    print(f\"{comp}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-task correlation analysis\n",
    "print(\"\\nCross-task prediction correlation...\")\n",
    "\n",
    "# Calculate correlation between predicted values\n",
    "pred_df = pd.DataFrame(test_predictions, columns=task_cols)\n",
    "pred_corr = pred_df.corr()\n",
    "\n",
    "# Calculate correlation between actual values\n",
    "actual_df = pd.DataFrame(test_targets, columns=task_cols)\n",
    "actual_corr = actual_df.corr()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Predicted correlation\n",
    "sns.heatmap(pred_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax1, cbar_kws={\"shrink\": 0.8})\n",
    "ax1.set_title('Predicted Values Correlation')\n",
    "\n",
    "# Actual correlation\n",
    "sns.heatmap(actual_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, ax=ax2, cbar_kws={\"shrink\": 0.8})\n",
    "ax2.set_title('Actual Values Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by molecular properties\n",
    "print(\"\\nAnalyzing performance by molecular properties...\")\n",
    "\n",
    "# Merge predictions with molecular properties\n",
    "test_df_with_pred = test_df.copy()\n",
    "for i, task in enumerate(task_cols):\n",
    "    test_df_with_pred[f'{task}_pred'] = test_predictions[:, i]\n",
    "    test_df_with_pred[f'{task}_error'] = test_predictions[:, i] - test_targets[:, i]\n",
    "\n",
    "# Analyze error by molecular weight\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "properties_to_analyze = ['MolWt', 'LogP', 'HeavyAtomCount', 'NumRotatableBonds']\n",
    "\n",
    "for idx, prop in enumerate(properties_to_analyze):\n",
    "    if prop not in test_df_with_pred.columns:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for task in task_cols[:2]:  # Plot first 2 tasks\n",
    "        error_col = f'{task}_error'\n",
    "        if error_col in test_df_with_pred.columns:\n",
    "            mask = test_df_with_pred[error_col].notna()\n",
    "            if mask.sum() > 0:\n",
    "                x = test_df_with_pred.loc[mask, prop]\n",
    "                y = test_df_with_pred.loc[mask, error_col].abs()\n",
    "                ax.scatter(x, y, alpha=0.5, label=task, s=10)\n",
    "    \n",
    "    ax.set_xlabel(prop)\n",
    "    ax.set_ylabel('Absolute Error')\n",
    "    ax.set_title(f'Prediction Error vs {prop}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Error Analysis by Molecular Properties', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "print(\"\\nSaving model and results...\")\n",
    "\n",
    "checkpoint_dir = Path(\"../checkpoints\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': train_losses[-1],\n",
    "    'val_loss': val_losses[-1],\n",
    "    'task_cols': task_cols,\n",
    "    'task_ranges': task_ranges,\n",
    "    'test_results': test_results,\n",
    "    'config': {\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': 66,\n",
    "        'drug_node_h_dims': [128, 256, 128],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_dir / 'mtl_dta_model.pt')\n",
    "print(f\"Model saved to {checkpoint_dir / 'mtl_dta_model.pt'}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    **{f'{task}_actual': test_targets[:, i] for i, task in enumerate(task_cols)},\n",
    "    **{f'{task}_pred': test_predictions[:, i] for i, task in enumerate(task_cols)}\n",
    "})\n",
    "predictions_df.to_csv(checkpoint_dir / 'test_predictions.csv', index=False)\n",
    "print(f\"Predictions saved to {checkpoint_dir / 'test_predictions.csv'}\")\n",
    "\n",
    "# Save training history\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_metrics_history': val_metrics_history\n",
    "}\n",
    "with open(checkpoint_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n",
    "print(f\"Training history saved to {checkpoint_dir / 'training_history.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "print(f\"  Total samples processed: {len(df_good)}\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Tasks: {', '.join(task_cols)}\")\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for task, metrics in test_results.items():\n",
    "    print(f\"  {task}: R²={metrics['r2']:.3f}, RMSE={metrics['rmse']:.3f}\")\n",
    "\n",
    "print(f\"\\nOverall: R²={overall_r2:.3f}, RMSE={overall_rmse:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Complete MTL-GNN-DTA pipeline executed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
