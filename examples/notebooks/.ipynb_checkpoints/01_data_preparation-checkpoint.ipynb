{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d61477-aeb1-4d34-a2b5-b521c5cbfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example Notebook 1: Data Preparation for MTL-GNN-DTA\n",
    "This notebook demonstrates how to prepare and process data for training\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# # Data Preparation for MTL-GNN-DTA\n",
    "# \n",
    "# This notebook demonstrates:\n",
    "# 1. Loading and processing raw affinity data\n",
    "# 2. Standardizing molecular structures\n",
    "# 3. Computing molecular properties\n",
    "# 4. Preparing data for model training\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Import MTL-GNN-DTA modules\n",
    "from mtl_gnn_dta import Config, AffinityPredictor\n",
    "from mtl_gnn_dta.preprocessing import (\n",
    "    standardize_protein,\n",
    "    standardize_ligand,\n",
    "    validate_structures\n",
    ")\n",
    "from mtl_gnn_dta.features import DrugFeaturizer, ProteinFeaturizer\n",
    "from mtl_gnn_dta.utils import setup_logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"MTL-GNN-DTA Data Preparation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Configuration\n",
    "\n",
    "# %%\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Or load from file\n",
    "# config = Config('../../experiments/configs/default_config.yaml')\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Data directory: {config.data.processed_dir}\")\n",
    "print(f\"  Task names: {config.model.task_names}\")\n",
    "print(f\"  Batch size: {config.data.batch_size}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Load Raw Data\n",
    "\n",
    "# %%\n",
    "# Example: Load a sample dataset\n",
    "# In practice, replace with your actual data loading\n",
    "\n",
    "# Create sample data for demonstration\n",
    "sample_data = pd.DataFrame({\n",
    "    'protein_pdb_path': ['data/structures/protein_001.pdb'] * 100,\n",
    "    'ligand_sdf_path': ['data/structures/ligand_001.sdf'] * 100,\n",
    "    'smiles': ['CC(C)CC1=CC=C(C=C1)C(C)C(O)=O'] * 100,\n",
    "    'pKi': np.random.normal(7.0, 1.5, 100),\n",
    "    'pEC50': np.random.normal(6.5, 1.2, 100),\n",
    "    'pKd': np.random.normal(7.2, 1.3, 100),\n",
    "    'pIC50': np.random.normal(6.8, 1.4, 100)\n",
    "})\n",
    "\n",
    "# Add some missing values to simulate real data\n",
    "for col in ['pKi', 'pEC50', 'pKd', 'pIC50']:\n",
    "    mask = np.random.random(100) < 0.2\n",
    "    sample_data.loc[mask, col] = np.nan\n",
    "\n",
    "print(f\"Loaded {len(sample_data)} data points\")\n",
    "print(\"\\nData overview:\")\n",
    "print(sample_data.info())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Data Quality Analysis\n",
    "\n",
    "# %%\n",
    "# Analyze task distributions\n",
    "task_cols = ['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, task in enumerate(task_cols):\n",
    "    ax = axes[i]\n",
    "    valid_data = sample_data[task].dropna()\n",
    "    \n",
    "    ax.hist(valid_data, bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(valid_data.mean(), color='red', linestyle='--', label=f'Mean: {valid_data.mean():.2f}')\n",
    "    ax.axvline(valid_data.median(), color='green', linestyle='--', label=f'Median: {valid_data.median():.2f}')\n",
    "    \n",
    "    ax.set_xlabel(task)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{task} Distribution (n={len(valid_data)})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Activity Value Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nTask Statistics:\")\n",
    "print(\"=\"*50)\n",
    "for task in task_cols:\n",
    "    valid_data = sample_data[task].dropna()\n",
    "    print(f\"{task:10s}: n={len(valid_data):4d}, mean={valid_data.mean():.2f}, \"\n",
    "          f\"std={valid_data.std():.2f}, min={valid_data.min():.2f}, max={valid_data.max():.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Molecular Featurization Example\n",
    "\n",
    "# %%\n",
    "# Initialize featurizers\n",
    "drug_featurizer = DrugFeaturizer()\n",
    "protein_featurizer = ProteinFeaturizer()\n",
    "\n",
    "# Example: Featurize a drug from SMILES\n",
    "smiles_example = \"CC(C)CC1=CC=C(C=C1)C(C)C(O)=O\"\n",
    "print(f\"Featurizing SMILES: {smiles_example}\")\n",
    "\n",
    "drug_graph = drug_featurizer.featurize_from_smiles(smiles_example)\n",
    "if drug_graph:\n",
    "    print(f\"  Nodes: {drug_graph.x.shape[0]}\")\n",
    "    print(f\"  Node features: {drug_graph.x.shape[1]}\")\n",
    "    print(f\"  Edges: {drug_graph.edge_index.shape[1]}\")\n",
    "    print(f\"  Edge features: {drug_graph.edge_attr.shape[1] if drug_graph.edge_attr is not None else 0}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Data Splitting\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train/validation/test\n",
    "train_data, test_data = train_test_split(\n",
    "    sample_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, \n",
    "    test_size=0.125,  # 0.125 * 0.8 = 0.1 of total\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data split:\")\n",
    "print(f\"  Train: {len(train_data)} ({len(train_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_data)} ({len(val_data)/len(sample_data)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_data)} ({len(test_data)/len(sample_data)*100:.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Calculate Task Ranges for Multi-Task Learning\n",
    "\n",
    "# %%\n",
    "# Calculate task ranges for loss weighting\n",
    "task_ranges = {}\n",
    "for task in task_cols:\n",
    "    valid_values = train_data[task].dropna()\n",
    "    if len(valid_values) > 0:\n",
    "        task_ranges[task] = valid_values.max() - valid_values.min()\n",
    "    else:\n",
    "        task_ranges[task] = 1.0\n",
    "\n",
    "print(\"\\nTask ranges for loss weighting:\")\n",
    "print(\"=\"*50)\n",
    "for task, range_val in task_ranges.items():\n",
    "    weight = 1.0 / range_val if range_val > 0 else 1.0\n",
    "    normalized_weight = weight / sum(1.0/r if r > 0 else 1.0 for r in task_ranges.values())\n",
    "    print(f\"{task:10s}: range={range_val:.2f}, normalized_weight={normalized_weight:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Save Processed Data\n",
    "\n",
    "# %%\n",
    "# Save processed data\n",
    "output_dir = Path(config.data.processed_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save splits\n",
    "train_data.to_parquet(output_dir / 'train_data.parquet', index=False)\n",
    "val_data.to_parquet(output_dir / 'val_data.parquet', index=False)\n",
    "test_data.to_parquet(output_dir / 'test_data.parquet', index=False)\n",
    "\n",
    "# Save task ranges\n",
    "import json\n",
    "with open(output_dir / 'task_ranges.json', 'w') as f:\n",
    "    json.dump(task_ranges, f, indent=2)\n",
    "\n",
    "print(f\"\\nData saved to {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "for file in output_dir.glob('*.parquet'):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Data Quality Checks\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per task:\")\n",
    "print(\"=\"*50)\n",
    "for task in task_cols:\n",
    "    missing = train_data[task].isna().sum()\n",
    "    total = len(train_data)\n",
    "    print(f\"{task:10s}: {missing:4d} / {total:4d} ({missing/total*100:.1f}%)\")\n",
    "\n",
    "# Check correlations between tasks\n",
    "correlation_matrix = train_data[task_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Task Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Next Steps\n",
    "# \n",
    "# Now that the data is prepared, you can:\n",
    "# 1. Move to `02_model_training.ipynb` to train the model\n",
    "# 2. Customize the featurization process for your specific molecules\n",
    "# 3. Add additional data preprocessing steps as needed\n",
    "# \n",
    "# The prepared data includes:\n",
    "# - Standardized molecular structures\n",
    "# - Computed molecular properties\n",
    "# - Task value distributions\n",
    "# - Train/validation/test splits\n",
    "# - Task ranges for multi-task learning"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
