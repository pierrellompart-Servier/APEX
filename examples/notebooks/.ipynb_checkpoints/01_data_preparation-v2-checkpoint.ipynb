{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Standardization for MTL-GNN-DTA\n",
    "\n",
    "This notebook provides a complete pipeline for:\n",
    "1. Loading and combining multiple affinity datasets\n",
    "2. Standardizing protein and ligand structures\n",
    "3. Computing molecular properties and ligand efficiency\n",
    "4. Quality filtering and validation\n",
    "5. Preparing data for model training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdbfixer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Protein processing\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPDB\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDBParser, PDBIO\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdbfixer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDBFixer\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenmm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDBFile\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# MTL-GNN-DTA imports\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdbfixer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Crippen, Lipinski, rdMolDescriptors, QED, Draw\n",
    "from rdkit.Chem.rdPartialCharges import ComputeGasteigerCharges\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "# Protein processing\n",
    "from Bio.PDB import PDBParser, PDBIO\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "\n",
    "# MTL-GNN-DTA imports\n",
    "from mtl_gnn_dta.preprocessing import pdb_processor, sdf_processor, validator\n",
    "from mtl_gnn_dta.utils import setup_logging\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "N_PROC = cpu_count() - 1\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(f\"MTL-GNN-DTA Data Preparation Pipeline\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Using {N_PROC} CPU cores for parallel processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pdbfixer (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pdbfixer\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdbfixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions for Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "POLAR_HEAVY = {7, 8, 15, 16}  # N, O, P, S\n",
    "\n",
    "def dg_to_kd(delta_g_kcal, temp_k=298.15):\n",
    "    \"\"\"Convert ΔG to Kd\"\"\"\n",
    "    import math\n",
    "    R = 1.987e-3  # kcal/mol·K\n",
    "    kd_molar = -math.log10(math.exp(delta_g_kcal / (R * temp_k)))\n",
    "    return kd_molar\n",
    "\n",
    "def standardize_smiles_from_sdf(sdf_path):\n",
    "    \"\"\"Standardize SMILES from SDF file\"\"\"\n",
    "    POLAR = {7, 8, 15, 16}  # N, O, P, S\n",
    "    \n",
    "    try:\n",
    "        mol = Chem.MolFromMolFile(sdf_path, removeHs=False, sanitize=False)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        # Sanitize\n",
    "        sanitize_result = Chem.SanitizeMol(mol, catchErrors=True)\n",
    "        if sanitize_result != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "            mol_cleaned = rdMolStandardize.Cleanup(mol)\n",
    "            if mol_cleaned is None:\n",
    "                return None\n",
    "            mol = mol_cleaned\n",
    "        \n",
    "        # Remove non-polar hydrogens\n",
    "        to_del = []\n",
    "        for a in mol.GetAtoms():\n",
    "            if a.GetAtomicNum() == 1:\n",
    "                nbs = a.GetNeighbors()\n",
    "                if len(nbs) > 0 and nbs[0].GetAtomicNum() not in POLAR:\n",
    "                    to_del.append(a.GetIdx())\n",
    "        \n",
    "        if to_del:\n",
    "            em = Chem.EditableMol(mol)\n",
    "            for idx in sorted(to_del, reverse=True):\n",
    "                em.RemoveAtom(idx)\n",
    "            mol = em.GetMol()\n",
    "        \n",
    "        # Update and standardize\n",
    "        mol.UpdatePropertyCache(strict=False)\n",
    "        AllChem.AssignAtomChiralTagsFromStructure(mol, replaceExistingTags=False)\n",
    "        Chem.AssignStereochemistry(mol, force=True, cleanIt=False)\n",
    "        \n",
    "        # Add polar hydrogens\n",
    "        targets = []\n",
    "        for a in mol.GetAtoms():\n",
    "            if a.GetAtomicNum() in POLAR and a.GetNumImplicitHs() > 0:\n",
    "                targets.append(a.GetIdx())\n",
    "        \n",
    "        if targets:\n",
    "            mol = Chem.AddHs(mol, addCoords=False, onlyOnAtoms=targets)\n",
    "        \n",
    "        # Final standardization\n",
    "        mol = rdMolStandardize.Cleanup(mol)\n",
    "        mol = rdMolStandardize.Normalizer().normalize(mol)\n",
    "        mol = rdMolStandardize.FragmentParent(mol)\n",
    "        mol = rdMolStandardize.TautomerEnumerator().Canonicalize(mol)\n",
    "        \n",
    "        # Clear isotopes\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom.SetIsotope(0)\n",
    "        \n",
    "        Chem.AssignStereochemistry(mol, force=True, cleanIt=True)\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=True, canonical=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error standardizing {sdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def compute_props(smiles):\n",
    "    \"\"\"Compute molecular properties\"\"\"\n",
    "    if not isinstance(smiles, str) or smiles.strip() == '':\n",
    "        return {k: None for k in ['InChIKey', 'MolWt', 'HeavyAtomCount', 'QED', \n",
    "                                  'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', \n",
    "                                  'TPSA', 'LogP']}\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {k: None for k in ['InChIKey', 'MolWt', 'HeavyAtomCount', 'QED', \n",
    "                                  'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', \n",
    "                                  'TPSA', 'LogP']}\n",
    "    \n",
    "    return {\n",
    "        'InChIKey': Chem.MolToInchiKey(mol),\n",
    "        'MolWt': Descriptors.MolWt(mol),\n",
    "        'HeavyAtomCount': mol.GetNumHeavyAtoms(),\n",
    "        'QED': QED.qed(mol),\n",
    "        'NumHDonors': Lipinski.NumHDonors(mol),\n",
    "        'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
    "        'NumRotatableBonds': Lipinski.NumRotatableBonds(mol),\n",
    "        'TPSA': rdMolDescriptors.CalcTPSA(mol),\n",
    "        'LogP': Crippen.MolLogP(mol)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Combine Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_dir = Path(\"../data/curated/exp/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load all available datasets\n",
    "df_list = []\n",
    "dataset_info = []\n",
    "\n",
    "# List of expected dataset files\n",
    "dataset_files = [\n",
    "    \"pKd_FEP_Wang_2015.parquet\",\n",
    "    \"pKd_FEP_Zariquiey_extended_Wang_2015.parquet\",\n",
    "    \"pKi_PDBbind2020.parquet\",\n",
    "    \"pKd_PDBbind2020.parquet\",\n",
    "    \"pKi_HiQBind.parquet\",\n",
    "    \"pKd_HiQBind.parquet\",\n",
    "    \"pIC50_HiQBind.parquet\",\n",
    "    \"pEC50_HiQBind.parquet\",\n",
    "    \"pKi_BioLip2.parquet\",\n",
    "    \"pKd_BioLip2.parquet\",\n",
    "    \"pIC50_BioLip2.parquet\",\n",
    "    \"pEC50_BioLip2.parquet\",\n",
    "    \"pKi_BindingNetv1.parquet\",\n",
    "    \"pKd_BindingNetv1.parquet\",\n",
    "    \"pIC50_BindingNetv1.parquet\",\n",
    "    \"pEC50_BindingNetv1.parquet\",\n",
    "    \"pKi_BindingNetv2.parquet\",\n",
    "    \"pKd_BindingNetv2.parquet\",\n",
    "    \"pIC50_BindingNetv2.parquet\",\n",
    "    \"pEC50_BindingNetv2.parquet\"\n",
    "]\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "for fname in dataset_files:\n",
    "    full_path = data_dir / fname\n",
    "    if full_path.exists():\n",
    "        try:\n",
    "            df = pd.read_parquet(full_path)\n",
    "            df[\"source_file\"] = fname\n",
    "            df[\"is_experimental\"] = any(x in fname for x in [\"BioLip\", \"PDBbind\", \"HiQBind\"])\n",
    "            df_list.append(df)\n",
    "            \n",
    "            dataset_info.append({\n",
    "                'file': fname,\n",
    "                'samples': len(df),\n",
    "                'experimental': df[\"is_experimental\"].iloc[0]\n",
    "            })\n",
    "            print(f\"  ✓ {fname}: {len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading {fname}: {e}\")\n",
    "\n",
    "# Combine all datasets\n",
    "if df_list:\n",
    "    df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nTotal samples loaded: {len(df_combined):,}\")\n",
    "else:\n",
    "    print(\"No datasets found. Creating sample data...\")\n",
    "    # Create sample data for demonstration\n",
    "    df_combined = pd.DataFrame({\n",
    "        'protein_pdb_path': ['data/protein_001.pdb'] * 1000,\n",
    "        'ligand_sdf_path': ['data/ligand_001.sdf'] * 1000,\n",
    "        'smiles': ['CC(C)CC1=CC=C(C=C1)C(C)C(O)=O'] * 1000,\n",
    "        'pKi': np.random.normal(7.0, 1.5, 1000),\n",
    "        'pKd': np.random.normal(7.2, 1.3, 1000),\n",
    "        'pIC50': np.random.normal(6.8, 1.4, 1000),\n",
    "        'pEC50': np.random.normal(6.5, 1.2, 1000),\n",
    "        'resolution': np.random.uniform(1.5, 2.5, 1000),\n",
    "        'source_file': 'sample_data',\n",
    "        'is_experimental': True\n",
    "    })\n",
    "\n",
    "# Clean up source file names\n",
    "df_combined[\"source_file\"] = df_combined[\"source_file\"].str.replace(r\"^[^_]*_\", \"\", regex=True).str.replace(\".parquet\", \"\", regex=False)\n",
    "\n",
    "# Replace infinite values\n",
    "df_combined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"\\nDataset shape: {df_combined.shape}\")\n",
    "print(f\"Columns: {df_combined.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Dataset distribution\n",
    "ax = axes[0, 0]\n",
    "source_counts = df_combined[\"source_file\"].value_counts()\n",
    "source_counts.head(10).plot(kind='barh', ax=ax)\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"Top 10 Data Sources\")\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Experimental vs Computational\n",
    "ax = axes[0, 1]\n",
    "exp_counts = df_combined[\"is_experimental\"].value_counts()\n",
    "exp_counts.plot(kind='bar', ax=ax)\n",
    "ax.set_xticklabels(['Computational', 'Experimental'], rotation=0)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Experimental vs Computational Data\")\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Resolution distribution (if available)\n",
    "ax = axes[1, 0]\n",
    "if 'resolution' in df_combined.columns:\n",
    "    resolution_data = df_combined['resolution'].dropna()\n",
    "    if len(resolution_data) > 0:\n",
    "        resolution_data.hist(bins=50, ax=ax)\n",
    "        ax.axvline(2.5, color='red', linestyle='--', label='2.5Å cutoff')\n",
    "        ax.set_xlabel(\"Resolution (Å)\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(\"Crystal Structure Resolution\")\n",
    "        ax.legend()\n",
    "\n",
    "# Missing data analysis\n",
    "ax = axes[1, 1]\n",
    "task_cols = ['pKi', 'pKd', 'pIC50', 'pEC50']\n",
    "missing_data = []\n",
    "for col in task_cols:\n",
    "    if col in df_combined.columns:\n",
    "        missing_pct = df_combined[col].isna().sum() / len(df_combined) * 100\n",
    "        missing_data.append(missing_pct)\n",
    "    else:\n",
    "        missing_data.append(100)\n",
    "\n",
    "ax.bar(task_cols, missing_data)\n",
    "ax.set_ylabel(\"Missing (%)\")\n",
    "ax.set_title(\"Missing Data by Task\")\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Activity Value Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze activity value distributions\n",
    "task_cols = ['pKi', 'pKd', 'pIC50', 'pEC50', 'pKd (Wang, FEP)']\n",
    "available_tasks = [col for col in task_cols if col in df_combined.columns]\n",
    "\n",
    "if available_tasks:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(available_tasks[:6]):\n",
    "        ax = axes[i]\n",
    "        data = df_combined[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Histogram with KDE\n",
    "            data.hist(bins=50, ax=ax, alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            # Add statistics\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "            \n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'{col} (n={len(data):,})')\n",
    "            ax.legend(fontsize=8)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(available_tasks), 6):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Activity Value Distributions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nActivity Value Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    for col in available_tasks:\n",
    "        data = df_combined[col].dropna()\n",
    "        if len(data) > 0:\n",
    "            print(f\"{col:20s}: n={len(data):6d}, mean={data.mean():6.2f}, \"\n",
    "                  f\"std={data.std():5.2f}, min={data.min():5.2f}, max={data.max():5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial data filtering...\")\n",
    "print(f\"Starting samples: {len(df_combined):,}\")\n",
    "\n",
    "# Filter by activity values (remove outliers)\n",
    "for col in available_tasks:\n",
    "    if col in df_combined.columns:\n",
    "        before = len(df_combined)\n",
    "        df_combined = df_combined[df_combined[col].isna() | (df_combined[col] > 3)]\n",
    "        df_combined = df_combined[df_combined[col].isna() | (df_combined[col] < 15)]\n",
    "        after = len(df_combined)\n",
    "        if before != after:\n",
    "            print(f\"  Filtered {before - after} samples with {col} outliers\")\n",
    "\n",
    "# Filter by resolution\n",
    "if 'resolution' in df_combined.columns:\n",
    "    before = len(df_combined)\n",
    "    df_combined = df_combined[df_combined['resolution'].isna() | (df_combined['resolution'] > 0)]\n",
    "    df_combined = df_combined[df_combined['resolution'].isna() | (df_combined['resolution'] < 3)]\n",
    "    after = len(df_combined)\n",
    "    if before != after:\n",
    "        print(f\"  Filtered {before - after} samples with poor resolution\")\n",
    "\n",
    "print(f\"Samples after filtering: {len(df_combined):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Standardize Protein Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization directories\n",
    "protein_out_dir = Path(\"../data/standardized_clean/protein\")\n",
    "ligand_out_dir = Path(\"../data/standardized_clean/ligand\")\n",
    "protein_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "ligand_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample a subset for demonstration (remove this line for full processing)\n",
    "df_sample = df_combined.sample(min(100, len(df_combined)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Processing {len(df_sample)} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize proteins (if paths exist)\n",
    "if 'protein_pdb_path' in df_sample.columns:\n",
    "    from mtl_gnn_dta.preprocessing.pdb_processor import clean_protein_structure\n",
    "    \n",
    "    def process_protein(args):\n",
    "        idx, row = args\n",
    "        try:\n",
    "            in_path = row['protein_pdb_path']\n",
    "            out_path = protein_out_dir / f\"{idx}.pdb\"\n",
    "            \n",
    "            if os.path.exists(in_path):\n",
    "                success = clean_protein_structure(in_path, str(out_path))\n",
    "                if success:\n",
    "                    return str(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein {idx}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Standardizing protein structures...\")\n",
    "    protein_args = [(idx, row) for idx, row in df_sample.iterrows()]\n",
    "    \n",
    "    with Pool(min(4, N_PROC)) as pool:\n",
    "        protein_paths = list(tqdm(\n",
    "            pool.imap(process_protein, protein_args),\n",
    "            total=len(protein_args),\n",
    "            desc=\"Proteins\"\n",
    "        ))\n",
    "    \n",
    "    df_sample['standardized_protein_pdb'] = protein_paths\n",
    "else:\n",
    "    df_sample['standardized_protein_pdb'] = None\n",
    "    print(\"No protein paths found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Standardize Ligand Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize ligands (if paths exist)\n",
    "if 'ligand_sdf_path' in df_sample.columns:\n",
    "    from mtl_gnn_dta.preprocessing.sdf_processor import standardize_ligand\n",
    "    \n",
    "    def process_ligand(args):\n",
    "        idx, row = args\n",
    "        try:\n",
    "            in_path = row['ligand_sdf_path']\n",
    "            out_path = ligand_out_dir / f\"{idx}.sdf\"\n",
    "            \n",
    "            if os.path.exists(in_path):\n",
    "                success = standardize_ligand(in_path, str(out_path))\n",
    "                if success:\n",
    "                    return str(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ligand {idx}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Standardizing ligand structures...\")\n",
    "    ligand_args = [(idx, row) for idx, row in df_sample.iterrows()]\n",
    "    \n",
    "    with Pool(min(4, N_PROC)) as pool:\n",
    "        ligand_paths = list(tqdm(\n",
    "            pool.imap(process_ligand, ligand_args),\n",
    "            total=len(ligand_args),\n",
    "            desc=\"Ligands\"\n",
    "        ))\n",
    "    \n",
    "    df_sample['standardized_ligand_sdf'] = ligand_paths\n",
    "else:\n",
    "    df_sample['standardized_ligand_sdf'] = None\n",
    "    print(\"No ligand paths found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Standardize SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize SMILES\n",
    "print(\"Standardizing SMILES...\")\n",
    "\n",
    "# If we have standardized SDF files, extract SMILES from them\n",
    "if 'standardized_ligand_sdf' in df_sample.columns:\n",
    "    smiles_list = []\n",
    "    for sdf_path in tqdm(df_sample['standardized_ligand_sdf'], desc=\"Extracting SMILES\"):\n",
    "        if sdf_path and os.path.exists(sdf_path):\n",
    "            smiles = standardize_smiles_from_sdf(sdf_path)\n",
    "        else:\n",
    "            smiles = None\n",
    "        smiles_list.append(smiles)\n",
    "    df_sample['std_smiles'] = smiles_list\n",
    "# Otherwise, standardize from existing SMILES\n",
    "elif 'smiles' in df_sample.columns:\n",
    "    df_sample['std_smiles'] = df_sample['smiles'].apply(\n",
    "        lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x), canonical=True) \n",
    "        if pd.notna(x) and Chem.MolFromSmiles(x) else None\n",
    "    )\n",
    "\n",
    "# Remove samples with invalid SMILES\n",
    "before = len(df_sample)\n",
    "df_sample = df_sample[df_sample['std_smiles'].notna()]\n",
    "after = len(df_sample)\n",
    "print(f\"Removed {before - after} samples with invalid SMILES\")\n",
    "print(f\"Remaining samples: {len(df_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute Molecular Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute molecular properties\n",
    "print(\"Computing molecular properties...\")\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "smiles_list = df_sample['std_smiles'].tolist()\n",
    "props = Parallel(n_jobs=N_PROC)(\n",
    "    delayed(compute_props)(smi) for smi in tqdm(smiles_list, desc=\"Properties\")\n",
    ")\n",
    "\n",
    "props_df = pd.DataFrame(props)\n",
    "df_sample = pd.concat([df_sample.reset_index(drop=True), props_df], axis=1)\n",
    "\n",
    "print(f\"Computed properties for {len(df_sample)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate Ligand Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ligand efficiency\n",
    "print(\"Calculating ligand efficiency...\")\n",
    "\n",
    "# Find available activity columns\n",
    "activity_cols = []\n",
    "for col in df_sample.columns:\n",
    "    col_lower = col.strip().lower()\n",
    "    if col_lower in ['pki', 'pkd', 'pec50', 'pic50'] or 'pkd' in col_lower:\n",
    "        activity_cols.append(col)\n",
    "\n",
    "print(f\"Found activity columns: {activity_cols}\")\n",
    "\n",
    "# Calculate LE for each activity\n",
    "for col in activity_cols:\n",
    "    le_col = f'LE_{col}'\n",
    "    le_norm_col = f'LEnorm_{col}'\n",
    "    \n",
    "    df_sample[le_col] = df_sample.apply(\n",
    "        lambda row: row[col] / row['HeavyAtomCount']\n",
    "        if pd.notnull(row[col]) and pd.notnull(row['HeavyAtomCount']) and row['HeavyAtomCount'] > 0\n",
    "        else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df_sample[le_norm_col] = df_sample.apply(\n",
    "        lambda row: row[le_col] / row['MolWt']\n",
    "        if pd.notnull(row.get(le_col)) and pd.notnull(row.get('MolWt')) and row['MolWt'] > 0\n",
    "        else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Calculate mean LE\n",
    "le_cols = [c for c in df_sample.columns if c.startswith(\"LE_\") and not c.startswith(\"LEnorm_\")]\n",
    "le_norm_cols = [c for c in df_sample.columns if c.startswith(\"LEnorm_\")]\n",
    "\n",
    "if le_cols:\n",
    "    df_sample['LE'] = df_sample[le_cols].mean(axis=1, skipna=True)\n",
    "if le_norm_cols:\n",
    "    df_sample['LE_norm'] = df_sample[le_norm_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "print(\"Ligand efficiency calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Molecular Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot property distributions\n",
    "property_cols = ['LogP', 'QED', 'MolWt', 'HeavyAtomCount', 'TPSA', 'NumRotatableBonds']\n",
    "if 'LE' in df_sample.columns:\n",
    "    property_cols.append('LE')\n",
    "if 'LE_norm' in df_sample.columns:\n",
    "    property_cols.append('LE_norm')\n",
    "\n",
    "available_props = [col for col in property_cols if col in df_sample.columns]\n",
    "\n",
    "if available_props:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(available_props[:8]):\n",
    "        ax = axes[i]\n",
    "        data = df_sample[col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            data.hist(bins=30, ax=ax, alpha=0.7, edgecolor='black')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_title(f'{col} Distribution')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(available_props), 8):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Molecular Property Distributions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Quality-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filtering criteria\n",
    "print(\"\\nApplying quality filters...\")\n",
    "print(f\"Starting samples: {len(df_sample)}\")\n",
    "\n",
    "# Count carbon atoms\n",
    "def count_carbon_atoms(smiles):\n",
    "    if pd.isna(smiles):\n",
    "        return 0\n",
    "    return smiles.count('C') + smiles.count('c')\n",
    "\n",
    "df_sample['carbon_count'] = df_sample['std_smiles'].apply(count_carbon_atoms)\n",
    "\n",
    "# Define filters\n",
    "filters = {\n",
    "    'carbon_lt_4': df_sample['carbon_count'] < 4,\n",
    "    'low_heavy': df_sample['HeavyAtomCount'] < 5,\n",
    "    'high_heavy': df_sample['HeavyAtomCount'] > 75,\n",
    "    'high_MW': df_sample['MolWt'] > 1000,\n",
    "}\n",
    "\n",
    "# Add LE filters if available\n",
    "if 'LE' in df_sample.columns:\n",
    "    filters['low_le'] = df_sample['LE'] <= 0.05\n",
    "    filters['high_le'] = df_sample['LE'] >= 0.7\n",
    "\n",
    "if 'LE_norm' in df_sample.columns:\n",
    "    filters['high_le_norm'] = df_sample['LE_norm'] >= 0.003\n",
    "\n",
    "# Apply filters\n",
    "bad_filter = pd.Series([False] * len(df_sample))\n",
    "for name, filter_mask in filters.items():\n",
    "    n_filtered = filter_mask.sum()\n",
    "    if n_filtered > 0:\n",
    "        print(f\"  {name}: {n_filtered} samples\")\n",
    "        bad_filter |= filter_mask\n",
    "\n",
    "# Split good and bad samples\n",
    "df_bad = df_sample[bad_filter]\n",
    "df_good = df_sample[~bad_filter]\n",
    "\n",
    "print(f\"\\nFiltered out {len(df_bad)} samples\")\n",
    "print(f\"Remaining good samples: {len(df_good)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Display Top/Bottom Molecules by Ligand Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top and bottom molecules by LE\n",
    "if 'LE' in df_good.columns and len(df_good) > 0:\n",
    "    df_le_valid = df_good.dropna(subset=['LE'])\n",
    "    \n",
    "    if len(df_le_valid) > 0:\n",
    "        # Get top and bottom 5\n",
    "        top_le = df_le_valid.nlargest(min(5, len(df_le_valid)), 'LE')\n",
    "        bottom_le = df_le_valid.nsmallest(min(5, len(df_le_valid)), 'LE')\n",
    "        \n",
    "        print(\"\\nTop 5 molecules by Ligand Efficiency:\")\n",
    "        print(top_le[['std_smiles', 'LE', 'MolWt', 'HeavyAtomCount']].to_string())\n",
    "        \n",
    "        print(\"\\nBottom 5 molecules by Ligand Efficiency:\")\n",
    "        print(bottom_le[['std_smiles', 'LE', 'MolWt', 'HeavyAtomCount']].to_string())\n",
    "        \n",
    "        # Visualize molecules\n",
    "        try:\n",
    "            from rdkit.Chem import Draw\n",
    "            \n",
    "            print(\"\\nTop 5 molecules visualization:\")\n",
    "            top_mols = [Chem.MolFromSmiles(smi) for smi in top_le['std_smiles'].head(5)]\n",
    "            img = Draw.MolsToGridImage(top_mols, molsPerRow=5, subImgSize=(200, 200))\n",
    "            display(img)\n",
    "            \n",
    "            print(\"\\nBottom 5 molecules visualization:\")\n",
    "            bottom_mols = [Chem.MolFromSmiles(smi) for smi in bottom_le['std_smiles'].head(5)]\n",
    "            img = Draw.MolsToGridImage(bottom_mols, molsPerRow=5, subImgSize=(200, 200))\n",
    "            display(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not visualize molecules: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate final dataset\n",
    "from mtl_gnn_dta.preprocessing.validator import DataValidator\n",
    "\n",
    "validator = DataValidator()\n",
    "\n",
    "# Generate validation report\n",
    "print(\"\\nGenerating validation report...\")\n",
    "validation_report = validator.generate_validation_report(\n",
    "    df_good, \n",
    "    [col for col in activity_cols if col in df_good.columns]\n",
    ")\n",
    "\n",
    "print(\"\\nValidation Report:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {validation_report['total_samples']}\")\n",
    "\n",
    "if validation_report['molecule_validation']:\n",
    "    print(f\"\\nMolecule validation:\")\n",
    "    print(f\"  Valid: {validation_report['molecule_validation']['valid']}\")\n",
    "    print(f\"  Invalid: {validation_report['molecule_validation']['invalid']}\")\n",
    "    print(f\"  Percentage valid: {validation_report['molecule_validation']['percentage_valid']:.1f}%\")\n",
    "\n",
    "print(f\"\\nActivity validation:\")\n",
    "print(f\"  Valid: {validation_report['activity_validation']['valid']}\")\n",
    "print(f\"  Invalid: {validation_report['activity_validation']['invalid']}\")\n",
    "print(f\"  Percentage valid: {validation_report['activity_validation']['percentage_valid']:.1f}%\")\n",
    "\n",
    "print(f\"\\nFile validation:\")\n",
    "print(f\"  Valid: {validation_report['file_validation']['valid']}\")\n",
    "print(f\"  Invalid: {validation_report['file_validation']['invalid']}\")\n",
    "print(f\"  Percentage valid: {validation_report['file_validation']['percentage_valid']:.1f}%\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Overall valid: {validation_report['summary']['overall_valid']:.1f}%\")\n",
    "print(f\"  Ready for training: {validation_report['summary']['ready_for_training']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training\n",
    "print(\"\\nSplitting data into train/val/test sets...\")\n",
    "\n",
    "# First split: train+val vs test (80/20)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df_good, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: train vs val (90/10 of train+val)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df_good)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df_good)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_good)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Calculate Task Ranges for Multi-Task Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate task ranges for loss weighting\n",
    "task_ranges = {}\n",
    "task_weights = {}\n",
    "\n",
    "for task in activity_cols:\n",
    "    if task in train_df.columns:\n",
    "        valid_values = train_df[task].dropna()\n",
    "        if len(valid_values) > 0:\n",
    "            task_range = valid_values.max() - valid_values.min()\n",
    "            task_ranges[task] = task_range\n",
    "            task_weights[task] = 1.0 / task_range if task_range > 0 else 1.0\n",
    "\n",
    "# Normalize weights\n",
    "if task_weights:\n",
    "    total_weight = sum(task_weights.values())\n",
    "    task_weights = {k: v/total_weight for k, v in task_weights.items()}\n",
    "\n",
    "print(\"\\nTask ranges and weights for multi-task learning:\")\n",
    "print(\"=\"*60)\n",
    "for task in task_ranges:\n",
    "    print(f\"{task:20s}: range={task_ranges[task]:.2f}, weight={task_weights[task]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save splits\n",
    "print(\"\\nSaving processed data...\")\n",
    "\n",
    "train_df.to_parquet(output_dir / \"train_data.parquet\", index=False)\n",
    "val_df.to_parquet(output_dir / \"val_data.parquet\", index=False)\n",
    "test_df.to_parquet(output_dir / \"test_data.parquet\", index=False)\n",
    "\n",
    "# Save task ranges and weights\n",
    "with open(output_dir / \"task_ranges.json\", 'w') as f:\n",
    "    json.dump(task_ranges, f, indent=2)\n",
    "\n",
    "with open(output_dir / \"task_weights.json\", 'w') as f:\n",
    "    json.dump(task_weights, f, indent=2)\n",
    "\n",
    "# Save complete standardized dataset\n",
    "df_good.to_parquet(output_dir / \"standardized_data.parquet\", index=False)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_samples': len(df_good),\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'task_columns': activity_cols,\n",
    "    'task_ranges': task_ranges,\n",
    "    'task_weights': task_weights,\n",
    "    'validation_report': validation_report\n",
    "}\n",
    "\n",
    "with open(output_dir / \"metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Data saved to {output_dir}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal dataset statistics:\")\n",
    "print(f\"  Total samples: {len(df_good):,}\")\n",
    "print(f\"  Unique molecules: {df_good['InChIKey'].nunique():,}\")\n",
    "print(f\"  Average molecular weight: {df_good['MolWt'].mean():.1f} ± {df_good['MolWt'].std():.1f}\")\n",
    "print(f\"  Average heavy atoms: {df_good['HeavyAtomCount'].mean():.1f} ± {df_good['HeavyAtomCount'].std():.1f}\")\n",
    "\n",
    "if 'LE' in df_good.columns:\n",
    "    print(f\"  Average ligand efficiency: {df_good['LE'].mean():.3f} ± {df_good['LE'].std():.3f}\")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Training: {len(train_df):,} ({len(train_df)/len(df_good)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_df):,} ({len(val_df)/len(df_good)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_df):,} ({len(test_df)/len(df_good)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTasks available for training:\")\n",
    "for task in task_ranges:\n",
    "    n_train = train_df[task].notna().sum()\n",
    "    n_val = val_df[task].notna().sum()\n",
    "    n_test = test_df[task].notna().sum()\n",
    "    print(f\"  {task}: train={n_train}, val={n_val}, test={n_test}\")\n",
    "\n",
    "print(\"\\n✅ Data is ready for model training!\")\n",
    "print(\"Next step: Run the model training notebook (02_model_training.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "rldif118",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python (rldif118)",
   "language": "python",
   "name": "rldif118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
