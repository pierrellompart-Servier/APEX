{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49d831-35ff-49a2-8bd2-8c8b97c12e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example Notebook 2: Model Training for MTL-GNN-DTA\n",
    "This notebook demonstrates how to train the multi-task learning model\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# # Model Training for MTL-GNN-DTA\n",
    "# \n",
    "# This notebook demonstrates:\n",
    "# 1. Loading prepared data\n",
    "# 2. Initializing the MTL-DTA model\n",
    "# 3. Training with multi-task learning\n",
    "# 4. Monitoring training progress\n",
    "# 5. Saving trained models\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../../')\n",
    "\n",
    "# Import MTL-GNN-DTA modules\n",
    "from mtl_gnn_dta import (\n",
    "    Config, \n",
    "    MTL_DTAModel,\n",
    "    Trainer,\n",
    "    create_data_loaders\n",
    ")\n",
    "from mtl_gnn_dta.data import MTL_DTA\n",
    "from mtl_gnn_dta.features import ProteinFeaturizer, DrugFeaturizer\n",
    "from mtl_gnn_dta.models import MaskedMSELoss\n",
    "from mtl_gnn_dta.training import EarlyStopping\n",
    "from mtl_gnn_dta.utils import setup_logging, plot_training_history\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Load Configuration and Data\n",
    "\n",
    "# %%\n",
    "# Load configuration\n",
    "config = Config()\n",
    "print(\"Configuration loaded\")\n",
    "\n",
    "# Load prepared data\n",
    "data_dir = Path(config.data.processed_dir)\n",
    "train_data = pd.read_parquet(data_dir / 'train_data.parquet')\n",
    "val_data = pd.read_parquet(data_dir / 'val_data.parquet')\n",
    "test_data = pd.read_parquet(data_dir / 'test_data.parquet')\n",
    "\n",
    "# Load task ranges\n",
    "with open(data_dir / 'task_ranges.json', 'r') as f:\n",
    "    task_ranges = json.load(f)\n",
    "\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"  Train: {len(train_data)} samples\")\n",
    "print(f\"  Validation: {len(val_data)} samples\")\n",
    "print(f\"  Test: {len(test_data)} samples\")\n",
    "print(f\"  Tasks: {list(task_ranges.keys())}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Create Data Loaders\n",
    "\n",
    "# %%\n",
    "# For demonstration, we'll create mock data loaders\n",
    "# In practice, these would load actual protein structures and drug molecules\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create mock data for demonstration\n",
    "def create_mock_data(df, task_cols):\n",
    "    \"\"\"Create mock graph data for demonstration\"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Mock protein graph\n",
    "        protein_data = Data(\n",
    "            x=torch.randn(100, 1280),  # 100 residues, ESM-2 embedding dim\n",
    "            edge_index=torch.randint(0, 100, (2, 300))  # Random edges\n",
    "        )\n",
    "        \n",
    "        # Mock drug graph\n",
    "        drug_data = Data(\n",
    "            x=torch.randn(20, 66),  # 20 atoms, feature dim\n",
    "            edge_index=torch.randint(0, 20, (2, 40)),  # Random edges\n",
    "            edge_attr=torch.randn(40, 6)  # Edge features\n",
    "        )\n",
    "        \n",
    "        # Target values\n",
    "        y = torch.zeros(len(task_cols))\n",
    "        for i, task in enumerate(task_cols):\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                y[i] = float(row[task])\n",
    "            else:\n",
    "                y[i] = float('nan')\n",
    "        \n",
    "        data_list.append({\n",
    "            'protein': protein_data,\n",
    "            'drug': drug_data,\n",
    "            'y': y\n",
    "        })\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "# Create datasets\n",
    "task_cols = config.model.task_names\n",
    "train_dataset = create_mock_data(train_data.head(100), task_cols)  # Use subset for demo\n",
    "val_dataset = create_mock_data(val_data.head(20), task_cols)\n",
    "test_dataset = create_mock_data(test_data.head(20), task_cols)\n",
    "\n",
    "print(f\"Created datasets with {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test samples\")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_batch(batch):\n",
    "    proteins = [item['protein'] for item in batch]\n",
    "    drugs = [item['drug'] for item in batch]\n",
    "    ys = torch.stack([item['y'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'protein': Batch.from_data_list(proteins),\n",
    "        'drug': Batch.from_data_list(drugs),\n",
    "        'y': ys\n",
    "    }\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "print(f\"Created data loaders with batch size {32}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Initialize Model\n",
    "\n",
    "# %%\n",
    "# Create model\n",
    "model = MTL_DTAModel(\n",
    "    task_names=task_cols,\n",
    "    prot_emb_dim=config.model.prot_emb_dim,\n",
    "    prot_gcn_dims=config.model.prot_gcn_dims,\n",
    "    prot_fc_dims=config.model.prot_fc_dims,\n",
    "    drug_node_in_dim=config.model.drug_node_in_dim,\n",
    "    drug_node_h_dims=config.model.drug_node_h_dims,\n",
    "    drug_fc_dims=config.model.drug_fc_dims,\n",
    "    mlp_dims=config.model.mlp_dims,\n",
    "    mlp_dropout=config.model.mlp_dropout\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Tasks: {task_cols}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Setup Training\n",
    "\n",
    "# %%\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=config.training.learning_rate,\n",
    "    weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=config.training.scheduler_factor,\n",
    "    patience=config.training.scheduler_patience,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = MaskedMSELoss(task_ranges=task_ranges).to(device)\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.training.patience,\n",
    "    min_delta=config.training.min_delta\n",
    ")\n",
    "\n",
    "print(\"Training setup complete:\")\n",
    "print(f\"  Optimizer: {config.training.optimizer}\")\n",
    "print(f\"  Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  Loss function: MaskedMSELoss with task weighting\")\n",
    "print(f\"  Early stopping patience: {config.training.patience}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Training Loop\n",
    "\n",
    "# %%\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 20  # Reduced for demonstration\n",
    "print(f\"\\nStarting training for {n_epochs} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} - Training\"):\n",
    "        # Move data to device\n",
    "        protein_batch = batch['protein'].to(device)\n",
    "        drug_batch = batch['drug'].to(device)\n",
    "        y_batch = batch['y'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(drug_batch, protein_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            protein_batch = batch['protein'].to(device)\n",
    "            drug_batch = batch['drug'].to(device)\n",
    "            y_batch = batch['y'].to(device)\n",
    "            \n",
    "            predictions = model(drug_batch, protein_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Check for best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nLoaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Plot Training History\n",
    "\n",
    "# %%\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training History (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Evaluate on Test Set\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        protein_batch = batch['protein'].to(device)\n",
    "        drug_batch = batch['drug'].to(device)\n",
    "        y_batch = batch['y']\n",
    "        \n",
    "        predictions = model(drug_batch, protein_batch)\n",
    "        \n",
    "        test_predictions.append(predictions.cpu())\n",
    "        test_targets.append(y_batch)\n",
    "\n",
    "# Concatenate all predictions\n",
    "test_predictions = torch.cat(test_predictions, dim=0).numpy()\n",
    "test_targets = torch.cat(test_targets, dim=0).numpy()\n",
    "\n",
    "# Calculate metrics per task\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, task in enumerate(task_cols):\n",
    "    # Get valid (non-NaN) values\n",
    "    mask = ~np.isnan(test_targets[:, i])\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    task_preds = test_predictions[mask, i]\n",
    "    task_targets = test_targets[mask, i]\n",
    "    \n",
    "    r2 = r2_score(task_targets, task_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(task_targets, task_preds))\n",
    "    mae = mean_absolute_error(task_targets, task_preds)\n",
    "    \n",
    "    print(f\"{task:10s}: R²={r2:.3f}, RMSE={rmse:.3f}, MAE={mae:.3f} (n={mask.sum()})\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Save Trained Model\n",
    "\n",
    "# %%\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path(config.training.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': train_losses[-1],\n",
    "    'val_loss': val_losses[-1],\n",
    "    'task_cols': task_cols,\n",
    "    'task_ranges': task_ranges,\n",
    "    'config': config.to_dict()\n",
    "}\n",
    "\n",
    "checkpoint_path = checkpoint_dir / 'model_checkpoint.pt'\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"\\nModel saved to {checkpoint_path}\")\n",
    "\n",
    "# Also save training history\n",
    "history_path = checkpoint_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f, indent=2)\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Next Steps\n",
    "# \n",
    "# Now that the model is trained, you can:\n",
    "# 1. Move to `03_analysis.ipynb` for detailed analysis\n",
    "# 2. Use the trained model for predictions on new data\n",
    "# 3. Fine-tune hyperparameters for better performance\n",
    "# 4. Implement cross-validation for more robust evaluation\n",
    "# \n",
    "# The trained model can be loaded and used for predictions with:\n",
    "# ```python\n",
    "# from mtl_gnn_dta import AffinityPredictor\n",
    "# predictor = AffinityPredictor(model_path='path/to/checkpoint.pt')\n",
    "# predictions = predictor.predict_from_files(protein_path, ligand_path)\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
