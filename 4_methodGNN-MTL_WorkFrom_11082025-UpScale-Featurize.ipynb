{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb42dc8-77b1-4ea2-8a93-849a53a2d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53c109-35fd-4bed-b396-44ba6eab6354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LETTER_TO_NUM = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                       'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                       'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19,\n",
    "                       'N': 2, 'Y': 18, 'M': 12, 'X':20}\n",
    "\n",
    "NUM_TO_LETTER = {v:k for k, v in LETTER_TO_NUM.items()}\n",
    "\n",
    "ATOM_VOCAB = [\n",
    "    'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca',\n",
    "    'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag',\n",
    "    'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni',\n",
    "    'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'unk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22941f1-a773-461a-8368-38041f0d91c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609fcb2-a1c0-4610-bf45-b94a185000b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drug-target binding affinity datasets\n",
    "\"\"\"\n",
    "import math\n",
    "import yaml\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class DTA(data.Dataset):\n",
    "    \"\"\"\n",
    "    Base class for loading drug-target binding affinity datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, df=None, data_list=None, onthefly=False,\n",
    "                prot_featurize_fn=None, drug_featurize_fn=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "            df : pd.DataFrame with columns [`drug`, `protein`, `y`],\n",
    "                where `drug`: drug key, `protein`: protein key, `y`: binding affinity.\n",
    "            data_list : list of dict (same order as df)\n",
    "                if `onthefly` is True, data_list has the PDB coordinates and SMILES strings\n",
    "                    {`drug`: SDF file path, `protein`: coordinates dict (`pdb_data` in `DTATask`), `y`: float}\n",
    "                if `onthefly` is False, data_list has the cached torch_geometric graphs\n",
    "                    {`drug`: `torch_geometric.data.Data`, `protein`: `torch_geometric.data.Data`, `y`: float}\n",
    "                `protein` has attributes:\n",
    "                    -x          alpha carbon coordinates, shape [n_nodes, 3]\n",
    "                    -edge_index edge indices, shape [2, n_edges]\n",
    "                    -seq        sequence converted to int tensor according to `self.letter_to_num`, shape [n_nodes]\n",
    "                    -name       name of the protein structure, string\n",
    "                    -node_s     node scalar features, shape [n_nodes, 6]\n",
    "                    -node_v     node vector features, shape [n_nodes, 3, 3]\n",
    "                    -edge_s     edge scalar features, shape [n_edges, 39]\n",
    "                    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "                    -mask       node mask, `False` for nodes with missing data that are excluded from message passing\n",
    "                    -seq_emb    sequence embedding (ESM1b), shape [n_nodes, 1280]\n",
    "                `drug` has attributes:\n",
    "                    -x          atom coordinates, shape [n_nodes, 3]\n",
    "                    -edge_index edge indices, shape [2, n_edges]\n",
    "                    -node_s     node scalar features, shape [n_nodes, 66]\n",
    "                    -node_v     node vector features, shape [n_nodes, 1, 3]\n",
    "                    -edge_s     edge scalar features, shape [n_edges, 16]\n",
    "                    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "                    -name       name of the drug, string\n",
    "            onthefly : bool\n",
    "                whether to featurize data on the fly or pre-compute\n",
    "            prot_featurize_fn : function\n",
    "                function to featurize a protein.\n",
    "            drug_featurize_fn : function\n",
    "                function to featurize a drug.\n",
    "        \"\"\"\n",
    "        super(DTA, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.data_list = data_list\n",
    "        self.onthefly = onthefly\n",
    "        if onthefly:\n",
    "            assert prot_featurize_fn is not None, 'prot_featurize_fn must be provided'\n",
    "            assert drug_featurize_fn is not None, 'drug_featurize_fn must be provided'\n",
    "        self.prot_featurize_fn = prot_featurize_fn\n",
    "        self.drug_featurize_fn = drug_featurize_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.onthefly:\n",
    "            drug = self.drug_featurize_fn(\n",
    "                self.data_list[idx]['drug'],\n",
    "                name=self.data_list[idx]['drug_name']\n",
    "            )\n",
    "            prot = self.prot_featurize_fn(\n",
    "                self.data_list[idx]['protein'],\n",
    "                name=self.data_list[idx]['protein_name']\n",
    "            )\n",
    "        else:\n",
    "            drug = self.data_list[idx]['drug']\n",
    "            prot = self.data_list[idx]['protein']\n",
    "        y = self.data_list[idx]['y']\n",
    "        item = {'drug': drug, 'protein': prot, 'y': y}\n",
    "        return item\n",
    "\n",
    "\n",
    "def create_fold(df, fold_seed, frac):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds by random splitting.\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test = df.sample(frac = test_frac, replace = False, random_state = fold_seed)\n",
    "    train_val = df[~df.index.isin(test.index)]\n",
    "    val = train_val.sample(frac = val_frac/(1-test_frac), replace = False, random_state = 1)\n",
    "    train = train_val[~train_val.index.isin(val.index)]\n",
    "\n",
    "    return {'train': train.reset_index(drop = True),\n",
    "            'valid': val.reset_index(drop = True),\n",
    "            'test': test.reset_index(drop = True)}\n",
    "\n",
    "\n",
    "def create_fold_setting_cold(df, fold_seed, frac, entity):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds by drug/protein-wise splitting.\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    gene_drop = df[entity].drop_duplicates().sample(frac = test_frac, replace = False, random_state = fold_seed).values\n",
    "\n",
    "    test = df[df[entity].isin(gene_drop)]\n",
    "\n",
    "    train_val = df[~df[entity].isin(gene_drop)]\n",
    "\n",
    "    gene_drop_val = train_val[entity].drop_duplicates().sample(frac = val_frac/(1-test_frac), replace = False, random_state = fold_seed).values\n",
    "    val = train_val[train_val[entity].isin(gene_drop_val)]\n",
    "    train = train_val[~train_val[entity].isin(gene_drop_val)]\n",
    "\n",
    "    return {'train': train.reset_index(drop = True),\n",
    "            'valid': val.reset_index(drop = True),\n",
    "            'test': test.reset_index(drop = True)}\n",
    "\n",
    "\n",
    "def create_full_ood_set(df, fold_seed, frac):\n",
    "    \"\"\"\n",
    "    Create train/valid/test folds such that drugs and proteins are\n",
    "    not overlapped in train and test sets. Train and valid may share\n",
    "    drugs and proteins (random split).\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test_drugs = df['drug'].drop_duplicates().sample(frac=test_frac, replace=False, random_state=fold_seed).values\n",
    "    test_prots = df['protein'].drop_duplicates().sample(frac=test_frac, replace=False, random_state=fold_seed).values\n",
    "\n",
    "    test = df[(df['drug'].isin(test_drugs)) & (df['protein'].isin(test_prots))]\n",
    "    train_val = df[(~df['drug'].isin(test_drugs)) & (~df['protein'].isin(test_prots))]\n",
    "\n",
    "    val = train_val.sample(frac=val_frac/(1-test_frac), replace=False, random_state=fold_seed)\n",
    "    train = train_val[~train_val.index.isin(val.index)]\n",
    "\n",
    "    return {'train': train.reset_index(drop=True),\n",
    "            'valid': val.reset_index(drop=True),\n",
    "            'test': test.reset_index(drop=True)}\n",
    "\n",
    "\n",
    "def create_seq_identity_fold(df, mmseqs_seq_clus_df, fold_seed, frac, min_clus_in_split=5):\n",
    "    \"\"\"\n",
    "    Adapted from: https://github.com/drorlab/atom3d/blob/master/atom3d/splits/sequence.py\n",
    "    Clusters are selected randomly into validation and test sets,\n",
    "    but to ensure that there is some diversity in each set\n",
    "    (i.e. a split does not consist of a single sequence cluster), a minimum number of clusters in each split is enforced.\n",
    "    Some data examples may be removed in order to satisfy this constraint.\n",
    "    \"\"\"\n",
    "    _rng = np.random.RandomState(fold_seed)\n",
    "\n",
    "    def _parse_mmseqs_cluster_res(mmseqs_seq_clus_df):\n",
    "        clus2seq, seq2clus = {}, {}\n",
    "        for rep, sdf in mmseqs_seq_clus_df.groupby('rep'):\n",
    "            for seq in sdf['seq']:\n",
    "                if rep not in clus2seq:\n",
    "                    clus2seq[rep] = []\n",
    "                clus2seq[rep].append(seq)\n",
    "                seq2clus[seq] = rep\n",
    "        return seq2clus, clus2seq\n",
    "\n",
    "    def _create_cluster_split(df, seq2clus, clus2seq, to_use, split_size, min_clus_in_split):\n",
    "        data = df.copy()\n",
    "        all_prot = set(seq2clus.keys())\n",
    "        used = all_prot.difference(to_use)\n",
    "        split = None\n",
    "        while True:\n",
    "            p = _rng.choice(sorted(to_use))\n",
    "            c = seq2clus[p]\n",
    "            members = set(clus2seq[c])\n",
    "            members = members.difference(used)\n",
    "            if len(members) == 0:\n",
    "                continue\n",
    "            # ensure that at least min_fam_in_split families in each split\n",
    "            max_clust_size = int(np.ceil(split_size / min_clus_in_split))\n",
    "            sel_prot = list(members)[:max_clust_size]\n",
    "            sel_df = data[data['protein'].isin(sel_prot)]\n",
    "            split = sel_df if split is None else pd.concat([split, sel_df])\n",
    "            to_use = to_use.difference(members)\n",
    "            used = used.union(members)\n",
    "            if len(split) >= split_size:\n",
    "                break\n",
    "        split = split.reset_index(drop=True)\n",
    "        return split, to_use\n",
    "\n",
    "    seq2clus, clus2seq = _parse_mmseqs_cluster_res(mmseqs_seq_clus_df)\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test_size, val_size = len(df) * test_frac, len(df) * val_frac\n",
    "    to_use = set(seq2clus.keys())\n",
    "\n",
    "    val_df, to_use = _create_cluster_split(df, seq2clus, clus2seq, to_use, val_size, min_clus_in_split)\n",
    "    test_df, to_use = _create_cluster_split(df, seq2clus, clus2seq, to_use, test_size, min_clus_in_split)\n",
    "    train_df = df[df['protein'].isin(to_use)].reset_index(drop=True)\n",
    "    train_df['split'] = 'train'\n",
    "    val_df['split'] = 'valid'\n",
    "    test_df['split'] = 'test'\n",
    "\n",
    "    assert len(set(train_df['protein']) & set(val_df['protein'])) == 0\n",
    "    assert len(set(test_df['protein']) & set(val_df['protein'])) == 0\n",
    "    assert len(set(train_df['protein']) & set(test_df['protein'])) == 0\n",
    "\n",
    "    return {'train': train_df.reset_index(drop=True),\n",
    "            'valid': val_df.reset_index(drop=True),\n",
    "            'test': test_df.reset_index(drop=True)}\n",
    "\n",
    "\n",
    "class DTATask(object):\n",
    "    \"\"\"\n",
    "    Drug-target binding task (e.g., KIBA or Davis).\n",
    "    Three splits: train/valid/test, each split is a DTA() class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            task_name=None,\n",
    "            df=None,\n",
    "            prot_pdb_id=None, pdb_data=None,\n",
    "            emb_dir=None,\n",
    "            drug_sdf_dir=None,\n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_clus_df=None,\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_name: str\n",
    "            Name of the task (e.g., KIBA, Davis, etc.)\n",
    "        df: pd.DataFrame\n",
    "            Dataframe containing the data\n",
    "        prot_pdb_id: dict\n",
    "            Dictionary mapping protein name to PDB ID\n",
    "        pdb_data: dict\n",
    "            A json format of pocket structure data, where key is the PDB ID\n",
    "            and value is the corresponding PDB structure data in a dictionary:\n",
    "                -'name': kinase name\n",
    "                -'UniProt_id': UniProt ID\n",
    "                -'PDB_id': PDB ID,\n",
    "                -'chain': chain ID,\n",
    "                -'seq': pocket sequence,                \n",
    "                -'coords': coordinates of the 'N', 'CA', 'C', 'O' atoms of the pocket residues,\n",
    "                    - \"N\": [[x, y, z], ...]\n",
    "                    - \"CA\": [[], ...],\n",
    "                    - \"C\": [[], ...],\n",
    "                    - \"O\": [[], ...]               \n",
    "            (there are some other keys but only for internal use)\n",
    "        emb_dir: str\n",
    "            Directory containing the protein embeddings\n",
    "        drug_sdf_dir: str\n",
    "            Directory containing the drug SDF files\n",
    "        num_pos_emb: int\n",
    "            Dimension of positional embeddings\n",
    "        num_rbf: int\n",
    "            Number of radial basis functions\n",
    "        contact_cutoff: float\n",
    "            Cutoff distance for defining residue-residue contacts\n",
    "        split_method: str\n",
    "            how to split train/test sets, \n",
    "            -`random`: random split\n",
    "            -`protein`: split by protein\n",
    "            -`drug`: split by drug\n",
    "            -`both`: unseen drugs and proteins in test set\n",
    "            -`seqid`: split by protein sequence identity \n",
    "                (need to priovide the MMseqs2 sequence cluster result,\n",
    "                see `mmseqs_seq_clus_df`)\n",
    "        split_frac: list\n",
    "            Fraction of data in train/valid/test sets\n",
    "        mmseqs_seq_clus_df: pd.DataFrame\n",
    "            Dataframe containing the MMseqs2 sequence cluster result\n",
    "            using a desired sequence identity cutoff\n",
    "        seed: int\n",
    "            Random seed\n",
    "        onthefly: bool\n",
    "            whether to featurize data on the fly or pre-compute\n",
    "        \"\"\"\n",
    "        self.task_name = task_name        \n",
    "        self.prot_pdb_id = prot_pdb_id\n",
    "        self.pdb_data = pdb_data        \n",
    "        self.emb_dir = emb_dir\n",
    "        self.df = df\n",
    "        self.prot_featurize_params = dict(\n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff)        \n",
    "        self.drug_sdf_dir = drug_sdf_dir        \n",
    "        self._prot2pdb = None\n",
    "        self._pdb_graph_db = None        \n",
    "        self._drug2sdf_file = None\n",
    "        self._drug_sdf_db = None\n",
    "        self.split_method = split_method\n",
    "        self.split_frac = split_frac\n",
    "        self.mmseqs_seq_clus_df = mmseqs_seq_clus_df\n",
    "        self.seed = seed\n",
    "        self.onthefly = onthefly\n",
    "\n",
    "    def _format_pdb_entry(self, _data):\n",
    "        _coords = _data[\"coords\"]\n",
    "        entry = {\n",
    "            \"name\": _data[\"name\"],\n",
    "            \"seq\": _data[\"seq\"],\n",
    "            \"coords\": list(zip(_coords[\"N\"], _coords[\"CA\"], _coords[\"C\"], _coords[\"O\"])),\n",
    "        }        \n",
    "        if self.emb_dir is not None:\n",
    "            embed_file = f\"{_data['PDB_id']}.{_data['chain']}.pt\"\n",
    "            entry[\"embed\"] = f\"{self.emb_dir}/{embed_file}\"\n",
    "        return entry\n",
    "\n",
    "    @property\n",
    "    def prot2pdb(self):\n",
    "        if self._prot2pdb is None:\n",
    "            self._prot2pdb = {}\n",
    "            for prot, pdb in self.prot_pdb_id.items():\n",
    "                _pdb_entry = self.pdb_data[pdb]\n",
    "                self._prot2pdb[prot] = self._format_pdb_entry(_pdb_entry)\n",
    "        return self._prot2pdb\n",
    "\n",
    "    @property\n",
    "    def pdb_graph_db(self):\n",
    "        if self._pdb_graph_db is None:\n",
    "            self._pdb_graph_db = pdb_graph.pdb_to_graphs(self.prot2pdb,\n",
    "                self.prot_featurize_params)\n",
    "        return self._pdb_graph_db\n",
    "\n",
    "    @property\n",
    "    def drug2sdf_file(self):\n",
    "        if self._drug2sdf_file is None:            \n",
    "            drug2sdf_file = {f.stem : str(f) for f in Path(self.drug_sdf_dir).glob('*.sdf')}\n",
    "            # Convert str keys to int for Davis\n",
    "            if self.task_name == 'DAVIS' and all([k.isdigit() for k in drug2sdf_file.keys()]):\n",
    "                drug2sdf_file = {int(k) : v for k, v in drug2sdf_file.items()}\n",
    "            self._drug2sdf_file = drug2sdf_file\n",
    "        return self._drug2sdf_file\n",
    "\n",
    "    @property\n",
    "    def drug_sdf_db(self):\n",
    "        if self._drug_sdf_db is None:\n",
    "            self._drug_sdf_db = mol_graph.sdf_to_graphs(self.drug2sdf_file)\n",
    "        return self._drug_sdf_db\n",
    "\n",
    "\n",
    "    def build_data(self, df, onthefly=False):\n",
    "        records = df.to_dict('records')\n",
    "        data_list = []\n",
    "        for entry in records:\n",
    "            drug = entry['drug']\n",
    "            prot = entry['protein']\n",
    "            if onthefly:\n",
    "                pf = self.prot2pdb[prot]\n",
    "                df = self.drug2sdf_file[drug]\n",
    "            else:                \n",
    "                pf = self.pdb_graph_db[prot]                \n",
    "                df = self.drug_sdf_db[drug]\n",
    "            data_list.append({'drug': df, 'protein': pf, 'y': entry['y'],\n",
    "                'drug_name': drug, 'protein_name': prot})\n",
    "        if onthefly:\n",
    "            prot_featurize_fn = partial(\n",
    "                pdb_graph.featurize_protein_graph,\n",
    "                **self.prot_featurize_params)            \n",
    "            drug_featurize_fn = mol_graph.featurize_drug\n",
    "        else:\n",
    "            prot_featurize_fn, drug_featurize_fn = None, None\n",
    "        data = DTA(df=df, data_list=data_list, onthefly=onthefly,\n",
    "            prot_featurize_fn=prot_featurize_fn, drug_featurize_fn=drug_featurize_fn)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_split(self, df=None, split_method=None,\n",
    "            split_frac=None, seed=None, onthefly=None,\n",
    "            return_df=False):\n",
    "        df = df or self.df\n",
    "        split_method = split_method or self.split_method\n",
    "        split_frac = split_frac or self.split_frac\n",
    "        seed = seed or self.seed\n",
    "        onthefly = onthefly or self.onthefly\n",
    "        if split_method == 'random':\n",
    "            split_df = create_fold(self.df, seed, split_frac)\n",
    "        elif split_method == 'drug':\n",
    "            split_df = create_fold_setting_cold(self.df, seed, split_frac, 'drug')\n",
    "        elif split_method == 'protein':\n",
    "            split_df = create_fold_setting_cold(self.df, seed, split_frac, 'protein')\n",
    "        elif split_method == 'both':\n",
    "            split_df = create_full_ood_set(self.df, seed, split_frac)\n",
    "        elif split_method == 'seqid':\n",
    "            split_df = create_seq_identity_fold(\n",
    "                self.df, self.mmseqs_seq_clus_df, seed, split_frac)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown split method: {}\".format(split_method))\n",
    "        split_data = {}\n",
    "        for split, df in split_df.items():\n",
    "            split_data[split] = self.build_data(df, onthefly=onthefly)\n",
    "        if return_df:\n",
    "            return split_data, split_df\n",
    "        else:\n",
    "            return split_data\n",
    "\n",
    "\n",
    "class KIBA(DTATask):\n",
    "    \"\"\"\n",
    "    KIBA drug-target interaction dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            data_path='../data/KIBA/kiba_data.tsv',            \n",
    "            pdb_map='../data/KIBA/kiba_uniprot2pdb.yaml',\n",
    "            pdb_json='../data/structure/pockets_structure.json',                        \n",
    "            emb_dir='../data/esm1b',           \n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,            \n",
    "            drug_sdf_dir='../data/structure/kiba_mol3d_sdf',\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_cluster_file='../data/KIBA/kiba_cluster_id50_cluster.tsv',\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        df = pd.read_table(data_path)        \n",
    "        prot_pdb_id = yaml.safe_load(open(pdb_map, 'r'))\n",
    "        pdb_data = json.load(open(pdb_json, 'r'))                \n",
    "        mmseqs_seq_clus_df = pd.read_table(mmseqs_seq_cluster_file, names=['rep', 'seq'])\n",
    "        super(KIBA, self).__init__(\n",
    "            task_name='KIBA',\n",
    "            df=df, \n",
    "            prot_pdb_id=prot_pdb_id, pdb_data=pdb_data,\n",
    "            emb_dir=emb_dir,            \n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "            drug_sdf_dir=drug_sdf_dir,\n",
    "            split_method=split_method, split_frac=split_frac,\n",
    "            mmseqs_seq_clus_df=mmseqs_seq_clus_df,\n",
    "            seed=seed, onthefly=onthefly\n",
    "            )\n",
    "\n",
    "\n",
    "class DAVIS(DTATask):\n",
    "    \"\"\"\n",
    "    DAVIS drug-target interaction dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            data_path='../data/DAVIS/davis_data.tsv',            \n",
    "            pdb_map='../data/DAVIS/davis_protein2pdb.yaml',\n",
    "            pdb_json='../data/structure/pockets_structure.json',                        \n",
    "            emb_dir='../data/esm1b',           \n",
    "            num_pos_emb=16, num_rbf=16,\n",
    "            contact_cutoff=8.,            \n",
    "            drug_sdf_dir='../data/structure/davis_mol3d_sdf',\n",
    "            split_method='random', split_frac=[0.7, 0.1, 0.2],\n",
    "            mmseqs_seq_cluster_file='../data/DAVIS/davis_cluster_id50_cluster.tsv',\n",
    "            seed=42, onthefly=False\n",
    "        ):\n",
    "        df = pd.read_table(data_path)        \n",
    "        prot_pdb_id = yaml.safe_load(open(pdb_map, 'r'))\n",
    "        pdb_data = json.load(open(pdb_json, 'r'))        \n",
    "        mmseqs_seq_clus_df = pd.read_table(mmseqs_seq_cluster_file, names=['rep', 'seq'])\n",
    "        super(DAVIS, self).__init__(\n",
    "            task_name='DAVIS',\n",
    "            df=df, \n",
    "            prot_pdb_id=prot_pdb_id, pdb_data=pdb_data,\n",
    "            emb_dir=emb_dir,            \n",
    "            num_pos_emb=num_pos_emb, num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "            drug_sdf_dir=drug_sdf_dir,\n",
    "            split_method=split_method, split_frac=split_frac,\n",
    "            mmseqs_seq_clus_df=mmseqs_seq_clus_df,\n",
    "            seed=seed, onthefly=onthefly\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c80511-2ef1-47a7-a6e2-21236a2b6262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3931f-b6b7-4e9f-8ba0-f74b5c65bb8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "\n",
    "def _parallel_train_per_epoch(\n",
    "    kwargs=None, test_loader=None,\n",
    "    n_epochs=None, eval_freq=None, test_freq=None,\n",
    "    monitoring_score='pearson',\n",
    "    loss_fn=None, logger=None,\n",
    "    test_after_train=True,\n",
    "):\n",
    "    midx = kwargs['midx']\n",
    "    model = kwargs['model']\n",
    "    optimizer = kwargs['optimizer']\n",
    "    train_loader = kwargs['train_loader']\n",
    "    valid_loader = kwargs['valid_loader']\n",
    "    device = kwargs['device']\n",
    "    stopper = kwargs['stopper']\n",
    "    best_model_state_dict = kwargs['best_model_state_dict']\n",
    "    if stopper.early_stop:\n",
    "        return kwargs\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yh = model(xd, xp)\n",
    "            loss = loss_fn(yh, y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / step\n",
    "        if epoch % eval_freq == 0:\n",
    "            val_results = _parallel_test(\n",
    "                {'model': model, 'midx': midx, 'test_loader': valid_loader, 'device': device},\n",
    "                loss_fn=loss_fn, logger=logger\n",
    "            )\n",
    "            is_best = stopper.update(val_results['metrics'][monitoring_score])\n",
    "            if is_best:\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            logger.info(f\"M-{midx} E-{epoch} | Train Loss: {train_loss:.4f} | Valid Loss: {val_results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in val_results['metrics'].items()])\n",
    "                + f\" | best {monitoring_score}: {stopper.best_score:.4f}\"\n",
    "                )\n",
    "        if test_freq is not None and epoch % test_freq == 0:\n",
    "            test_results = _parallel_test(\n",
    "                {'midx': midx, 'model': model, 'test_loader': test_loader, 'device': device},\n",
    "                loss_fn=loss_fn, logger=logger\n",
    "            )\n",
    "            logger.info(f\"M-{midx} E-{epoch} | Test Loss: {test_results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in test_results['metrics'].items()])\n",
    "                )\n",
    "\n",
    "        if stopper.early_stop:\n",
    "            logger.info('Eearly stop at epoch {}'.format(epoch))\n",
    "\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "    if test_after_train:\n",
    "        test_results = _parallel_test(\n",
    "            {'midx': midx, 'model': model, 'test_loader': test_loader, 'device': device},\n",
    "            loss_fn=loss_fn,\n",
    "            test_tag=f\"Model {midx}\", print_log=True, logger=logger\n",
    "        )\n",
    "    rets = dict(midx = midx, model = model)\n",
    "    return rets\n",
    "\n",
    "\n",
    "def _parallel_test(\n",
    "    kwargs=None, loss_fn=None, \n",
    "    test_tag=None, print_log=False, logger=None,\n",
    "):\n",
    "    midx = kwargs['midx']\n",
    "    model = kwargs['model']\n",
    "    test_loader = kwargs['test_loader']\n",
    "    device = kwargs['device']\n",
    "    model.eval()\n",
    "    yt, yp, total_loss = torch.Tensor(), torch.Tensor(), 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_loader, start=1):\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            yh = model(xd, xp)\n",
    "            loss = loss_fn(yh, y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "            yp = torch.cat([yp, yh.detach().cpu()], dim=0)\n",
    "            yt = torch.cat([yt, y.detach().cpu()], dim=0)\n",
    "    yt = yt.numpy()\n",
    "    yp = yp.view(-1).numpy()\n",
    "    results = {\n",
    "        'midx': midx,\n",
    "        'y_true': yt,\n",
    "        'y_pred': yp,\n",
    "        'loss': total_loss / step,\n",
    "    }\n",
    "    eval_metrics = evaluation_metrics(\n",
    "        yt, yp,\n",
    "        eval_metrics=['mse', 'spearman', 'pearson']\n",
    "    )\n",
    "    results['metrics'] = eval_metrics\n",
    "    if print_log:\n",
    "        logger.info(f\"{test_tag} | Test Loss: {results['loss']:.4f} | \"\\\n",
    "            + ' | '.join([f'{k}: {v:.4f}' for k, v in results['metrics'].items()]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def _unpack_evidential_output(output):\n",
    "    mu, v, alpha, beta = torch.split(output, output.shape[1]//4, dim=1)\n",
    "    inverse_evidence = 1. / ((alpha - 1) * v)\n",
    "    var = beta * inverse_evidence\n",
    "    return mu, var, inverse_evidence\n",
    "\n",
    "\n",
    "class DTAExperiment(object):\n",
    "    def __init__(self,\n",
    "        task=None,\n",
    "        split_method='protein',\n",
    "        split_frac=[0.7, 0.1, 0.2],\n",
    "        prot_gcn_dims=[128, 128, 128], prot_gcn_bn=False,\n",
    "        prot_fc_dims=[1024, 128],\n",
    "        drug_in_dim=66, drug_fc_dims=[1024, 128], drug_gcn_dims=[128, 64],\n",
    "        mlp_dims=[1024, 512], mlp_dropout=0.25,\n",
    "        num_pos_emb=16, num_rbf=16,\n",
    "        contact_cutoff=8.,\n",
    "        n_ensembles=1, n_epochs=500, batch_size=256,\n",
    "        lr=0.001,        \n",
    "        seed=42, onthefly=False,\n",
    "        uncertainty=False, parallel=False,\n",
    "        output_dir='../output', save_log=False\n",
    "    ):\n",
    "        self.saver = Saver(output_dir)\n",
    "        self.logger = Logger(logfile=self.saver.save_dir/'exp.log' if save_log else None)\n",
    "\n",
    "        self.uncertainty = uncertainty\n",
    "        self.parallel = parallel\n",
    "        self.n_ensembles = n_ensembles\n",
    "        if self.uncertainty and self.n_ensembles < 2:\n",
    "            raise ValueError('n_ensembles must be greater than 1 when uncertainty is True')            \n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        dataset_klass = {\n",
    "            'kiba': KIBA,\n",
    "            'davis': DAVIS,\n",
    "        }[task]\n",
    "\n",
    "        self.dataset = dataset_klass(\n",
    "            split_method=split_method,\n",
    "            split_frac=split_frac,\n",
    "            seed=seed,\n",
    "            onthefly=onthefly,\n",
    "            num_pos_emb=num_pos_emb,\n",
    "            num_rbf=num_rbf,\n",
    "            contact_cutoff=contact_cutoff,\n",
    "        )\n",
    "        self._task_data_df_split = None\n",
    "        self._task_loader = None\n",
    "\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        if self.parallel and n_gpus < self.n_ensembles:\n",
    "            self.logger.warning(f\"Visible GPUs ({n_gpus}) is fewer than \"\n",
    "            f\"number of models ({self.n_ensembles}). Some models will be run on the same GPU\"\n",
    "            )\n",
    "        self.devices = [torch.device(f'cuda:{i % n_gpus}')\n",
    "            for i in range(self.n_ensembles)]\n",
    "        self.model_config = dict(\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=prot_gcn_dims,            \n",
    "            prot_fc_dims=prot_fc_dims,\n",
    "            drug_node_in_dim=[66, 1], \n",
    "            drug_node_h_dims=drug_gcn_dims,\n",
    "            drug_fc_dims=drug_fc_dims,            \n",
    "            mlp_dims=mlp_dims, mlp_dropout=mlp_dropout)\n",
    "        self.build_model()\n",
    "        self.criterion = F.mse_loss\n",
    "\n",
    "        self.split_method = split_method\n",
    "        self.split_frac = split_frac\n",
    "\n",
    "        self.logger.info(self.models[0])\n",
    "        self.logger.info(self.optimizers[0])\n",
    "\n",
    "    def build_model(self):\n",
    "        self.models = [DTAModel(**self.model_config).to(self.devices[i])\n",
    "                        for i in range(self.n_ensembles)]\n",
    "        self.optimizers = [optim.Adam(model.parameters(), lr=self.lr) for model in self.models]\n",
    "\n",
    "    def _get_data_loader(self, dataset, shuffle=False):\n",
    "        return torch_geometric.loader.DataLoader(\n",
    "                    dataset=dataset,\n",
    "                    batch_size=self.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    pin_memory=False,\n",
    "                    num_workers=0,\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def task_data_df_split(self):\n",
    "        if self._task_data_df_split is None:\n",
    "            (data, df) = self.dataset.get_split(return_df=True)\n",
    "            self._task_data_df_split = (data, df)\n",
    "        return self._task_data_df_split\n",
    "\n",
    "    @property\n",
    "    def task_data(self):\n",
    "        return self.task_data_df_split[0]\n",
    "\n",
    "    @property\n",
    "    def task_df(self):\n",
    "        return self.task_data_df_split[1]\n",
    "\n",
    "    @property\n",
    "    def task_loader(self):\n",
    "        if self._task_loader is None:\n",
    "            _loader = {\n",
    "                s: self._get_data_loader(\n",
    "                    self.task_data[s], shuffle=(s == 'train'))\n",
    "                for s in self.task_data\n",
    "            }\n",
    "            self._task_loader = _loader\n",
    "        return self._task_loader\n",
    "\n",
    "    def recalibrate_std(self, df, recalib_df):\n",
    "        y_mean = recalib_df['y_pred'].values\n",
    "        y_std = recalib_df['y_std'].values\n",
    "        y_true = recalib_df['y_true'].values\n",
    "        std_ratio = uct.recalibration.optimize_recalibration_ratio(\n",
    "            y_mean, y_std, y_true, criterion=\"miscal\")\n",
    "        df['y_std_recalib'] = df['y_std'] * std_ratio\n",
    "        return df\n",
    "\n",
    "    def _format_predict_df(self, results,\n",
    "            test_df=None, esb_yp=None, recalib_df=None):\n",
    "        \"\"\"\n",
    "        results: dict with keys y_pred, y_true, y_var\n",
    "        \"\"\"\n",
    "        df = self.task_df['test'].copy() if test_df is None else test_df.copy()\n",
    "        assert np.allclose(results['y_true'], df['y'].values)\n",
    "        df = df.rename(columns={'y': 'y_true'})\n",
    "        df['y_pred'] = results['y_pred']\n",
    "        if esb_yp is not None:\n",
    "            if self.uncertainty:\n",
    "                df['y_std'] = np.std(esb_yp, axis=0)\n",
    "                if recalib_df is not None:\n",
    "                    df = self.recalibrate_std(df, recalib_df)\n",
    "            for i in range(self.n_ensembles):\n",
    "                df[f'y_pred_{i + 1}'] = esb_yp[i]\n",
    "        return df\n",
    "\n",
    "    def train(self, n_epochs=None, patience=None,\n",
    "                eval_freq=1, test_freq=None,\n",
    "                monitoring_score='pearson',\n",
    "                train_data=None, valid_data=None,                \n",
    "                rebuild_model=False,\n",
    "                test_after_train=False):\n",
    "        n_epochs = n_epochs or self.n_epochs\n",
    "        if rebuild_model:\n",
    "            self.build_model()\n",
    "        tl, vl = self.task_loader['train'], self.task_loader['valid']\n",
    "        rets_list = []\n",
    "        for i in range(self.n_ensembles):\n",
    "            stp = EarlyStopping(eval_freq=eval_freq, patience=patience,\n",
    "                                    higher_better=(monitoring_score != 'mse'))\n",
    "            rets = dict(\n",
    "                midx = i + 1,\n",
    "                model = self.models[i],\n",
    "                optimizer = self.optimizers[i],\n",
    "                device = self.devices[i],\n",
    "                train_loader = tl,\n",
    "                valid_loader = vl,\n",
    "                stopper = stp,\n",
    "                best_model_state_dict = None,\n",
    "            )\n",
    "            rets_list.append(rets)\n",
    "\n",
    "        rets_list = Parallel(n_jobs=(self.n_ensembles if self.parallel else 1), prefer=\"threads\")(\n",
    "            delayed(_parallel_train_per_epoch)(\n",
    "                kwargs=rets_list[i],\n",
    "                test_loader=self.task_loader['test'],\n",
    "                n_epochs=n_epochs, eval_freq=eval_freq, test_freq=test_freq,\n",
    "                monitoring_score=monitoring_score,\n",
    "                loss_fn=self.criterion, logger=self.logger,\n",
    "                test_after_train=test_after_train,\n",
    "            ) for i in range(self.n_ensembles))\n",
    "\n",
    "        for i, rets in enumerate(rets_list):\n",
    "            self.models[rets['midx'] - 1] = rets['model']\n",
    "\n",
    "\n",
    "    def test(self, test_model=None, test_loader=None,\n",
    "                test_data=None, test_df=None,\n",
    "                recalib_df=None,\n",
    "                save_prediction=False, save_df_name='prediction.tsv',\n",
    "                test_tag=None, print_log=False):\n",
    "        test_models = self.models if test_model is None else [test_model]\n",
    "        if test_data is not None:\n",
    "            assert test_df is not None, 'test_df must be provided if test_data used'\n",
    "            test_loader = self._get_data_loader(test_data)\n",
    "        elif test_loader is not None:\n",
    "            assert test_df is not None, 'test_df must be provided if test_loader used'\n",
    "        else:\n",
    "            test_loader = self.task_loader['test']\n",
    "        rets_list = []\n",
    "        for i, model in enumerate(test_models):\n",
    "            rets = _parallel_test(\n",
    "                kwargs={\n",
    "                    'midx': i + 1,\n",
    "                    'model': model,\n",
    "                    'test_loader': test_loader,\n",
    "                    'device': self.devices[i],\n",
    "                },\n",
    "                loss_fn=self.criterion,\n",
    "                test_tag=f\"Model {i+1}\", print_log=True, logger=self.logger\n",
    "            )\n",
    "            rets_list.append(rets)\n",
    "\n",
    "\n",
    "        esb_yp, esb_loss = None, 0\n",
    "        for rets in rets_list:\n",
    "            esb_yp = rets['y_pred'].reshape(1, -1) if esb_yp is None else\\\n",
    "                np.vstack((esb_yp, rets['y_pred'].reshape(1, -1)))\n",
    "            esb_loss += rets['loss']\n",
    "\n",
    "        y_true = rets['y_true']\n",
    "        y_pred = np.mean(esb_yp, axis=0)\n",
    "        esb_loss /= len(test_models)\n",
    "        results = {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "            'loss': esb_loss,\n",
    "        }\n",
    "\n",
    "        eval_metrics = evaluation_metrics(\n",
    "            y_true, y_pred,\n",
    "            eval_metrics=['mse', 'spearman', 'pearson']\n",
    "        )\n",
    "        results['metrics'] = eval_metrics\n",
    "        results['df'] = self._format_predict_df(results,\n",
    "            test_df=test_df, esb_yp=esb_yp, recalib_df=recalib_df)\n",
    "        if save_prediction:\n",
    "            self.saver.save_df(results['df'], save_df_name, float_format='%g')\n",
    "        if print_log:\n",
    "            self.logger.info(f\"{test_tag} | Test Loss: {results['loss']:.4f} | \"\\\n",
    "                + ' | '.join([f'{k}: {v:.4f}' for k, v in results['metrics'].items()]))\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec765d-8b35-48a5-9f33-860a70b44aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d227b1-aeb1-4b03-8792-e3f8fb043952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Geometric Vector Perceptrons\n",
    "From: https://github.com/drorlab/gvp-pytorch/blob/82af6b22eaf8311c15733117b0071408d24ed877/gvp/__init__.py\n",
    "\"\"\"\n",
    "import torch, functools\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "def tuple_sum(*args):\n",
    "    '''\n",
    "    Sums any number of tuples (s, V) elementwise.\n",
    "    '''\n",
    "    return tuple(map(sum, zip(*args)))\n",
    "\n",
    "def tuple_cat(*args, dim=-1):\n",
    "    '''\n",
    "    Concatenates any number of tuples (s, V) elementwise.\n",
    "    \n",
    "    :param dim: dimension along which to concatenate when viewed\n",
    "                as the `dim` index for the scalar-channel tensors.\n",
    "                This means that `dim=-1` will be applied as\n",
    "                `dim=-2` for the vector-channel tensors.\n",
    "    '''\n",
    "    dim %= len(args[0][0].shape)\n",
    "    s_args, v_args = list(zip(*args))\n",
    "    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n",
    "\n",
    "def tuple_index(x, idx):\n",
    "    '''\n",
    "    Indexes into a tuple (s, V) along the first dimension.\n",
    "    \n",
    "    :param idx: any object which can be used to index into a `torch.Tensor`\n",
    "    '''\n",
    "    return x[0][idx], x[1][idx]\n",
    "\n",
    "def randn(n, dims, device=\"cpu\"):\n",
    "    '''\n",
    "    Returns random tuples (s, V) drawn elementwise from a normal distribution.\n",
    "    \n",
    "    :param n: number of data points\n",
    "    :param dims: tuple of dimensions (n_scalar, n_vector)\n",
    "    \n",
    "    :return: (s, V) with s.shape = (n, n_scalar) and\n",
    "             V.shape = (n, n_vector, 3)\n",
    "    '''\n",
    "    return torch.randn(n, dims[0], device=device), \\\n",
    "            torch.randn(n, dims[1], 3, device=device)\n",
    "\n",
    "def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n",
    "    '''\n",
    "    L2 norm of tensor clamped above a minimum value `eps`.\n",
    "    \n",
    "    :param sqrt: if `False`, returns the square of the L2 norm\n",
    "    '''\n",
    "    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n",
    "    return torch.sqrt(out) if sqrt else out\n",
    "\n",
    "def _split(x, nv):\n",
    "    '''\n",
    "    Splits a merged representation of (s, V) back into a tuple. \n",
    "    Should be used only with `_merge(s, V)` and only if the tuple \n",
    "    representation cannot be used.\n",
    "    \n",
    "    :param x: the `torch.Tensor` returned from `_merge`\n",
    "    :param nv: the number of vector channels in the input to `_merge`\n",
    "    '''\n",
    "    v = torch.reshape(x[..., -3*nv:], x.shape[:-1] + (nv, 3))\n",
    "    s = x[..., :-3*nv]\n",
    "    return s, v\n",
    "\n",
    "def _merge(s, v):\n",
    "    '''\n",
    "    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n",
    "    vector channels are flattened and appended to the scalar channels.\n",
    "    Should be used only if the tuple representation cannot be used.\n",
    "    Use `_split(x, nv)` to reverse.\n",
    "    '''\n",
    "    v = torch.reshape(v, v.shape[:-2] + (3*v.shape[-2],))\n",
    "    return torch.cat([s, v], -1)\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    '''\n",
    "    Geometric Vector Perceptron. See manuscript and README.md\n",
    "    for more details.\n",
    "    \n",
    "    :param in_dims: tuple (n_scalar, n_vector)\n",
    "    :param out_dims: tuple (n_scalar, n_vector)\n",
    "    :param h_dim: intermediate number of vector channels, optional\n",
    "    :param activations: tuple of functions (scalar_act, vector_act)\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, h_dim=None,\n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVP, self).__init__()\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.vector_gate = vector_gate\n",
    "        if self.vi: \n",
    "            self.h_dim = h_dim or max(self.vi, self.vo) \n",
    "            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n",
    "            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n",
    "            if self.vo:\n",
    "                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n",
    "                if self.vector_gate: self.wsv = nn.Linear(self.so, self.vo)\n",
    "        else:\n",
    "            self.ws = nn.Linear(self.si, self.so)\n",
    "        \n",
    "        self.scalar_act, self.vector_act = activations\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`, \n",
    "                  or (if vectors_in is 0), a single `torch.Tensor`\n",
    "        :return: tuple (s, V) of `torch.Tensor`,\n",
    "                 or (if vectors_out is 0), a single `torch.Tensor`\n",
    "        '''\n",
    "        if self.vi:\n",
    "            s, v = x\n",
    "            v = torch.transpose(v, -1, -2)\n",
    "            vh = self.wh(v)    \n",
    "            vn = _norm_no_nan(vh, axis=-2)\n",
    "            s = self.ws(torch.cat([s, vn], -1))\n",
    "            if self.vo: \n",
    "                v = self.wv(vh) \n",
    "                v = torch.transpose(v, -1, -2)\n",
    "                if self.vector_gate: \n",
    "                    if self.vector_act:\n",
    "                        gate = self.wsv(self.vector_act(s))\n",
    "                    else:\n",
    "                        gate = self.wsv(s)\n",
    "                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n",
    "                elif self.vector_act:\n",
    "                    v = v * self.vector_act(\n",
    "                        _norm_no_nan(v, axis=-1, keepdims=True))\n",
    "        else:\n",
    "            s = self.ws(x)\n",
    "            if self.vo:\n",
    "                v = torch.zeros(s.shape[0], self.vo, 3,\n",
    "                                device=self.dummy_param.device)\n",
    "        if self.scalar_act:\n",
    "            s = self.scalar_act(s)\n",
    "        \n",
    "        return (s, v) if self.vo else s\n",
    "\n",
    "class _VDropout(nn.Module):\n",
    "    '''\n",
    "    Vector channel dropout where the elements of each\n",
    "    vector channel are dropped together.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(_VDropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: `torch.Tensor` corresponding to vector channels\n",
    "        '''\n",
    "        device = self.dummy_param.device\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = torch.bernoulli(\n",
    "            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n",
    "        ).unsqueeze(-1)\n",
    "        x = mask * x / (1 - self.drop_rate)\n",
    "        return x\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    '''\n",
    "    Combined dropout for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.sdropout = nn.Dropout(drop_rate)\n",
    "        self.vdropout = _VDropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if type(x) is torch.Tensor:\n",
    "            return self.sdropout(x)\n",
    "        s, v = x\n",
    "        return self.sdropout(s), self.vdropout(v)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Combined LayerNorm for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, dims):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.s, self.v = dims\n",
    "        self.scalar_norm = nn.LayerNorm(self.s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if not self.v:\n",
    "            return self.scalar_norm(x)\n",
    "        s, v = x\n",
    "        vn = _norm_no_nan(v, axis=-1, keepdims=True, sqrt=False)\n",
    "        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True))\n",
    "        return self.scalar_norm(s), v / vn\n",
    "\n",
    "class GVPConv(MessagePassing):\n",
    "    '''\n",
    "    Graph convolution / message passing with Geometric Vector Perceptrons.\n",
    "    Takes in a graph with node and edge embeddings,\n",
    "    and returns new node embeddings.\n",
    "    \n",
    "    This does NOT do residual updates and pointwise feedforward layers\n",
    "    ---see `GVPConvLayer`.\n",
    "    \n",
    "    :param in_dims: input node embedding dimensions (n_scalar, n_vector)\n",
    "    :param out_dims: output node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_layers: number of GVPs in the message function\n",
    "    :param module_list: preconstructed message function, overrides n_layers\n",
    "    :param aggr: should be \"add\" if some incoming edges are masked, as in\n",
    "                 a masked autoregressive decoder architecture, otherwise \"mean\"\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, edge_dims,\n",
    "                 n_layers=3, module_list=None, aggr=\"mean\", \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVPConv, self).__init__(aggr=aggr)\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.se, self.ve = edge_dims\n",
    "        \n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        \n",
    "        module_list = module_list or []\n",
    "        if not module_list:\n",
    "            if n_layers == 1:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), \n",
    "                        (self.so, self.vo), activations=(None, None)))\n",
    "            else:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), out_dims)\n",
    "                )\n",
    "                for i in range(n_layers - 2):\n",
    "                    module_list.append(GVP_(out_dims, out_dims))\n",
    "                module_list.append(GVP_(out_dims, out_dims,\n",
    "                                       activations=(None, None)))\n",
    "        self.message_func = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        '''\n",
    "        x_s, x_v = x\n",
    "        message = self.propagate(edge_index, \n",
    "                    s=x_s, v=x_v.reshape(x_v.shape[0], 3*x_v.shape[1]),\n",
    "                    edge_attr=edge_attr)\n",
    "        return _split(message, self.vo) \n",
    "\n",
    "    def message(self, s_i, v_i, s_j, v_j, edge_attr):\n",
    "        v_j = v_j.view(v_j.shape[0], v_j.shape[1]//3, 3)\n",
    "        v_i = v_i.view(v_i.shape[0], v_i.shape[1]//3, 3)\n",
    "        message = tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n",
    "        message = self.message_func(message)\n",
    "        return _merge(*message)\n",
    "\n",
    "\n",
    "class GVPConvLayer(nn.Module):\n",
    "    '''\n",
    "    Full graph convolution / message passing layer with \n",
    "    Geometric Vector Perceptrons. Residually updates node embeddings with\n",
    "    aggregated incoming messages, applies a pointwise feedforward \n",
    "    network to node embeddings, and returns updated node embeddings.\n",
    "    \n",
    "    To only compute the aggregated messages, see `GVPConv`.\n",
    "    \n",
    "    :param node_dims: node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_message: number of GVPs to use in message function\n",
    "    :param n_feedforward: number of GVPs to use in feedforward function\n",
    "    :param drop_rate: drop probability in all dropout layers\n",
    "    :param autoregressive: if `True`, this `GVPConvLayer` will be used\n",
    "           with a different set of input node embeddings for messages\n",
    "           where src >= dst\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, node_dims, edge_dims,\n",
    "                 n_message=3, n_feedforward=2, drop_rate=.1,\n",
    "                 autoregressive=False, \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        \n",
    "        super(GVPConvLayer, self).__init__()\n",
    "        self.conv = GVPConv(node_dims, node_dims, edge_dims, n_message,\n",
    "                           aggr=\"add\" if autoregressive else \"mean\",\n",
    "                           activations=activations, vector_gate=vector_gate)\n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        self.norm = nn.ModuleList([LayerNorm(node_dims) for _ in range(2)])\n",
    "        self.dropout = nn.ModuleList([Dropout(drop_rate) for _ in range(2)])\n",
    "\n",
    "        ff_func = []\n",
    "        if n_feedforward == 1:\n",
    "            ff_func.append(GVP_(node_dims, node_dims, activations=(None, None)))\n",
    "        else:\n",
    "            hid_dims = 4*node_dims[0], 2*node_dims[1]\n",
    "            ff_func.append(GVP_(node_dims, hid_dims))\n",
    "            for i in range(n_feedforward-2):\n",
    "                ff_func.append(GVP_(hid_dims, hid_dims))\n",
    "            ff_func.append(GVP_(hid_dims, node_dims, activations=(None, None)))\n",
    "        self.ff_func = nn.Sequential(*ff_func)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr,\n",
    "                autoregressive_x=None, node_mask=None):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. \n",
    "                If not `None`, will be used as src node embeddings\n",
    "                for forming messages where src >= dst. The corrent node \n",
    "                embeddings `x` will still be the base of the update and the \n",
    "                pointwise feedforward.\n",
    "        :param node_mask: array of type `bool` to index into the first\n",
    "                dim of node embeddings (s, V). If not `None`, only\n",
    "                these nodes will be updated.\n",
    "        '''\n",
    "        \n",
    "        if autoregressive_x is not None:\n",
    "            src, dst = edge_index\n",
    "            mask = src < dst\n",
    "            edge_index_forward = edge_index[:, mask]\n",
    "            edge_index_backward = edge_index[:, ~mask]\n",
    "            edge_attr_forward = tuple_index(edge_attr, mask)\n",
    "            edge_attr_backward = tuple_index(edge_attr, ~mask)\n",
    "            \n",
    "            dh = tuple_sum(\n",
    "                self.conv(x, edge_index_forward, edge_attr_forward),\n",
    "                self.conv(autoregressive_x, edge_index_backward, edge_attr_backward)\n",
    "            )\n",
    "            \n",
    "            count = scatter_add(torch.ones_like(dst), dst,\n",
    "                        dim_size=dh[0].size(0)).clamp(min=1).unsqueeze(-1)\n",
    "            \n",
    "            dh = dh[0] / count, dh[1] / count.unsqueeze(-1)\n",
    "\n",
    "        else:\n",
    "            dh = self.conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_ = x\n",
    "            x, dh = tuple_index(x, node_mask), tuple_index(dh, node_mask)\n",
    "            \n",
    "        x = self.norm[0](tuple_sum(x, self.dropout[0](dh)))\n",
    "        \n",
    "        dh = self.ff_func(x)\n",
    "        x = self.norm[1](tuple_sum(x, self.dropout[1](dh)))\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_[0][node_mask], x_[1][node_mask] = x[0], x[1]\n",
    "            x = x_\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d98cc-d70d-403d-a4c7-d42853242cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7ae00-8629-4b44-96cb-86a98b4a89a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def eval_mse(y_true, y_pred, squared=True):\n",
    "    \"\"\"Evaluate mse/rmse and return the results.\n",
    "    squared: bool, default=True\n",
    "        If True returns MSE value, if False returns RMSE value.\n",
    "    \"\"\"\n",
    "    return metrics.mean_squared_error(y_true, y_pred, squared=squared)\n",
    "\n",
    "def eval_pearson(y_true, y_pred):\n",
    "    \"\"\"Evaluate Pearson correlation and return the results.\"\"\"\n",
    "    return stats.pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def eval_spearman(y_true, y_pred):\n",
    "    \"\"\"Evaluate Spearman correlation and return the results.\"\"\"\n",
    "    return stats.spearmanr(y_true, y_pred)[0]\n",
    "\n",
    "def eval_r2(y_true, y_pred):\n",
    "    \"\"\"Evaluate R2 and return the results.\"\"\"\n",
    "    return metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "def eval_auroc(y_true, y_pred):\n",
    "    \"\"\"Evaluate AUROC and return the results.\"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_pred)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def eval_auprc(y_true, y_pred):\n",
    "    \"\"\"Evaluate AUPRC and return the results.\"\"\"\n",
    "    pre, rec, _ = metrics.precision_recall_curve(y_true, y_pred)\n",
    "    return metrics.auc(rec, pre)\n",
    "\n",
    "\n",
    "def evaluation_metrics(y_true=None, y_pred=None,\n",
    "\t\teval_metrics=[]):\n",
    "    \"\"\"Evaluate eval_metrics and return the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: true labels\n",
    "    y_pred: predicted labels\n",
    "    eval_metrics: a list of evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for m in eval_metrics:\n",
    "        if m == 'mse':\n",
    "            s = eval_mse(y_true, y_pred, squared=True)\n",
    "        elif m == 'rmse':\n",
    "            s = eval_mse(y_true, y_pred, squared=False)\n",
    "        elif m == 'pearson':\n",
    "            s = eval_pearson(y_true, y_pred)\n",
    "        elif m == 'spearman':\n",
    "            s = eval_spearman(y_true, y_pred)\n",
    "        elif m == 'r2':\n",
    "            s = eval_r2(y_true, y_pred)\n",
    "        elif m == 'auroc':\n",
    "            s = eval_auroc(y_true, y_pred)\n",
    "        elif m == 'auprc':\n",
    "            s = eval_auprc(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError('Unknown evaluation metric: {}'.format(m))\n",
    "        results[m] = s        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca458d-a138-46f4-9c00-1036e5c83d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc134f2c-52cd-4079-89de-d4113b745fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "\n",
    "class Prot3DGraphModel(nn.Module):\n",
    "    def __init__(self,\n",
    "        d_vocab=21, d_embed=20,\n",
    "        d_dihedrals=6, d_pretrained_emb=1280, d_edge=39,\n",
    "        d_gcn=[128, 256, 256],\n",
    "    ):\n",
    "        super(Prot3DGraphModel, self).__init__()\n",
    "        d_gcn_in = d_gcn[0]\n",
    "        self.embed = nn.Embedding(d_vocab, d_embed)\n",
    "        self.proj_node = nn.Linear(d_embed + d_dihedrals + d_pretrained_emb, d_gcn_in)\n",
    "        self.proj_edge = nn.Linear(d_edge, d_gcn_in)\n",
    "        gcn_layer_sizes = [d_gcn_in] + d_gcn\n",
    "        layers = []\n",
    "        for i in range(len(gcn_layer_sizes) - 1):            \n",
    "            layers.append((\n",
    "                torch_geometric.nn.TransformerConv(\n",
    "                    gcn_layer_sizes[i], gcn_layer_sizes[i + 1], edge_dim=d_gcn_in),\n",
    "                'x, edge_index, edge_attr -> x'\n",
    "            ))            \n",
    "            layers.append(nn.LeakyReLU())            \n",
    "        \n",
    "        self.gcn = torch_geometric.nn.Sequential(\n",
    "            'x, edge_index, edge_attr', layers)        \n",
    "        self.pool = torch_geometric.nn.global_mean_pool\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.seq, data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = self.embed(x)\n",
    "        s = data.node_s\n",
    "        emb = data.seq_emb\n",
    "        x = torch.cat([x, s, emb], dim=-1)\n",
    "\n",
    "        edge_attr = data.edge_s\n",
    "\n",
    "        x = self.proj_node(x)\n",
    "        edge_attr = self.proj_edge(edge_attr)\n",
    "\n",
    "        x = self.gcn(x, edge_index, edge_attr)\n",
    "        x = torch_geometric.nn.global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DrugGVPModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        node_in_dim=[66, 1], node_h_dim=[128, 64],\n",
    "        edge_in_dim=[16, 1], edge_h_dim=[32, 1],\n",
    "        num_layers=3, drop_rate=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_in_dim : list of int\n",
    "            Input dimension of drug node features (si, vi).\n",
    "            Scalar node feartures have shape (N, si).\n",
    "            Vector node features have shape (N, vi, 3).\n",
    "        node_h_dims : list of int\n",
    "            Hidden dimension of drug node features (so, vo).\n",
    "            Scalar node feartures have shape (N, so).\n",
    "            Vector node features have shape (N, vo, 3).\n",
    "        \"\"\"\n",
    "        super(DrugGVPModel, self).__init__()\n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm(node_in_dim),\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None))\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm(edge_in_dim),\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate)\n",
    "            for _ in range(num_layers))\n",
    "\n",
    "        ns, _ = node_h_dim\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(node_h_dim),\n",
    "            GVP(node_h_dim, (ns, 0)))\n",
    "\n",
    "    def forward(self, xd):\n",
    "        # Unpack input data\n",
    "        h_V = (xd.node_s, xd.node_v)\n",
    "        h_E = (xd.edge_s, xd.edge_v)\n",
    "        edge_index = xd.edge_index\n",
    "        batch = xd.batch\n",
    "\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        out = self.W_out(h_V)\n",
    "\n",
    "        # per-graph mean\n",
    "        out = torch_geometric.nn.global_add_pool(out, batch)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DTAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=[128, 256, 256],\n",
    "            prot_fc_dims=[1024, 128],\n",
    "            drug_node_in_dim=[66, 1], drug_node_h_dims=[128, 64],\n",
    "            drug_edge_in_dim=[16, 1], drug_edge_h_dims=[32, 1],            \n",
    "            drug_fc_dims=[1024, 128],\n",
    "            mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super(DTAModel, self).__init__()\n",
    "\n",
    "        self.drug_model = DrugGVPModel(\n",
    "            node_in_dim=drug_node_in_dim, node_h_dim=drug_node_h_dims,\n",
    "            edge_in_dim=drug_edge_in_dim, edge_h_dim=drug_edge_h_dims,\n",
    "        )\n",
    "        drug_emb_dim = drug_node_h_dims[0]\n",
    "\n",
    "        self.prot_model = Prot3DGraphModel(\n",
    "            d_pretrained_emb=prot_emb_dim, d_gcn=prot_gcn_dims\n",
    "        )\n",
    "        prot_emb_dim = prot_gcn_dims[-1]\n",
    "\n",
    "        self.drug_fc = self.get_fc_layers(\n",
    "            [drug_emb_dim] + drug_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "       \n",
    "        self.prot_fc = self.get_fc_layers(\n",
    "            [prot_emb_dim] + prot_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "\n",
    "        self.top_fc = self.get_fc_layers(\n",
    "            [drug_fc_dims[-1] + prot_fc_dims[-1]] + mlp_dims + [1],\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "\n",
    "    def get_fc_layers(self, hidden_sizes,\n",
    "            dropout=0, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True):\n",
    "        act_fn = torch.nn.LeakyReLU()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:])):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if not no_last_activation or i != len(hidden_sizes) - 2:\n",
    "                layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                if not no_last_dropout or i != len(hidden_sizes) - 2:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            if batchnorm and i != len(hidden_sizes) - 2:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xd, xp):\n",
    "        xd = self.drug_model(xd)\n",
    "        xp = self.prot_model(xp)\n",
    "\n",
    "        xd = self.drug_fc(xd)\n",
    "        xp = self.prot_fc(xp)\n",
    "\n",
    "        x = torch.cat([xd, xp], dim=1)\n",
    "        x = self.top_fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea2858-a77c-4602-9326-5a73895f5c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mol_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad8eed-07c4-4fc8-912e-718abd457cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rdkit \n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch_geometric\n",
    "import torch_cluster\n",
    "\n",
    "\n",
    "\n",
    "def onehot_encoder(a=None, alphabet=None, default=None, drop_first=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: array of numerical value of categorical feature classes.\n",
    "    alphabet: valid values of feature classes.\n",
    "    default: default class if out of alphabet.\n",
    "    Returns\n",
    "    -------\n",
    "    A 2-D one-hot array with size |x| * |alphabet|\n",
    "    '''\n",
    "    # replace out-of-vocabulary classes\n",
    "    alphabet_set = set(alphabet)\n",
    "    a = [x if x in alphabet_set else default for x in a]\n",
    "\n",
    "    # cast to category to force class not present\n",
    "    a = pd.Categorical(a, categories=alphabet)\n",
    "\n",
    "    onehot = pd.get_dummies(pd.Series(a), columns=alphabet, drop_first=drop_first)\n",
    "    return onehot.values\n",
    "\n",
    "\n",
    "def _build_atom_feature(mol):\n",
    "    # dim: 44 + 7 + 7 + 7 + 1\n",
    "    feature_alphabet = {\n",
    "        # (alphabet, default value)\n",
    "        'GetSymbol': (ATOM_VOCAB, 'unk'),\n",
    "        'GetDegree': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetTotalNumHs': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetImplicitValence': ([0, 1, 2, 3, 4, 5, 6], 6),\n",
    "        'GetIsAromatic': ([0, 1], 1)\n",
    "    }\n",
    "\n",
    "    atom_feature = None\n",
    "    for attr in ['GetSymbol', 'GetDegree', 'GetTotalNumHs',\n",
    "                'GetImplicitValence', 'GetIsAromatic']:\n",
    "        feature = [getattr(atom, attr)() for atom in mol.GetAtoms()]\n",
    "        feature = onehot_encoder(feature,\n",
    "                    alphabet=feature_alphabet[attr][0],\n",
    "                    default=feature_alphabet[attr][1],\n",
    "                    drop_first=(attr in ['GetIsAromatic']) # binary-class feature\n",
    "                )\n",
    "        atom_feature = feature if atom_feature is None else np.concatenate((atom_feature, feature), axis=1)\n",
    "    atom_feature = atom_feature.astype(np.float32)\n",
    "    return atom_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _build_edge_feature(coords, edge_index, D_max=4.5, num_rbf=16):\n",
    "    E_vectors = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), D_max=D_max, D_count=num_rbf)\n",
    "\n",
    "    edge_s = rbf\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "    edge_s, edge_v = map(torch.nan_to_num, (edge_s, edge_v))\n",
    "\n",
    "    return edge_s, edge_v\n",
    "\n",
    "\n",
    "def sdf_to_graphs(data_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list: dict, drug key -> sdf file path\n",
    "    Returns\n",
    "    -------\n",
    "    graphs : dict\n",
    "        A list of torch_geometric graphs. drug key -> graph\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for key, sdf_path in tqdm(data_list.items(), desc='sdf'):\n",
    "        graphs[key] = featurize_drug(sdf_path, name=key)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def featurize_drug(sdf_path, name=None, edge_cutoff=4.5, num_rbf=16):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_path: str\n",
    "        Path to sdf file\n",
    "    name: str\n",
    "        Name of drug\n",
    "    Returns\n",
    "    -------\n",
    "    graph: torch_geometric.data.Data\n",
    "        A torch_geometric graph\n",
    "    \"\"\"\n",
    "    mol = rdkit.Chem.MolFromMolFile(sdf_path)\n",
    "    conf = mol.GetConformer()\n",
    "    with torch.no_grad():\n",
    "        coords = conf.GetPositions()\n",
    "        coords = torch.as_tensor(coords, dtype=torch.float32)\n",
    "        atom_feature = _build_atom_feature(mol)\n",
    "        atom_feature = torch.as_tensor(atom_feature, dtype=torch.float32)\n",
    "        edge_index = torch_cluster.radius_graph(coords, r=edge_cutoff)\n",
    "\n",
    "    node_s = atom_feature\n",
    "    node_v = coords.unsqueeze(1)\n",
    "    # edge_v, edge_index = _build_edge_feature(mol)\n",
    "    edge_s, edge_v = _build_edge_feature(\n",
    "        coords, edge_index, D_max=edge_cutoff, num_rbf=num_rbf)\n",
    "\n",
    "    data = torch_geometric.data.Data(\n",
    "        x=coords, edge_index=edge_index, name=name,\n",
    "        node_v=node_v, node_s=node_s, edge_v=edge_v, edge_s=edge_s)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58102e29-ffe6-4f61-8133-5e060bdad409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c657dd-d7c1-41c5-81a6-7ea0361ee12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_train_args(parser):\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--task', help='Task name')\n",
    "    parser.add_argument('--split_method', default='random',\n",
    "        choices=['random', 'protein', 'drug', 'both', 'seqid'],\n",
    "        help='Split method: random, protein, drug, or both')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "        help='Random Seed')\n",
    "\n",
    "    # Data representation parameters\n",
    "    parser.add_argument('--contact_cutoff', type=float, default=8.,\n",
    "        help='cutoff of C-alpha distance to define protein contact graph')\n",
    "    parser.add_argument('--num_pos_emb', type=int, default=16,\n",
    "        help='number of positional embeddings')\n",
    "    parser.add_argument('--num_rbf', type=int, default=16,\n",
    "        help='number of RBF kernels')\n",
    "\n",
    "    # Protein model parameters\n",
    "    parser.add_argument('--prot_gcn_dims', type=int, nargs='+', default=[128, 256, 256],\n",
    "        help='protein GCN layers dimensions')\n",
    "    parser.add_argument('--prot_fc_dims', type=int, nargs='+', default=[1024, 128],\n",
    "        help='protein FC layers dimensions')\n",
    "\n",
    "    # Drug model parameters\n",
    "    parser.add_argument('--drug_gcn_dims', type=int, nargs='+', default=[128, 64],\n",
    "        help='drug GVP hidden layers dimensions')\n",
    "    parser.add_argument('--drug_fc_dims', type=int, nargs='+', default=[1024, 128],\n",
    "        help='drug FC layers dimensions')\n",
    "\n",
    "    # Top model parameters\n",
    "    parser.add_argument('--mlp_dims', type=int, nargs='+', default=[1024, 512],\n",
    "        help='top MLP layers dimensions')\n",
    "    parser.add_argument('--mlp_dropout', type=float, default=0.25,\n",
    "        help='dropout rate in top MLP')\n",
    "\n",
    "    # uncertainty parameters\n",
    "    parser.add_argument('--uncertainty', action='store_true',\n",
    "        help='estimate uncertainty')\n",
    "    parser.add_argument('--recalibrate', action='store_true',\n",
    "        help='recalibrate uncertainty')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument('--n_ensembles', type=int, default=1,\n",
    "        help='number of ensembles')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "        help='batch size')\n",
    "    parser.add_argument('--n_epochs', type=int, default=500,\n",
    "        help='number of epochs')\n",
    "    parser.add_argument('--patience', action='store', type=int,\n",
    "        help='patience for early stopping')\n",
    "    parser.add_argument('--eval_freq', type=int, default=1,\n",
    "        help='evaluation frequency')\n",
    "    parser.add_argument('--test_freq', type=int,\n",
    "        help='test frequency')\n",
    "    parser.add_argument('--lr', type=float, default=0.0005,\n",
    "        help='learning rate')\n",
    "    parser.add_argument('--monitor_metric', default='pearson',\n",
    "        help='validation metric to monitor for deciding best checkpoint')\n",
    "    parser.add_argument('--parallel', action='store_true',\n",
    "        help='run ensembles in parallel on multiple GPUs')\n",
    "\n",
    "    # Save parameters\n",
    "    parser.add_argument('--output_dir', action='store', default='../output', help='output folder')\n",
    "    parser.add_argument('--save_log', action='store_true', default=False, help='save log file')\n",
    "    parser.add_argument('--save_checkpoint', action='store_true', default=False, help='save checkpoint')\n",
    "    parser.add_argument('--save_prediction', action='store_true', default=False, help='save prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb1bb0-95bc-4f7a-9009-1980ebc167ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pdb_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc096f-9f21-4ae4-91f8-f6760fbeff24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from\n",
    "https://github.com/jingraham/neurips19-graph-protein-design\n",
    "https://github.com/drorlab/gvp-pytorch\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_cluster\n",
    "\n",
    "\n",
    "def pdb_to_graphs(prot_data, params):\n",
    "    \"\"\"\n",
    "    Converts a list of protein dict to a list of torch_geometric graphs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    prot_data : dict\n",
    "        A list of protein data dict. see format in `featurize_protein_graph()`.\n",
    "    params : dict\n",
    "        A dictionary of parameters defined in `featurize_protein_graph()`.\n",
    "    Returns\n",
    "    -------\n",
    "    graphs : dict\n",
    "        A list of torch_geometric graphs. protein key -> graph\n",
    "    \"\"\"\n",
    "    graphs = {}\n",
    "    for key, struct in tqdm(prot_data.items(), desc='pdb'):\n",
    "        graphs[key] = featurize_protein_graph(\n",
    "            struct, name=key, **params)\n",
    "    return graphs\n",
    "\n",
    "def featurize_protein_graph(\n",
    "        protein, name=None,\n",
    "        num_pos_emb=16, num_rbf=16,        \n",
    "        contact_cutoff=8.,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters: see comments of DTATask() in dta.py\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        coords = torch.as_tensor(protein['coords'], dtype=torch.float32)\n",
    "        seq = torch.as_tensor([LETTER_TO_NUM[a] for a in protein['seq']], dtype=torch.long)        \n",
    "        seq_emb = torch.load(protein['embed'])\n",
    "\n",
    "        mask = torch.isfinite(coords.sum(dim=(1,2)))\n",
    "        coords[~mask] = np.inf\n",
    "\n",
    "        X_ca = coords[:, 1]        \n",
    "        ca_mask = torch.isfinite(X_ca.sum(dim=(1)))\n",
    "        ca_mask = ca_mask.float()\n",
    "        ca_mask_2D = torch.unsqueeze(ca_mask, 0) * torch.unsqueeze(ca_mask, 1)\n",
    "        dX_ca = torch.unsqueeze(X_ca, 0) - torch.unsqueeze(X_ca, 1)\n",
    "        D_ca = ca_mask_2D * torch.sqrt(torch.sum(dX_ca**2, 2) + 1e-6)\n",
    "        edge_index = torch.nonzero((D_ca < contact_cutoff) & (ca_mask_2D == 1))\n",
    "        edge_index = edge_index.t().contiguous()\n",
    "        \n",
    "\n",
    "        O_feature = _local_frame(X_ca, edge_index)\n",
    "        pos_embeddings = _positional_embeddings(edge_index, num_embeddings=num_pos_emb)\n",
    "        E_vectors = X_ca[edge_index[0]] - X_ca[edge_index[1]]\n",
    "        rbf = _rbf(E_vectors.norm(dim=-1), D_count=num_rbf)\n",
    "\n",
    "        dihedrals = _dihedrals(coords)\n",
    "        orientations = _orientations(X_ca)\n",
    "        sidechains = _sidechains(coords)\n",
    "\n",
    "        node_s = dihedrals\n",
    "        node_v = torch.cat([orientations, sidechains.unsqueeze(-2)], dim=-2)\n",
    "        edge_s = torch.cat([rbf, O_feature, pos_embeddings], dim=-1)\n",
    "        edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "        node_s, node_v, edge_s, edge_v = map(torch.nan_to_num,\n",
    "                (node_s, node_v, edge_s, edge_v))\n",
    "\n",
    "    data = torch_geometric.data.Data(x=X_ca, seq=seq, name=name,\n",
    "                                        node_s=node_s, node_v=node_v,\n",
    "                                        edge_s=edge_s, edge_v=edge_v,\n",
    "                                        edge_index=edge_index, mask=mask,                                        \n",
    "                                        seq_emb=seq_emb)\n",
    "    return data\n",
    "\n",
    "\n",
    "def _dihedrals(X, eps=1e-7):\n",
    "    X = torch.reshape(X[:, :3], [3 * X.shape[0], 3])\n",
    "    dX = X[1:] - X[:-1]\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_2 = U[:-2]\n",
    "    u_1 = U[1:-1]\n",
    "    u_0 = U[2:]\n",
    "\n",
    "    # Backbone normals\n",
    "    n_2 = _normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "    n_1 = _normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "    # Angle between normals\n",
    "    cosD = torch.sum(n_2 * n_1, -1)\n",
    "    cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n",
    "    D = torch.sign(torch.sum(u_2 * n_1, -1)) * torch.acos(cosD)\n",
    "\n",
    "    # This scheme will remove phi[0], psi[-1], omega[-1]\n",
    "    D = F.pad(D, [1, 2])\n",
    "    D = torch.reshape(D, [-1, 3])\n",
    "    # Lift angle representations to the circle\n",
    "    D_features = torch.cat([torch.cos(D), torch.sin(D)], 1)\n",
    "    return D_features\n",
    "\n",
    "\n",
    "def _positional_embeddings(edge_index,\n",
    "                            num_embeddings=None,\n",
    "                            period_range=[2, 1000]):\n",
    "    d = edge_index[0] - edge_index[1]\n",
    "\n",
    "    frequency = torch.exp(\n",
    "        torch.arange(0, num_embeddings, 2, dtype=torch.float32)\n",
    "        * -(np.log(10000.0) / num_embeddings)\n",
    "    )\n",
    "    angles = d.unsqueeze(-1) * frequency\n",
    "    E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n",
    "    return E\n",
    "\n",
    "\n",
    "def _orientations(X):\n",
    "    forward = _normalize(X[1:] - X[:-1])\n",
    "    backward = _normalize(X[:-1] - X[1:])\n",
    "    forward = F.pad(forward, [0, 0, 0, 1])\n",
    "    backward = F.pad(backward, [0, 0, 1, 0])\n",
    "    return torch.cat([forward.unsqueeze(-2), backward.unsqueeze(-2)], -2)\n",
    "\n",
    "\n",
    "def _sidechains(X):\n",
    "    n, origin, c = X[:, 0], X[:, 1], X[:, 2]\n",
    "    c, n = _normalize(c - origin), _normalize(n - origin)\n",
    "    bisector = _normalize(c + n)\n",
    "    perp = _normalize(torch.cross(c, n))\n",
    "    vec = -bisector * math.sqrt(1 / 3) - perp * math.sqrt(2 / 3)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    '''\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    '''\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n",
    "    '''\n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "\n",
    "def _local_frame(X, edge_index, eps=1e-6):\n",
    "    dX = X[1:] - X[:-1]\n",
    "    U = _normalize(dX, dim=-1)\n",
    "    u_2 = U[:-2]\n",
    "    u_1 = U[1:-1]\n",
    "    u_0 = U[2:]\n",
    "\n",
    "    # Backbone normals\n",
    "    n_2 = _normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "    n_1 = _normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "    o_1 = _normalize(u_2 - u_1, dim=-1)\n",
    "    O = torch.stack((o_1, n_2, torch.cross(o_1, n_2)), 1)\n",
    "    O = F.pad(O, (0, 0, 0, 0, 1, 2), 'constant', 0)\n",
    "\n",
    "    # dX = X[edge_index[0]] - X[edge_index[1]]\n",
    "    dX = X[edge_index[1]] - X[edge_index[0]]\n",
    "    dX = _normalize(dX, dim=-1)\n",
    "    # dU = torch.bmm(O[edge_index[1]], dX.unsqueeze(2)).squeeze(2)\n",
    "    dU = torch.bmm(O[edge_index[0]], dX.unsqueeze(2)).squeeze(2)\n",
    "    R = torch.bmm(O[edge_index[0]].transpose(-1,-2), O[edge_index[1]])\n",
    "    Q = _quaternions(R)\n",
    "    O_features = torch.cat((dU,Q), dim=-1)\n",
    "\n",
    "    return O_features\n",
    "\n",
    "\n",
    "def _quaternions(R):\n",
    "    # Simple Wikipedia version\n",
    "    # en.wikipedia.org/wiki/Rotation_matrix#Quaternion\n",
    "    # For other options see math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n",
    "    diag = torch.diagonal(R, dim1=-2, dim2=-1)\n",
    "    Rxx, Ryy, Rzz = diag.unbind(-1)\n",
    "    magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n",
    "            Rxx - Ryy - Rzz,\n",
    "        - Rxx + Ryy - Rzz,\n",
    "        - Rxx - Ryy + Rzz\n",
    "    ], -1)))\n",
    "    _R = lambda i,j: R[:, i, j]\n",
    "    signs = torch.sign(torch.stack([\n",
    "        _R(2,1) - _R(1,2),\n",
    "        _R(0,2) - _R(2,0),\n",
    "        _R(1,0) - _R(0,1)\n",
    "    ], -1))\n",
    "    xyz = signs * magnitudes\n",
    "    # The relu enforces a non-negative trace\n",
    "    w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n",
    "    Q = torch.cat((xyz, w), -1)\n",
    "    Q = F.normalize(Q, dim=-1)\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e637af3-15d5-4356-982b-7422397d7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17642161-f1cd-44c2-b2b9-6ee3500778b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "import torch\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, logfile=None, level=logging.INFO):\n",
    "        '''\n",
    "        logfile: pathlib object\n",
    "        '''\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(level)\n",
    "        formatter = logging.Formatter(\"%(asctime)s\\t%(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        for hd in self.logger.handlers[:]:\n",
    "            self.logger.removeHandler(hd)\n",
    "\n",
    "        sh = logging.StreamHandler(sys.stdout)\n",
    "        sh.setFormatter(formatter)\n",
    "        self.logger.addHandler(sh)\n",
    "\n",
    "        if logfile is not None:\n",
    "            logfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "            fh = logging.FileHandler(logfile, 'w')\n",
    "            fh.setFormatter(formatter)\n",
    "            self.logger.addHandler(fh)\n",
    "\n",
    "    def debug(self, msg):\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "    def info(self, msg):\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    def warning(self, msg):\n",
    "        self.logger.warning(msg)\n",
    "\n",
    "    def error(self, msg):\n",
    "        self.logger.error(msg)\n",
    "\n",
    "\n",
    "class Saver(object):\n",
    "    def __init__(self, output_dir):        \n",
    "        self.save_dir = pathlib.Path(output_dir)\n",
    "    \n",
    "    def mkdir(self):\n",
    "        self.save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def save_ckp(self, pt, filename='checkpoint.pt'):\n",
    "        self.mkdir()\n",
    "        torch.save(pt, str(self.save_dir/filename))\n",
    "\n",
    "    def save_df(self, df, filename, float_format='%.6f'):\n",
    "        self.mkdir()\n",
    "        df.to_csv(self.save_dir/filename, float_format=float_format, index=False, sep='\\t')\n",
    "    \n",
    "    def save_config(self, config, filename, overwrite=True):\n",
    "        self.mkdir()\n",
    "        with open(self.save_dir/filename, 'w') as f:\n",
    "            yaml.dump(config, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, \n",
    "            patience=100, eval_freq=1, best_score=None, \n",
    "            delta=1e-9, higher_better=True):\n",
    "        self.patience = patience\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_score = best_score\n",
    "        self.delta = delta\n",
    "        self.higher_better = higher_better\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def not_improved(self, val_score):\n",
    "        if np.isnan(val_score):\n",
    "            return True\n",
    "        if self.higher_better:\n",
    "            return val_score < self.best_score + self.delta\n",
    "        else:\n",
    "            return val_score > self.best_score - self.delta\n",
    "    \n",
    "    def update(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            is_best = True\n",
    "        elif self.not_improved(val_score):\n",
    "            self.counter += self.eval_freq\n",
    "            if (self.patience is not None) and (self.counter > self.patience):\n",
    "                self.early_stop = True\n",
    "            is_best = False\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            is_best = True\n",
    "        return is_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe98214-96b9-4569-a931-200a9526b44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60bf18-13dc-4ccd-836e-9d488c5645f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_esm_embedding(seq, esm_model):\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = esm_model(**inputs)\n",
    "    token_representations = outputs.last_hidden_state\n",
    "    # remove [CLS] and [EOS] if needed\n",
    "    emb = token_representations[0, 1:-1]\n",
    "    return emb.cpu()\n",
    "\n",
    "\n",
    "\n",
    "# Dataset builder for DTA class\n",
    "def build_dataset(df_fold, pdb_structures, exp_cols = \"pKi\", is_pred = False):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        if is_pred == True:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": 0\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": float(row[exp_cols]),\n",
    "            })\n",
    "    return DTA(df=df_fold, data_list=data_list)\n",
    "\n",
    "\n",
    "def extract_backbone_coords(structure, pdb_id, pdb_path):\n",
    "    coords = {\"N\": [], \"CA\": [], \"C\": [], \"O\": []}\n",
    "    seq = \"\"\n",
    "\n",
    "    model = structure[0]\n",
    "\n",
    "    valid_chain = None\n",
    "    for chain in model:\n",
    "        if any(is_aa(res, standard=True) for res in chain):\n",
    "            valid_chain = chain\n",
    "            break\n",
    "\n",
    "    if valid_chain is None:\n",
    "        print(\"No valid chains: \", pdb_id, pdb_path)\n",
    "        return None, None, None\n",
    "\n",
    "    chain_id = valid_chain.id\n",
    "\n",
    "    for res in valid_chain:\n",
    "        if not is_aa(res, standard=True):\n",
    "            continue\n",
    "        seq += res.resname[0]  # fallback, not exact 1-letter code\n",
    "\n",
    "        for atom_name in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "            if atom_name in res:\n",
    "                coords[atom_name].append(res[atom_name].coord.tolist())\n",
    "            else:\n",
    "                coords[atom_name].append([float(\"nan\")] * 3)\n",
    "\n",
    "    return seq, coords, chain_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc203eb-e8d2-47e0-9034-1edf69bc2983",
   "metadata": {},
   "source": [
    "# MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71a36e-c25c-463d-bd25-d4e12a1cc4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= MODIFICATIONS FOR MULTI-TASK LEARNING =============\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# 1. MASKED MSE LOSS WITH TASK WEIGHTING\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked MSE loss that handles NaN values and applies task-specific weighting\n",
    "    based on the inverse of the range of each task.\n",
    "    \"\"\"\n",
    "    def __init__(self, task_ranges=None):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.task_ranges = task_ranges\n",
    "        if task_ranges is not None:\n",
    "            # Calculate task weights based on inverse range\n",
    "            weights = []\n",
    "            for range_val in task_ranges.values():\n",
    "                weights.append(1.0 / range_val if range_val > 0 else 1.0)\n",
    "            total_weight = sum(weights)\n",
    "            self.task_weights = torch.tensor([w / total_weight for w in weights])\n",
    "        else:\n",
    "            self.task_weights = None\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: [batch_size, n_tasks]\n",
    "        target: [batch_size, n_tasks]\n",
    "        \"\"\"\n",
    "        # Create mask for non-NaN values\n",
    "        mask = ~torch.isnan(target)\n",
    "        \n",
    "        # Calculate MSE only for non-NaN values\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # Apply mask\n",
    "        pred_masked = pred[mask]\n",
    "        target_masked = target[mask]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        se = (pred_masked - target_masked) ** 2\n",
    "        \n",
    "        # If we have task weights, apply them\n",
    "        if self.task_weights is not None:\n",
    "            # Expand mask to get task indices\n",
    "            task_indices = torch.where(mask)[1]\n",
    "            weights = self.task_weights.to(pred.device)[task_indices]\n",
    "            weighted_se = se * weights\n",
    "            loss = weighted_se.mean()\n",
    "        else:\n",
    "            loss = se.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "# 2. MODIFIED DTA MODEL FOR MULTI-TASK LEARNING\n",
    "class MTL_DTAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            task_names=['pKi', 'pEC50', 'pKd', 'pIC50'],  # List of tasks\n",
    "            prot_emb_dim=1280,\n",
    "            prot_gcn_dims=[128, 256, 256],\n",
    "            prot_fc_dims=[1024, 128],\n",
    "            drug_node_in_dim=[66, 1], drug_node_h_dims=[128, 64],\n",
    "            drug_edge_in_dim=[16, 1], drug_edge_h_dims=[32, 1],            \n",
    "            drug_fc_dims=[1024, 128],\n",
    "            mlp_dims=[1024, 512], mlp_dropout=0.25):\n",
    "        super(MTL_DTAModel, self).__init__()\n",
    "        \n",
    "        self.task_names = task_names\n",
    "        self.n_tasks = len(task_names)\n",
    "        \n",
    "        # Same encoders as before\n",
    "        self.drug_model = DrugGVPModel(\n",
    "            node_in_dim=drug_node_in_dim, node_h_dim=drug_node_h_dims,\n",
    "            edge_in_dim=drug_edge_in_dim, edge_h_dim=drug_edge_h_dims,\n",
    "        )\n",
    "        drug_emb_dim = drug_node_h_dims[0]\n",
    "        \n",
    "        self.prot_model = Prot3DGraphModel(\n",
    "            d_pretrained_emb=prot_emb_dim, d_gcn=prot_gcn_dims\n",
    "        )\n",
    "        prot_emb_dim = prot_gcn_dims[-1]\n",
    "        \n",
    "        self.drug_fc = self.get_fc_layers(\n",
    "            [drug_emb_dim] + drug_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "       \n",
    "        self.prot_fc = self.get_fc_layers(\n",
    "            [prot_emb_dim] + prot_fc_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Shared representation layers\n",
    "        self.shared_fc = self.get_fc_layers(\n",
    "            [drug_fc_dims[-1] + prot_fc_dims[-1]] + mlp_dims,\n",
    "            dropout=mlp_dropout, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True)\n",
    "        \n",
    "        # Task-specific heads (one for each task)\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            task: nn.Linear(mlp_dims[-1], 1) for task in task_names\n",
    "        })\n",
    "    \n",
    "    def get_fc_layers(self, hidden_sizes,\n",
    "            dropout=0, batchnorm=False,\n",
    "            no_last_dropout=True, no_last_activation=True):\n",
    "        act_fn = torch.nn.LeakyReLU()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(hidden_sizes[:-1], hidden_sizes[1:])):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if not no_last_activation or i != len(hidden_sizes) - 2:\n",
    "                layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                if not no_last_dropout or i != len(hidden_sizes) - 2:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "            if batchnorm and i != len(hidden_sizes) - 2:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xd, xp):\n",
    "        # Encode drug and protein\n",
    "        xd = self.drug_model(xd)\n",
    "        xp = self.prot_model(xp)\n",
    "        \n",
    "        # Process through FC layers\n",
    "        xd = self.drug_fc(xd)\n",
    "        xp = self.prot_fc(xp)\n",
    "        \n",
    "        # Concatenate and process through shared layers\n",
    "        x = torch.cat([xd, xp], dim=1)\n",
    "        shared_repr = self.shared_fc(x)\n",
    "        \n",
    "        # Generate predictions for each task\n",
    "        outputs = []\n",
    "        for task in self.task_names:\n",
    "            task_pred = self.task_heads[task](shared_repr)\n",
    "            outputs.append(task_pred)\n",
    "        \n",
    "        # Stack outputs: [batch_size, n_tasks]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# 3. MODIFIED DTA DATASET CLASS\n",
    "class MTL_DTA(data.Dataset):\n",
    "    def __init__(self, df=None, data_list=None, task_cols=None, onthefly=False,\n",
    "                prot_featurize_fn=None, drug_featurize_fn=None):\n",
    "        super(MTL_DTA, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.data_list = data_list\n",
    "        self.task_cols = task_cols or ['pKi', 'pEC50', 'pKd', 'pIC50']\n",
    "        self.onthefly = onthefly\n",
    "        self.prot_featurize_fn = prot_featurize_fn\n",
    "        self.drug_featurize_fn = drug_featurize_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.onthefly:\n",
    "            drug = self.drug_featurize_fn(\n",
    "                self.data_list[idx]['drug'],\n",
    "                name=self.data_list[idx]['drug_name']\n",
    "            )\n",
    "            prot = self.prot_featurize_fn(\n",
    "                self.data_list[idx]['protein'],\n",
    "                name=self.data_list[idx]['protein_name']\n",
    "            )\n",
    "        else:\n",
    "            drug = self.data_list[idx]['drug']\n",
    "            prot = self.data_list[idx]['protein']\n",
    "        \n",
    "        # Get multi-task targets\n",
    "        y_multi = []\n",
    "        for task in self.task_cols:\n",
    "            val = self.data_list[idx].get(task, np.nan)\n",
    "            y_multi.append(val if not pd.isna(val) else np.nan)\n",
    "        \n",
    "        y = torch.tensor(y_multi, dtype=torch.float32)\n",
    "        \n",
    "        item = {'drug': drug, 'protein': prot, 'y': y}\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Dataset builder for DTA class\n",
    "def build_dataset(df_fold, pdb_structures, exp_cols = \"pKi\", is_pred = False):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        if is_pred == True:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": 0\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            data_list.append({\n",
    "                \"protein\": protein,\n",
    "                \"drug\": drug,\n",
    "                \"y\": float(row[exp_cols]),\n",
    "            })\n",
    "    return DTA(df=df_fold, data_list=data_list)\n",
    "\n",
    "\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 5. MODIFIED TRAINING LOOP\n",
    "def train_mtl_model(model, train_loader, valid_loader, task_cols, task_ranges, \n",
    "                    n_epochs=100, lr=0.0005, device='cuda', patience=20):\n",
    "    \"\"\"\n",
    "    Training loop for multi-task learning model\n",
    "    \n",
    "    Args:\n",
    "        task_cols: List of task column names\n",
    "        task_ranges: Dict mapping task names to their value ranges\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedMSELoss(task_ranges=task_ranges)\n",
    "    stopper = EarlyStopping(patience=patience, higher_better=False)\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            xd = batch['drug'].to(device)\n",
    "            xp = batch['protein'].to(device)\n",
    "            y = batch['y'].to(device)  # [batch_size, n_tasks]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xd, xp)  # [batch_size, n_tasks]\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_n_batches = 0\n",
    "        task_metrics = {task: {'mse': 0, 'n': 0} for task in task_cols}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xd = batch['drug'].to(device)\n",
    "                xp = batch['protein'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                \n",
    "                pred = model(xd, xp)\n",
    "                loss = criterion(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                val_n_batches += 1\n",
    "                \n",
    "                # Calculate per-task metrics\n",
    "                for i, task in enumerate(task_cols):\n",
    "                    mask = ~torch.isnan(y[:, i])\n",
    "                    if mask.sum() > 0:\n",
    "                        task_mse = F.mse_loss(pred[mask, i], y[mask, i])\n",
    "                        task_metrics[task]['mse'] += task_mse.item()\n",
    "                        task_metrics[task]['n'] += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / n_batches\n",
    "        avg_val_loss = val_loss / val_n_batches if val_n_batches > 0 else float('inf')\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Valid Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        for task in task_cols:\n",
    "            if task_metrics[task]['n'] > 0:\n",
    "                avg_task_mse = task_metrics[task]['mse'] / task_metrics[task]['n']\n",
    "                print(f\"  {task} MSE: {avg_task_mse:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if stopper.update(avg_val_loss):\n",
    "            best_model = model.state_dict()\n",
    "        if stopper.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 6. EXAMPLE USAGE\n",
    "def prepare_mtl_experiment(df, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Prepare data for multi-task learning\n",
    "    \"\"\"\n",
    "    # Calculate task ranges for weighting\n",
    "    task_ranges = {}\n",
    "    for task in task_cols:\n",
    "        if task in df.columns:\n",
    "            valid_values = df[task].dropna()\n",
    "            if len(valid_values) > 0:\n",
    "                task_ranges[task] = valid_values.max() - valid_values.min()\n",
    "            else:\n",
    "                task_ranges[task] = 1.0\n",
    "        else:\n",
    "            task_ranges[task] = 1.0\n",
    "    \n",
    "    print(\"Task ranges for weighting:\")\n",
    "    for task, range_val in task_ranges.items():\n",
    "        weight = 1.0 / range_val if range_val > 0 else 1.0\n",
    "        normalized_weight = weight / sum(1.0/r if r > 0 else 1.0 for r in task_ranges.values())\n",
    "        print(f\"  {task}: range={range_val:.2f}, weight={normalized_weight:.4f}\")\n",
    "    \n",
    "    return task_ranges\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def structureJSON(df, esm_model):\n",
    "    structure_dict = {}\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        pdb_path = row[\"standardized_protein_pdb\"]\n",
    "        try:\n",
    "\n",
    "            pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "\n",
    "            structure = parser.get_structure(pdb_id, pdb_path)\n",
    "            seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "            if seq is None:\n",
    "                available = [c.id for c in structure[0]]\n",
    "                print(f\"[SKIP] {pdb_id}: no usable chain found (available: {available})\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Stack in order: N, CA, C, O --> [L, 4, 3]\n",
    "            coords_stacked = []\n",
    "            for i in range(len(coords[\"N\"])):\n",
    "                coord_group = []\n",
    "                for atom in [\"N\", \"CA\", \"C\", \"O\"]:\n",
    "                    coord_group.append(coords[atom][i])\n",
    "                coords_stacked.append(coord_group)\n",
    "\n",
    "            if coords_stacked is None:\n",
    "                print(f\"[SKIP] {pdb_id}: no usable coords found (available: {pdb_path})\")\n",
    "                continue\n",
    "\n",
    "                \n",
    "            embedding = get_esm_embedding(seq, esm_model)\n",
    "            torch.save(embedding, f\"esm_embeddings/{pdb_id}.pt\")\n",
    "\n",
    "            if coords_stacked != None and embedding != None:\n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": f\"esm_embeddings/{pdb_id}.pt\"\n",
    "\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"../data/pockets_structure.json\", \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to pockets_structure.json\")\n",
    "\n",
    "    return(structure_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d52c7-479c-45ce-b88a-5bb69ed2a292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from Bio.PDB import PDBParser  # make sure Biopython is installed\n",
    "\n",
    "# Assumes you already have:\n",
    "# - extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "# - get_esm_embedding(seq, esm_model)\n",
    "\n",
    "ATOMS = (\"N\", \"CA\", \"C\", \"O\")\n",
    "EMBED_DIR = Path(\"esm_embeddings\")\n",
    "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _stack_backbone(coords):\n",
    "    # coords: dict with keys \"N\",\"CA\",\"C\",\"O\", each a list of [x,y,z]\n",
    "    L = len(coords[\"N\"])\n",
    "    return [[coords[a][i] for a in ATOMS] for i in range(L)]\n",
    "\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Worker: parse PDB, extract seq/coords/chain, return tuple or a skip marker.\n",
    "    Runs in a separate process; initializes its own parser.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdb_id = os.path.basename(pdb_path).split('.')[0]\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "\n",
    "        seq, coords, chain_id = extract_backbone_coords(structure, pdb_id, pdb_path)\n",
    "        if seq is None:\n",
    "            available = [c.id for c in structure[0]]\n",
    "            return (\"skip\", pdb_id, f\"no usable chain (available: {available})\")\n",
    "\n",
    "        if not coords or any(k not in coords for k in ATOMS) or len(coords[\"N\"]) == 0:\n",
    "            return (\"skip\", pdb_id, \"no usable coords\")\n",
    "\n",
    "        coords_stacked = _stack_backbone(coords)\n",
    "        if not coords_stacked:\n",
    "            return (\"skip\", pdb_id, \"empty coords after stacking\")\n",
    "\n",
    "        return (\"ok\", pdb_id, seq, coords_stacked, chain_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"error\", os.path.basename(pdb_path).split('.')[0], str(e))\n",
    "\n",
    "def structureJSON(df, esm_model, max_workers=None, embed_batch_size=8, out_json=\"../data/pockets_structure.json\"):\n",
    "    structure_dict = {}\n",
    "\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    results = []\n",
    "\n",
    "    # Phase 1: parallel PDB parsing + coordinate extraction\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"PDB -> seq/coords\"):\n",
    "            status_tuple = fut.result()\n",
    "            results.append(status_tuple)\n",
    "\n",
    "    # Log skips/errors (fast)\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            _, pdb_id, msg = r\n",
    "            print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "        elif tag == \"error\":\n",
    "            _, pdb_id, err = r\n",
    "            print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "\n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "\n",
    "    # Phase 2: embeddings on a single device (GPU/CPU) to avoid per-process model copies\n",
    "    # Optionally batch if your get_esm_embedding supports lists; otherwise do per-sequence.\n",
    "    # Here we do per-sequence by default; simple and safe.\n",
    "    for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, desc=\"ESM embeddings\"):\n",
    "        try:\n",
    "            embedding = get_esm_embedding(seq, esm_model)  # ensure this returns a tensor\n",
    "            torch.save(embedding, EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "\n",
    "            structure_dict[pdb_id] = {\n",
    "                \"name\": pdb_id,\n",
    "                \"UniProt_id\": \"UNKNOWN\",\n",
    "                \"PDB_id\": pdb_id,\n",
    "                \"chain\": chain_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords\": coords_stacked,         # [[N,CA,C,O], ...], each as [x,y,z]\n",
    "                \"embed\": str(EMBED_DIR / f\"{pdb_id}.pt\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ Done. Saved {len(structure_dict)} protein structures to {os.path.basename(out_json)}\")\n",
    "    return structure_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "def structureJSON_chunked(df, esm_model, max_workers=None, embed_batch_size=8, \n",
    "                          chunk_size=100000, out_dir=\"../data/structure_chunks/\"):\n",
    "    \"\"\"\n",
    "    Process structures in chunks to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model: ESM model for embeddings\n",
    "        max_workers: Number of parallel workers\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(EMBED_DIR, exist_ok=True)\n",
    "    \n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"Processing {total_pdbs} PDBs in {num_chunks} chunks of max {chunk_size} each\")\n",
    "    \n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"\\n=== Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_paths)} PDBs) ===\")\n",
    "        \n",
    "        structure_dict = {}\n",
    "        results = []\n",
    "        \n",
    "        # Phase 1: Parallel PDB parsing for this chunk\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = {ex.submit(_process_pdb_path, p): p for p in chunk_paths}\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                          desc=f\"Chunk {chunk_idx + 1} - PDB parsing\"):\n",
    "                status_tuple = fut.result()\n",
    "                results.append(status_tuple)\n",
    "        \n",
    "        # Log errors for this chunk\n",
    "        for r in results:\n",
    "            tag = r[0]\n",
    "            if tag == \"skip\":\n",
    "                _, pdb_id, msg = r\n",
    "                print(f\"[SKIP] {pdb_id}: {msg}\")\n",
    "            elif tag == \"error\":\n",
    "                _, pdb_id, err = r\n",
    "                print(f\"[ERROR] Failed to process {pdb_id}: {err}\")\n",
    "        \n",
    "        # Keep only successful items\n",
    "        ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                    for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                    for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "        \n",
    "        # Phase 2: ESM embeddings for this chunk\n",
    "        for pdb_id, seq, coords_stacked, chain_id in tqdm(ok_items, \n",
    "                                                          desc=f\"Chunk {chunk_idx + 1} - ESM embeddings\"):\n",
    "            try:\n",
    "                embedding = get_esm_embedding(seq, esm_model)\n",
    "                embed_path = EMBED_DIR / f\"{pdb_id}.pt\"\n",
    "                torch.save(embedding, embed_path)\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": str(embed_path)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Save this chunk\n",
    "        chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "        chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "        with open(chunk_path, \"w\") as f:\n",
    "            json.dump(structure_dict, f, indent=2)\n",
    "        \n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": chunk_idx,\n",
    "            \"filename\": chunk_filename,\n",
    "            \"path\": chunk_path,\n",
    "            \"num_structures\": len(structure_dict),\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        \n",
    "        print(f\"✅ Chunk {chunk_idx + 1} saved: {len(structure_dict)} structures to {chunk_filename}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ All chunks processed. Metadata saved to {metadata_path}\")\n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            # Quick scan to build index (could be saved in metadata for efficiency)\n",
    "            with open(chunk_path, \"r\") as f:\n",
    "                chunk_data = json.load(f)\n",
    "                for pdb_id in chunk_data.keys():\n",
    "                    self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# 4. MODIFIED BUILD DATASET FUNCTION\n",
    "def build_mtl_dataset(df_fold, pdb_structures, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    data_list = []\n",
    "    for i, row in df_fold.iterrows():\n",
    "        pdb_id = os.path.basename(row[\"standardized_ligand_sdf\"]).split(\".\")[0]\n",
    "        protein_json = pdb_structures.get(pdb_id)\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b9fa7-c897-4d2f-aac7-7b6d9a91019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "\n",
    "def process_single_chunk(args):\n",
    "    \"\"\"\n",
    "    Process a single chunk of PDB files independently.\n",
    "    This function is designed to be run in parallel.\n",
    "    \n",
    "    Args:\n",
    "        args: tuple of (chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name)\n",
    "    \n",
    "    Returns:\n",
    "        dict with chunk processing results\n",
    "    \"\"\"\n",
    "    chunk_idx, pdb_paths, out_dir, embed_dir, esm_model_name = args\n",
    "    \n",
    "    # Import inside function for multiprocessing\n",
    "    from transformers import EsmModel, EsmTokenizer\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    print(f\"\\n[Chunk {chunk_idx}] Starting processing of {len(pdb_paths)} PDBs\")\n",
    "    \n",
    "    # Load ESM model for this process\n",
    "    print(f\"[Chunk {chunk_idx}] Loading ESM model...\")\n",
    "    tokenizer = EsmTokenizer.from_pretrained(esm_model_name)\n",
    "    esm_model = EsmModel.from_pretrained(esm_model_name)\n",
    "    esm_model.eval()\n",
    "    \n",
    "    # Move to GPU if available (each process gets its own GPU memory)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        # For multi-GPU, assign different chunks to different GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        gpu_id = chunk_idx % num_gpus\n",
    "        device = torch.device(f'cuda:{gpu_id}')\n",
    "        esm_model = esm_model.to(device)\n",
    "        print(f\"[Chunk {chunk_idx}] Using GPU {gpu_id}\")\n",
    "    else:\n",
    "        print(f\"[Chunk {chunk_idx}] Using CPU\")\n",
    "    \n",
    "    structure_dict = {}\n",
    "    results = []\n",
    "    \n",
    "    # Phase 1: Parallel PDB parsing within this chunk\n",
    "    max_workers = min(8, cpu_count() // 4)  # Limit workers per chunk\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(_process_pdb_path, p): p for p in pdb_paths}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), \n",
    "                      desc=f\"Chunk {chunk_idx} - PDB parsing\", position=chunk_idx):\n",
    "            try:\n",
    "                status_tuple = fut.result(timeout=30)  # Add timeout\n",
    "                results.append(status_tuple)\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] Error processing PDB: {e}\")\n",
    "    \n",
    "    # Log errors\n",
    "    error_count = 0\n",
    "    skip_count = 0\n",
    "    for r in results:\n",
    "        tag = r[0]\n",
    "        if tag == \"skip\":\n",
    "            skip_count += 1\n",
    "        elif tag == \"error\":\n",
    "            error_count += 1\n",
    "    \n",
    "    if error_count > 0 or skip_count > 0:\n",
    "        print(f\"[Chunk {chunk_idx}] Skipped: {skip_count}, Errors: {error_count}\")\n",
    "    \n",
    "    # Keep only successful items\n",
    "    ok_items = [(pdb_id, seq, coords_stacked, chain_id)\n",
    "                for tag, pdb_id, *rest in results if tag == \"ok\"\n",
    "                for (seq, coords_stacked, chain_id) in [tuple(rest)]]\n",
    "    \n",
    "    # Phase 2: ESM embeddings (batch processing for efficiency)\n",
    "    print(f\"[Chunk {chunk_idx}] Computing ESM embeddings for {len(ok_items)} proteins...\")\n",
    "    \n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in batches to optimize GPU usage\n",
    "    batch_size = 8\n",
    "    for i in tqdm(range(0, len(ok_items), batch_size), \n",
    "                  desc=f\"Chunk {chunk_idx} - ESM embeddings\", position=chunk_idx):\n",
    "        batch = ok_items[i:i+batch_size]\n",
    "        \n",
    "        for pdb_id, seq, coords_stacked, chain_id in batch:\n",
    "            try:\n",
    "                # Compute embedding\n",
    "                with torch.no_grad():\n",
    "                    embedding = get_esm_embedding(seq, esm_model, tokenizer, device)\n",
    "                \n",
    "                # Save embedding\n",
    "                embed_path = os.path.join(embed_dir, f\"{pdb_id}.pt\")\n",
    "                torch.save(embedding.cpu(), embed_path)  # Save on CPU to free GPU memory\n",
    "                \n",
    "                structure_dict[pdb_id] = {\n",
    "                    \"name\": pdb_id,\n",
    "                    \"UniProt_id\": \"UNKNOWN\",\n",
    "                    \"PDB_id\": pdb_id,\n",
    "                    \"chain\": chain_id,\n",
    "                    \"seq\": seq,\n",
    "                    \"coords\": coords_stacked,\n",
    "                    \"embed\": embed_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"[Chunk {chunk_idx}] ESM embedding failed for {pdb_id}: {e}\")\n",
    "        \n",
    "        # Periodic garbage collection\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save chunk\n",
    "    chunk_filename = f\"structures_chunk_{chunk_idx:04d}.json\"\n",
    "    chunk_path = os.path.join(out_dir, chunk_filename)\n",
    "    with open(chunk_path, \"w\") as f:\n",
    "        json.dump(structure_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"[Chunk {chunk_idx}] ✅ Completed: {len(structure_dict)} structures saved\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del esm_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        \"chunk_idx\": chunk_idx,\n",
    "        \"filename\": chunk_filename,\n",
    "        \"path\": chunk_path,\n",
    "        \"num_structures\": len(structure_dict),\n",
    "        \"num_errors\": error_count,\n",
    "        \"num_skipped\": skip_count\n",
    "    }\n",
    "\n",
    "\n",
    "def get_esm_embedding(seq, esm_model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Get ESM embedding for a sequence.\n",
    "    \n",
    "    Args:\n",
    "        seq: Protein sequence\n",
    "        esm_model: ESM model\n",
    "        tokenizer: ESM tokenizer\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = esm_model(**inputs)\n",
    "        # Use mean pooling over sequence length\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def structureJSON_chunked(df, esm_model_name=\"facebook/esm2_t33_650M_UR50D\",\n",
    "                         chunk_size=100000, max_chunks_parallel=4,\n",
    "                         out_dir=\"../data/structure_chunks/\",\n",
    "                         embed_dir=\"../data/embeddings/\"):\n",
    "    \"\"\"\n",
    "    Process structures in parallel chunks to avoid memory issues and maximize speed.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with protein PDB paths\n",
    "        esm_model_name: Name of ESM model to use\n",
    "        chunk_size: Maximum entries per chunk (default 100000)\n",
    "        max_chunks_parallel: Maximum number of chunks to process in parallel\n",
    "        out_dir: Directory to save chunked JSON files\n",
    "        embed_dir: Directory to save embeddings\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata about created chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(embed_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique PDB paths (avoid duplicates)\n",
    "    pdb_paths = df[\"standardized_protein_pdb\"].unique().tolist()\n",
    "    total_pdbs = len(pdb_paths)\n",
    "    num_chunks = (total_pdbs + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Processing {total_pdbs} unique PDBs in {num_chunks} chunks\")\n",
    "    print(f\"Chunk size: {chunk_size}, Parallel chunks: {max_chunks_parallel}\")\n",
    "    print(f\"=\" * 80)\n",
    "    \n",
    "    # Prepare chunk arguments\n",
    "    chunk_args = []\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * chunk_size\n",
    "        end_idx = min((chunk_idx + 1) * chunk_size, total_pdbs)\n",
    "        chunk_paths = pdb_paths[start_idx:end_idx]\n",
    "        \n",
    "        chunk_args.append((\n",
    "            chunk_idx,\n",
    "            chunk_paths,\n",
    "            out_dir,\n",
    "            embed_dir,\n",
    "            esm_model_name\n",
    "        ))\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    chunk_results = []\n",
    "    \n",
    "    # Determine optimal number of parallel processes\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    if num_gpus > 0:\n",
    "        # If we have GPUs, process one chunk per GPU\n",
    "        parallel_chunks = min(max_chunks_parallel, num_gpus, num_chunks)\n",
    "        print(f\"Using {num_gpus} GPUs, processing {parallel_chunks} chunks in parallel\")\n",
    "    else:\n",
    "        # CPU only - limit parallelism to avoid memory issues\n",
    "        parallel_chunks = min(max_chunks_parallel, cpu_count() // 4, num_chunks)\n",
    "        print(f\"Using CPU only, processing {parallel_chunks} chunks in parallel\")\n",
    "    \n",
    "    # Process in batches of parallel chunks\n",
    "    for batch_start in range(0, num_chunks, parallel_chunks):\n",
    "        batch_end = min(batch_start + parallel_chunks, num_chunks)\n",
    "        batch_args = chunk_args[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\nProcessing chunk batch {batch_start+1}-{batch_end} of {num_chunks}\")\n",
    "        \n",
    "        if len(batch_args) == 1:\n",
    "            # Single chunk - process directly\n",
    "            result = process_single_chunk(batch_args[0])\n",
    "            chunk_results.append(result)\n",
    "        else:\n",
    "            # Multiple chunks - use multiprocessing\n",
    "            with Pool(processes=len(batch_args)) as pool:\n",
    "                results = pool.map(process_single_chunk, batch_args)\n",
    "                chunk_results.extend(results)\n",
    "        \n",
    "        # Garbage collection between batches\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create metadata\n",
    "    chunk_metadata = {\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"total_structures\": sum(r[\"num_structures\"] for r in chunk_results),\n",
    "        \"total_errors\": sum(r[\"num_errors\"] for r in chunk_results),\n",
    "        \"total_skipped\": sum(r[\"num_skipped\"] for r in chunk_results),\n",
    "        \"chunks\": []\n",
    "    }\n",
    "    \n",
    "    # Add chunk info with proper indices\n",
    "    start_idx = 0\n",
    "    for result in sorted(chunk_results, key=lambda x: x[\"chunk_idx\"]):\n",
    "        end_idx = start_idx + result[\"num_structures\"]\n",
    "        chunk_info = {\n",
    "            \"chunk_idx\": result[\"chunk_idx\"],\n",
    "            \"filename\": result[\"filename\"],\n",
    "            \"path\": result[\"path\"],\n",
    "            \"num_structures\": result[\"num_structures\"],\n",
    "            \"num_errors\": result[\"num_errors\"],\n",
    "            \"num_skipped\": result[\"num_skipped\"],\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx\n",
    "        }\n",
    "        chunk_metadata[\"chunks\"].append(chunk_info)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(out_dir, \"chunk_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"✅ Processing complete!\")\n",
    "    print(f\"  - Total structures: {chunk_metadata['total_structures']}\")\n",
    "    print(f\"  - Total errors: {chunk_metadata['total_errors']}\")\n",
    "    print(f\"  - Total skipped: {chunk_metadata['total_skipped']}\")\n",
    "    print(f\"  - Metadata saved: {metadata_path}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    return chunk_metadata\n",
    "\n",
    "\n",
    "# ============= Helper function for PDB processing =============\n",
    "def _process_pdb_path(pdb_path):\n",
    "    \"\"\"\n",
    "    Process a single PDB file to extract sequence and coordinates.\n",
    "    This function runs in a separate process.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (status, pdb_id, data...) where status is \"ok\", \"skip\", or \"error\"\n",
    "    \"\"\"\n",
    "    from Bio.PDB import PDBParser, is_aa\n",
    "    \n",
    "    parser = PDBParser(QUIET=True)\n",
    "    pdb_id = os.path.splitext(os.path.basename(pdb_path))[0]\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(pdb_id, pdb_path)\n",
    "        \n",
    "        # Get first model\n",
    "        model = structure[0]\n",
    "        \n",
    "        # Process each chain\n",
    "        for chain in model:\n",
    "            residues = [r for r in chain if is_aa(r)]\n",
    "            if len(residues) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract sequence\n",
    "            seq = ''.join([seq1(r.resname) for r in residues])\n",
    "            \n",
    "            # Extract coordinates [N, CA, C, O] for each residue\n",
    "            coords = []\n",
    "            for residue in residues:\n",
    "                try:\n",
    "                    n_coord = residue['N'].coord.tolist()\n",
    "                    ca_coord = residue['CA'].coord.tolist()\n",
    "                    c_coord = residue['C'].coord.tolist()\n",
    "                    o_coord = residue['O'].coord.tolist()\n",
    "                    coords.append([n_coord, ca_coord, c_coord, o_coord])\n",
    "                except:\n",
    "                    # Missing atoms - use zeros\n",
    "                    coords.append([[0,0,0], [0,0,0], [0,0,0], [0,0,0]])\n",
    "            \n",
    "            return (\"ok\", pdb_id, seq, coords, chain.id)\n",
    "        \n",
    "        return (\"skip\", pdb_id, \"No valid chains found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (\"error\", pdb_id, str(e))\n",
    "\n",
    "\n",
    "# ============= Optimized Chunk Loader (same as before) =============\n",
    "class StructureChunkLoader:\n",
    "    \"\"\"\n",
    "    Efficient loader for chunked structure dictionaries.\n",
    "    Loads chunks on-demand and caches them.\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_dir=\"../data/structure_chunks/\", cache_size=2):\n",
    "        self.chunk_dir = chunk_dir\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}  # chunk_idx -> structure_dict\n",
    "        self.cache_order = []  # LRU tracking\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = os.path.join(chunk_dir, \"chunk_metadata.json\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded metadata: {self.metadata['total_structures']} structures in {self.metadata['num_chunks']} chunks\")\n",
    "        \n",
    "        # Build lookup: pdb_id -> chunk_idx\n",
    "        self.pdb_to_chunk = {}\n",
    "        for chunk_info in self.metadata[\"chunks\"]:\n",
    "            chunk_path = os.path.join(chunk_dir, chunk_info[\"filename\"])\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, \"r\") as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                    for pdb_id in chunk_data.keys():\n",
    "                        self.pdb_to_chunk[pdb_id] = chunk_info[\"chunk_idx\"]\n",
    "            else:\n",
    "                print(f\"Warning: Chunk file not found: {chunk_path}\")\n",
    "    \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load a chunk into cache, managing cache size.\"\"\"\n",
    "        if chunk_idx in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.cache_order.remove(chunk_idx)\n",
    "            self.cache_order.append(chunk_idx)\n",
    "            return self.cache[chunk_idx]\n",
    "        \n",
    "        # Load chunk\n",
    "        chunk_info = self.metadata[\"chunks\"][chunk_idx]\n",
    "        chunk_path = os.path.join(self.chunk_dir, chunk_info[\"filename\"])\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        # Add to cache\n",
    "        self.cache[chunk_idx] = chunk_data\n",
    "        self.cache_order.append(chunk_idx)\n",
    "        \n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) > self.cache_size:\n",
    "            oldest = self.cache_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "            gc.collect()  # Force garbage collection\n",
    "        \n",
    "        return chunk_data\n",
    "    \n",
    "    def get(self, pdb_id):\n",
    "        \"\"\"Get structure for a specific PDB ID.\"\"\"\n",
    "        if pdb_id not in self.pdb_to_chunk:\n",
    "            return None\n",
    "        \n",
    "        chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "        chunk_data = self._load_chunk(chunk_idx)\n",
    "        return chunk_data.get(pdb_id)\n",
    "    \n",
    "    def get_batch(self, pdb_ids):\n",
    "        \"\"\"Get multiple structures efficiently by grouping by chunk.\"\"\"\n",
    "        # Group PDB IDs by chunk\n",
    "        chunk_groups = {}\n",
    "        for pdb_id in pdb_ids:\n",
    "            if pdb_id in self.pdb_to_chunk:\n",
    "                chunk_idx = self.pdb_to_chunk[pdb_id]\n",
    "                if chunk_idx not in chunk_groups:\n",
    "                    chunk_groups[chunk_idx] = []\n",
    "                chunk_groups[chunk_idx].append(pdb_id)\n",
    "        \n",
    "        # Load each chunk and extract structures\n",
    "        results = {}\n",
    "        for chunk_idx, chunk_pdb_ids in chunk_groups.items():\n",
    "            chunk_data = self._load_chunk(chunk_idx)\n",
    "            for pdb_id in chunk_pdb_ids:\n",
    "                if pdb_id in chunk_data:\n",
    "                    results[pdb_id] = chunk_data[pdb_id]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_available_pdb_ids(self):\n",
    "        \"\"\"Return set of all available PDB IDs.\"\"\"\n",
    "        return set(self.pdb_to_chunk.keys())\n",
    "\n",
    "\n",
    "def build_mtl_dataset_optimized(df_fold, chunk_loader, task_cols=['pKi', 'pEC50', 'pKd', 'pIC50']):\n",
    "    \"\"\"\n",
    "    Build MTL dataset efficiently using chunked structure loader.\n",
    "    \n",
    "    Args:\n",
    "        df_fold: DataFrame with fold data\n",
    "        chunk_loader: StructureChunkLoader instance\n",
    "        task_cols: List of task columns\n",
    "    \n",
    "    Returns:\n",
    "        MTL_DTA dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all protein IDs from the fold\n",
    "    protein_ids = df_fold[\"protein_id\"].tolist()\n",
    "    \n",
    "    # Batch load structures (efficient chunk-based loading)\n",
    "    print(f\"Loading structures for {len(protein_ids)} proteins...\")\n",
    "    structures_batch = chunk_loader.get_batch(protein_ids)\n",
    "    \n",
    "    # Process each row\n",
    "    skipped = 0\n",
    "    for i, row in tqdm(df_fold.iterrows(), total=len(df_fold), desc=\"Building dataset\"):\n",
    "        protein_id = row[\"protein_id\"]\n",
    "        \n",
    "        if protein_id not in structures_batch:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        protein_json = structures_batch[protein_id]\n",
    "        protein = featurize_protein_graph(protein_json)\n",
    "        drug = featurize_drug(row[\"standardized_ligand_sdf\"])\n",
    "        \n",
    "        # Collect all task values\n",
    "        task_values = {}\n",
    "        for task in task_cols:\n",
    "            if task in row and not pd.isna(row[task]):\n",
    "                task_values[task] = float(row[task])\n",
    "            else:\n",
    "                task_values[task] = np.nan\n",
    "        \n",
    "        data_entry = {\n",
    "            \"protein\": protein,\n",
    "            \"drug\": drug,\n",
    "        }\n",
    "        data_entry.update(task_values)\n",
    "        data_list.append(data_entry)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Warning: Skipped {skipped} entries due to missing structures\")\n",
    "    \n",
    "    return MTL_DTA(df=df_fold, data_list=data_list, task_cols=task_cols)\n",
    "\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ce606-ee95-4850-bb99-c3f6095e9858",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6b3cd-b54d-42d2-85f0-035da7ebd23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parser = PDBParser(QUIET=True)\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Defne tasks to train on\n",
    "task_cols = ['pKi', 'pEC50', 'pKd', 'pKd (Wang, FEP)', 'pIC50', 'potency']\n",
    "\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\", engine=\"fastparquet\")\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "col_nan = [\"standardized_protein_pdb\", \"standardized_ligand_sdf\"] + task_cols\n",
    "# df = df[df['is_experimental'] == True]\n",
    "df = df.dropna(how = \"all\", subset=col_nan)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[df[\"standardized_protein_pdb\"].isna()==False]\n",
    "df = df[df[\"standardized_ligand_sdf\"].isna()==False]\n",
    "df = df[:50000]\n",
    "\n",
    "# Calculate task ranges from your dataframe\n",
    "task_ranges = prepare_mtl_experiment(df, task_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bff598-24c9-4d26-924c-5dc700614db9",
   "metadata": {},
   "source": [
    "# Load ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309da2a6-a78f-4aa2-8413-63bee8a064e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load ESM-2 model\n",
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "esm_model = EsmModel.from_pretrained(model_name)\n",
    "esm_model.eval().cuda() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e8321-de80-48c4-9172-9d5463eb7e4b",
   "metadata": {},
   "source": [
    "# Generate structure dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ce19b-bfd0-4fc6-82eb-0480703ed6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# so chunk, then iterate in 4 chunk the esm embedding, then iterate each chuck in parralization and output each time the dict of strutucre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544717a-3be0-48b2-ae9b-1064c1437bef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)  # Must be at the very beginning\n",
    "\n",
    "from parallel_structure_processing_optimized import structureJSON_chunked_optimized\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "structure_metadata = structureJSON_chunked_optimized(\n",
    "    df,\n",
    "    num_gpus=1,\n",
    "    cpu_workers=90\n",
    ")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f5851-51ad-4c57-b73e-c0f576aa1c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(0.23*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518089f0-550a-4e90-86f1-14ae6388bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5996d3-8c90-4311-baa9-0ad8cb428c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(0.23-5.63)/5.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd8943-eb35-41f4-87ad-f0f551acc609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Total structures processed: {structure_metadata['total_structures']}\")\n",
    "print(f\"Total chunks created: {structure_metadata['num_chunks']}\")\n",
    "\n",
    "# Optional: Load and use the chunks later\n",
    "from parallel_structure_processing_optimized import StructureChunkLoader\n",
    "\n",
    "# Create a chunk loader (caches 2 chunks in memory at a time)\n",
    "chunk_loader = StructureChunkLoader(\n",
    "    chunk_dir=\"../data/structure_chunks/\",\n",
    "    cache_size=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92cff4-f3d6-4577-8456-e7a6e5c622b8",
   "metadata": {},
   "source": [
    "# Check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69276b-e6fe-4d5e-8e9a-50e4fce72edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/structure_chunks/structures_chunk_0000.json\", \"r\") as f:\n",
    "    pdb_structures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0a970-889e-4828-9def-bde6f9b7c353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c558cb3-37fe-4c33-bd90-fb24851ccbac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "rldif118",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python (rldif118)",
   "language": "python",
   "name": "rldif118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
